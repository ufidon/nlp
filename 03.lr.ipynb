{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/03.lr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/03.lr.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "üìù SALP chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression Classifier**\n",
    "- **Heading**: Introduction to Logistic Regression\n",
    "- **Content**:\n",
    "  - **Logistic regression** is a **discriminative** model used for binary classification.\n",
    "  - Instead of modeling the distribution of data, it directly estimates the probability of a class using the **logistic function**:\n",
    "    \\[\n",
    "    P(C=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_n X_n)}}\n",
    "    \\]\n",
    "  - The goal is to find the best-fitting **hyperplane** that separates the classes.\n",
    "- **Visual**: A diagram of a logistic curve.\n",
    "\n",
    "---\n",
    "\n",
    "### **Logistic Regression: Decision Boundaries**\n",
    "- **Heading**: Logistic Regression Decision Boundaries\n",
    "- **Content**:\n",
    "  - Logistic regression creates a **linear decision boundary** in the feature space.\n",
    "  - It works by estimating the **log odds** of the class labels.\n",
    "  - The decision boundary is determined by finding the hyperplane where $P(C=1) = 0.5$.\n",
    "- **Visual**: A 2D plot showing a linear decision boundary separating two classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparing Naive Bayes and Logistic Regression**\n",
    "- **Heading**: Naive Bayes vs. Logistic Regression\n",
    "- **Content**:\n",
    "  - **Naive Bayes**:\n",
    "    - **Generative** model.\n",
    "    - Assumes conditional independence of features.\n",
    "    - Works well with smaller datasets and high-dimensional data.\n",
    "  - **Logistic Regression**:\n",
    "    - **Discriminative** model.\n",
    "    - Finds a linear decision boundary.\n",
    "    - More flexible as it doesn‚Äôt rely on the conditional independence assumption.\n",
    "- **Visual**: Comparison table highlighting differences.\n",
    "\n",
    "---\n",
    "\n",
    "### **Evaluation Metrics: Precision and Recall**\n",
    "- **Heading**: Evaluating Classifiers\n",
    "- **Content**:\n",
    "  - **Precision**: What proportion of positive identifications were actually correct?\n",
    "  - **Recall**: What proportion of actual positives were correctly identified?\n",
    "  - **F1 Score**: Harmonic mean of precision and recall.\n",
    "  - These metrics are crucial for comparing Naive Bayes and Logistic Regression models on text classification tasks.\n",
    "- **Visual**: A confusion matrix and precision-recall curve.\n",
    "\n",
    "---\n",
    "\n",
    "### **Training and Cross-Validation**\n",
    "- **Heading**: Training, Dev, and Test Sets\n",
    "- **Content**:\n",
    "  - Both Naive Bayes and Logistic Regression are trained using **training**, **development**, and **test sets**.\n",
    "  - **Cross-validation** is used to optimize hyperparameters and avoid overfitting.\n",
    "  - Importance of using different sets for training and evaluation to get robust results.\n",
    "- **Visual**: Diagram showing the data split into training, dev, and test sets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Real-World Applications**\n",
    "- **Heading**: Applications of Naive Bayes and Logistic Regression\n",
    "- **Content**:\n",
    "  - **Naive Bayes**:\n",
    "    - Spam filtering.\n",
    "    - Sentiment analysis (e.g., positive/negative review classification).\n",
    "  - **Logistic Regression**:\n",
    "    - Text categorization (e.g., news article classification).\n",
    "    - Predicting customer churn based on reviews or text data.\n",
    "- **Visual**: Examples of applications with logos of tools (e.g., spam detection in Gmail).\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion and Key Takeaways**\n",
    "- **Heading**: Conclusion\n",
    "- **Content**:\n",
    "  - **Naive Bayes** is a simple, efficient model, particularly well-suited for high-dimensional text classification.\n",
    "  - **Logistic Regression** provides a more flexible, discriminative approach with linear decision boundaries.\n",
    "  - Both are widely used for **text classification** tasks like sentiment analysis, spam detection, and topic categorization.\n",
    "- **Visual**: Summary of key points."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
