{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/03.lr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/03.lr.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "üìù SALP chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generative Classifier: Naive Bayes**\n",
    "- Models $P(x | y)$ and $P(y)$, then applies Bayes‚Äô Theorem to compute $P(y | x)$.\n",
    "  - $P(y | x) \\propto P(x | y) P(y)$\n",
    "  - Assumes feature independence (Naive Bayes assumption).\n",
    "- **Generative model** ‚Äî can simulate data points.\n",
    "- **Types**:\n",
    "  - **Multinomial Naive Bayes**: Used for text classification (e.g., spam detection).\n",
    "  - **Gaussian Naive Bayes**: Used for continuous data.\n",
    "\n",
    "\n",
    "### **Classifier Types**\n",
    "- **Generative**: Model how data is generated (`Naive Bayes`)\n",
    "- **Discriminative**: Directly model decision boundary (`Logistic Regression`)\n",
    "\n",
    "\n",
    "### **Discriminative Classifier: Logistic Regression**\n",
    "  - Directly models $P(y | x)$ ‚Äî the probability of the label $y$ given the features $x$.\n",
    "    - $\\displaystyle P(y=1 | x) = \\frac{1}{1 + e^{-(\\theta^T x)}}$\n",
    "    - **Sigmoid function** for binary classification.\n",
    "  - **Decision boundary-based**.\n",
    "    - Does not assume underlying data distribution.\n",
    "- **Common Uses**:\n",
    "  - Binary and multiclass classification.\n",
    "  - Sentiment analysis, spam filtering.\n",
    "\n",
    "\n",
    "### **Mathematical Comparison**\n",
    "| **Aspect**     | **Logistic Regression**    | **Naive Bayes**    |\n",
    "|-------------|---------------|------------------------|\n",
    "| **Model Type**  | Discriminative     | Generative       |\n",
    "| **Estimates**    | $P(y \\| x)$ directly    | $P(x \\| y)$ and $P(y)$      |\n",
    "| **Feature Dependence**| No assumptions  | Assumes conditional independence   |\n",
    "| **Computational Cost**| Generally higher    | Often faster for high-dimensional data |\n",
    "| **Training Data Requirements** | Larger datasets for accuracy | Can work well with small datasets  |\n",
    "| **Strengths** | ‚ñ∑ Flexible with features.<br>‚ñ∑ Robust to correlated features.‚ñ∑ Directly optimizes decision boundary. | ‚ñ∑ Simple and fast, even with smaller datasets.<br>‚ñ∑ Good for text classification with a high number of features |\n",
    "| **Weaknesses** | ‚ñ∑ Requires larger datasets.<br>‚ñ∑ Can overfit without regularization. | ‚ñ∑ Assumption of feature independence is often unrealistic.<br>‚ñ∑ Can underperform when features are highly correlated. |\n",
    "| **Use cases** | ‚ñ∑ Large datasets with many features.<br>‚ñ∑ Features that are highly correlated.<br>‚ñ∑ A need for direct probability estimates. |‚ñ∑ Smaller datasets.<br>‚ñ∑ Assumption of feature independence is reasonable (e.g., bag of words in text).<br>‚ñ∑ A need for quick and simple model deployment.|"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
