{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/03.lr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/03.lr.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "üìù SALP chapter 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generative Classifier: Naive Bayes**\n",
    "- Models $P(x | y)$ and $P(y)$, then applies Bayes‚Äô Theorem to compute $P(y | x)$.\n",
    "  - $P(y | x) \\propto P(x | y) P(y)$\n",
    "  - Assumes feature independence (Naive Bayes assumption).\n",
    "- **Generative model** ‚Äî can simulate data points.\n",
    "- **Types**:\n",
    "  - **Multinomial Naive Bayes**: Used for text classification (e.g., spam detection).\n",
    "  - **Gaussian Naive Bayes**: Used for continuous data.\n",
    "\n",
    "\n",
    "### **Classifier Types**\n",
    "- **Generative**: Model how data is generated (`Naive Bayes`)\n",
    "- **Discriminative**: Directly model decision boundary (`Logistic Regression`)\n",
    "\n",
    "\n",
    "### **Discriminative Classifier: Logistic Regression**\n",
    "  - Directly models $P(y | x)$ ‚Äî the probability of the label $y$ given the features $x$.\n",
    "    - $\\displaystyle P(y=1 | x) = \\frac{1}{1 + e^{-(\\theta^T x)}}$\n",
    "    - **Sigmoid function** for binary classification.\n",
    "  - **Decision boundary-based**.\n",
    "    - Does not assume underlying data distribution.\n",
    "- **Common Uses**:\n",
    "  - Binary and multiclass classification.\n",
    "  - Sentiment analysis, spam filtering.\n",
    "\n",
    "\n",
    "### **Mathematical Comparison**\n",
    "| **Aspect**     | **Logistic Regression**    | **Naive Bayes**    |\n",
    "|-------------|---------------|------------------------|\n",
    "| **Model Type**  | Discriminative     | Generative       |\n",
    "| **Estimates**    | $P(y \\| x)$ directly    | $P(x \\| y)$ and $P(y)$      |\n",
    "| **Feature Dependence**| No assumptions  | Assumes conditional independence   |\n",
    "| **Computational Cost**| Generally higher    | Often faster for high-dimensional data |\n",
    "| **Training Data Requirements** | Larger datasets for accuracy | Can work well with small datasets  |\n",
    "| **Strengths** | ‚ñ∑ Flexible with features.<br>‚ñ∑ Robust to correlated features.‚ñ∑ Directly optimizes decision boundary. | ‚ñ∑ Simple and fast, even with smaller datasets.<br>‚ñ∑ Good for text classification with a high number of features |\n",
    "| **Weaknesses** | ‚ñ∑ Requires larger datasets.<br>‚ñ∑ Can overfit without regularization. | ‚ñ∑ Assumption of feature independence is often unrealistic.<br>‚ñ∑ Can underperform when features are highly correlated. |\n",
    "| **Use cases** | ‚ñ∑ Large datasets with many features.<br>‚ñ∑ Features that are highly correlated.<br>‚ñ∑ A need for direct probability estimates. |‚ñ∑ Smaller datasets.<br>‚ñ∑ Assumption of feature independence is reasonable (e.g., bag of words in text).<br>‚ñ∑ A need for quick and simple model deployment.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Components of a Probabilistic Classifier**\n",
    "- Machine learning classifiers like **naive Bayes** and **logistic regression** are probabilistic classifiers that use **supervised learning**.\n",
    "- They require a **training corpus** of $m$ input/output pairs: $(x^{(i)}, y^{(i)})$, where $x^{(i)}$ is an input observation and $y^{(i)}$ is the class label.\n",
    "\n",
    "\n",
    "### **Training Corpus**\n",
    "- In **sentiment classification**, $x^{(i)}$ might represent a document, and $y^{(i)}$ its sentiment label (positive or negative).\n",
    "  - Superscripts in parentheses refer to individual instances.\n",
    "- **Data Example Table:**\n",
    "  - $x^{(1)}$: \"The movie was great!\" ‚Üí $y^{(1)} = 1$ (positive).\n",
    "  - $x^{(2)}$: \"The movie was terrible.\" ‚Üí $y^{(2)} = 0$ (negative).\n",
    "\n",
    "\n",
    "### **Key Components of a Classifier**\n",
    "1. **Feature Representation** of the input.\n",
    "   - It turns inputs into structured data.\n",
    "2. **Classification Function** to estimate $\\hat{y}$ based on $p(y|x)$.\n",
    "   - It computes class probabilities.\n",
    "3. **Objective Function** for minimizing error.\n",
    "   - It measures how well the model performs.\n",
    "4. **Optimization Algorithm** to train the model.\n",
    "   - It is used to optimize the model during training.\n",
    "\n",
    "\n",
    "### **1. Feature Representation of the Input**\n",
    "- For each input observation $x^{(i)}$, represent it as a **vector of features**: $[x_1, x_2, \\dots, x_n]$.\n",
    "  - For instance $j$, **feature** $i$ is represented as $x_i^{(j)}$ or simplified as $x_i$.\n",
    "- In multiclass classification, features may be noted as $f_i(c, x)$, where $c$ represents the class.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Classification Function**\n",
    "- The classification function computes the **estimated class** $\\hat{y}$ using $p(y|x)$, the probability of $y$ given $x$.\n",
    "- **Logistic regression** uses the **sigmoid function** for binary classification, while **softmax** is used for multiclass classification.\n",
    "\n",
    "- For binary classification: $p(y=1|x) = \\dfrac{1}{1 + e^{-(w^T \\cdot x + b)}}$ (sigmoid).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **3. Objective Function**\n",
    "- We define an **objective function** to optimize, which typically involves minimizing a **loss function** that measures classification error.\n",
    "- In logistic regression, this is the **cross-entropy loss function**.\n",
    "\n",
    "- Binary Cross-Entropy Loss: $\\mathcal{L} = - [ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) ]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **4. Optimization Algorithm**\n",
    "- The optimization algorithm aims to minimize the objective function, updating the model parameters.\n",
    "- **Stochastic Gradient Descent (SGD)** is commonly used for this task.\n",
    "- SGD process:\n",
    "  - Initialize weights.\n",
    "  - Loop over training data: compute gradient, update weights.\n",
    "  - Repeat until convergence.\n",
    "\n",
    "\n",
    "\n",
    "### **Logistic Regression Phases**\n",
    "- **Training Phase:**\n",
    "  - Train the system by learning the **weights** $w$ and **bias** $b$ using stochastic gradient descent and minimizing the cross-entropy loss.\n",
    "  - Weight update: $w = w - \\eta \\nabla \\mathcal{L}$\n",
    "- **Testing Phase:**\n",
    "  - Given a test example $x$, compute $p(y|x)$ and return the label with the **higher probability**: $y = 1$ or $y = 0$.\n",
    "\n",
    "\n",
    "### **Test Example**\n",
    "- Input: \"The product is amazing!\" (vectorized features).\n",
    "- Logistic regression computes the probability of sentiment being **positive** or **negative**.\n",
    "- Output: $p(\\text{positive} | x) = 0.85$, so the system predicts **positive** sentiment $(y=1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Binary Logistic Regression**\n",
    "- **Goal:**\n",
    "  - Train a classifier that can make a **binary decision** about the class of a new input.\n",
    "- **Example:**\n",
    "  - $y = 1$: Positive class (e.g., positive sentiment).\n",
    "  - $y = 0$: Negative class (e.g., negative sentiment).\n",
    "- **Objective:**\n",
    "  - Estimate $P(y = 1 | x)$, the probability that the input belongs to class 1.\n",
    "\n",
    "\n",
    "### **Input Representation in Logistic Regression**\n",
    "- **Input Observation $x$:**\n",
    "  - Represented as a **vector of features**: $[x_1, x_2, \\dots, x_n]$.\n",
    "- **Example:**\n",
    "  - For sentiment classification, $x$ could represent word counts in a document\n",
    "    - e.g., \"awesome\" vs. \"abysmal\".\n",
    "\n",
    "\n",
    "### **Logistic Regression Classifier**\n",
    "- **Weights and Bias:**\n",
    "  - Logistic regression learns a **weight** $w_i$ for each feature $x_i$, and a **bias term** $b$.\n",
    "  - **Weight $w_i$:**\n",
    "    - Positive: Evidence that the input belongs to the positive class.\n",
    "    - Negative: Evidence that the input belongs to the negative class.\n",
    "- **Example:**\n",
    "  - \"awesome\" ‚Üí high positive weight $w_{\\text{awesome}}$.\n",
    "  - \"abysmal\" ‚Üí high negative weight $w_{\\text{abysmal}}$.\n",
    "\n",
    "\n",
    "### **Calculating the Score $z$**\n",
    "- **Weighted Sum of Features:**\n",
    "  - The classifier calculates the score $z$, the **weighted sum** of the features plus the bias:\n",
    "    - $\\displaystyle z = \\sum_{i=1}^{n} w_i x_i + b$\n",
    "  - **Dot Product Notation:**\n",
    "    - $z = w \\cdot x + b$\n",
    "\n",
    "\n",
    "### **From Score to Probability**\n",
    "- **Problem:**\n",
    "  - $z$ can range from $-\\infty$ to $+\\infty$, but we need a **probability** between 0 and 1.\n",
    "- **Solution:**\n",
    "  - Use the **sigmoid function** to convert $z$ to a probability.\n",
    "    - $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Generate values for z\n",
    "z_values = np.linspace(-10, 10, 100)\n",
    "sigmoid_values = sigmoid(z_values)\n",
    "\n",
    "# Plot sigmoid function\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(z_values, sigmoid_values, label=r'$\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$', color='blue')\n",
    "plt.axhline(0.5, color='gray', linestyle='--', label='y = 0.5')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(r'$\\sigma(z)$')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Sigmoid Function: Interpretation**\n",
    "- Converts real-valued numbers into the range $(0, 1)$.\n",
    "- **Nearly linear** around 0 but **squashes** outliers toward 0 or 1.\n",
    "- **Differentiable**, which is important for optimization.\n",
    "\n",
    "\n",
    "### **Calculating Probabilities**\n",
    "- **Positive Class Probability:**\n",
    "  $P(y = 1 | x) = \\sigma(w \\cdot x + b) = \\dfrac{1}{1 + e^{-(w \\cdot x + b)}}$\n",
    "- **Negative Class Probability:**\n",
    "  $P(y = 0 | x) = 1 - \\sigma(w \\cdot x + b) = \\dfrac{e^{-(w \\cdot x + b)}}{1 + e^{-(w \\cdot x + b)}}$\n",
    "\n",
    "\n",
    "### **Sigmoid Function Properties**\n",
    "- **Key Property:**\n",
    "  - $1 - \\sigma(x) = \\sigma(-x)$\n",
    "  - This allows us to express $P(y = 0)$ as $\\sigma(-(w \\cdot x + b))$.\n",
    "- **Logit Function:**\n",
    "  - The input to the sigmoid $z = w \\cdot x + b$ is called the **logit**.\n",
    "  - The **logit function** is the **inverse of the sigmoid**:\n",
    "  - $\\text{logit}(p) = \\sigma^{-1}(p) = \\ln\\left(\\dfrac{p}{1 - p}\\right)$\n",
    "\n",
    "\n",
    "## üçé **Binary Sentiment Classification**\n",
    "- **Task:** Predict the sentiment of a review.\n",
    "- **Input:** Feature vector $x$ representing word counts.\n",
    "- **Model:** Logistic regression with learned weights.\n",
    "- **Score Calculation:** \n",
    "  - $z = w \\cdot x + b$\n",
    "  - $z = w^T x + b$ (linear combination of weights and features)\n",
    "  - $w = [w_1, w_2, ..., w_n]$ is the weight vector\n",
    "  - $x = [x_1, x_2, ..., x_n]$ are the features\n",
    "  - $b$ is the bias term.\n",
    "- **Probability Calculation:**\n",
    "  - $P(\\text{positive} | x) = \\sigma(z)$\n",
    "- **Decision:** If $P(\\text{positive} | x) > 0.5$, predict positive sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1: Sample Product Reviews**\n",
    "\n",
    "Consider these three sample product reviews:\n",
    "\n",
    "1. **Review 1**: \"I love this product! It's amazing and works perfectly.\"\n",
    "2. **Review 2**: \"This is the worst product I have ever used. I hate it!\"\n",
    "3. **Review 3**: \"It's okay, but I've seen better.\"\n",
    "\n",
    "### **Step 2: Manually Extract Features**\n",
    "We need to manually extract features from the reviews. Let's use the following features:\n",
    "- $x_1$ - **Count of positive words** (e.g., love, amazing, perfect, etc.)\n",
    "- $x_2$ - **Count of negative words** (e.g., worst, hate, bad, etc.)\n",
    "- $x_3$ - **Presence of exclamation marks** (1 if any, 0 otherwise)\n",
    "- $x_4$ - **Log of total word count**\n",
    "\n",
    "| Review     | Positive Words | Negative Words | Exclamations | Log Word Count |\n",
    "|------------|----------------|----------------|--------------|----------------|\n",
    "| Review 1   | 3              | 0              | 1            | 2.19           |\n",
    "| Review 2   | 0              | 2              | 1            | 2.48           |\n",
    "| Review 3   | 0              | 0              | 0            | 1.79           |\n",
    "\n",
    "### **Step 3: Define Weights**\n",
    "Assume we have pre-trained weights from a logistic regression model:\n",
    "- $w = [2.5, -4.0, 1.2, 0.6]$ (weights for positive words, negative words, exclamations, and log word count)\n",
    "- $b = 0.2$ (bias term)\n",
    "\n",
    "### **Step 4: Manually Perform the Calculation**\n",
    "- $z = w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + b$\n",
    "#### **Review 1 Calculation**:\n",
    "- $z = 2.5 \\times 3 + (-4.0) \\times 0 + 1.2 \\times 1 + 0.6 \\times 2.19 + 0.2$\n",
    "- $z = 7.5 + 0 + 1.2 + 1.314 + 0.2 = 10.214$\n",
    "- Probability $P(y = 1 | x) = \\sigma(10.214) = \\dfrac{1}{1 + e^{-10.214}} = \\approx 1$ (Positive sentiment)\n",
    "\n",
    "#### **Review 2 Calculation**:\n",
    "- $z = 2.5 \\times 0 + (-4.0) \\times 2 + 1.2 \\times 1 + 0.6 \\times 2.48 + 0.2$\n",
    "- $z = 0 - 8 + 1.2 + 1.488 + 0.2 = -5.112$\n",
    "- Probability $P(y = 1 | x) = \\sigma(-5.112) \\approx 0.006$ (Negative sentiment)\n",
    "\n",
    "#### **Review 3 Calculation**:\n",
    "- $z = 2.5 \\times 0 + (-4.0) \\times 0 + 1.2 \\times 0 + 0.6 \\times 2.40 + 0.2$\n",
    "- $z = 0 - 0 + 0 + 1.44 + 0.2 = 1.64$\n",
    "- Probability $P(y = 1 | x) = \\sigma(1.64) \\approx 0.837$ (Positive sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Step 1: Define lists of positive and negative words\n",
    "positive_words = {\"love\", \"amazing\", \"perfect\", \"perfectly\", \"great\", \"good\", \"excellent\", \"like\"}\n",
    "negative_words = {\"worst\", \"hate\", \"bad\", \"terrible\", \"awful\", \"poor\", \"dislike\"}\n",
    "\n",
    "# Step 2: Function to extract features from a review\n",
    "def extract_features(review):\n",
    "    # Clean the review (remove punctuation and convert to lowercase)\n",
    "    review_clean = re.sub(r'[^\\w\\s]', '', review.lower())\n",
    "    words = review_clean.split()\n",
    "\n",
    "\n",
    "    # Feature 1: Count of positive words\n",
    "    positive_count = sum(1 for word in words if word in positive_words)\n",
    "    \n",
    "    # Feature 2: Count of negative words\n",
    "    negative_count = sum(1 for word in words if word in negative_words)\n",
    "    \n",
    "    # Feature 3: Presence of exclamation marks\n",
    "    exclamation_count = 1 if \"!\" in review else 0\n",
    "    \n",
    "    # Feature 4: Log of total word count\n",
    "    word_count = len(words)\n",
    "    print(f'\\n{review}\\n{review_clean}\\n{words}\\n{word_count}')\n",
    "\n",
    "    log_word_count = math.log(word_count) if word_count > 0 else 0\n",
    "    \n",
    "    # Return the features as a list\n",
    "    return [positive_count, negative_count, exclamation_count, log_word_count]\n",
    "\n",
    "# Step 3: List of sample reviews\n",
    "reviews = [\n",
    "    \"I love this product! It's amazing and works perfectly.\",  # Review 1\n",
    "    \"This is the worst product I have ever used. I hate it!\",  # Review 2\n",
    "    \"It's okay, but I've seen better.\"                         # Review 3\n",
    "]\n",
    "\n",
    "# Step 4: Extract features for each review\n",
    "feature_matrix = np.array([extract_features(review) for review in reviews])\n",
    "\n",
    "# Step 5: Define weights and bias (as provided)\n",
    "weights = np.array([2.5, -4.0, 1.2, 0.6])\n",
    "bias = 0.2\n",
    "\n",
    "# Step 6: Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Step 7: Calculate z for each review\n",
    "z_values = np.dot(feature_matrix, weights) + bias\n",
    "\n",
    "# Step 8: Apply sigmoid to get probabilities\n",
    "probabilities = sigmoid(z_values)\n",
    "\n",
    "# Step 9: Classify based on probabilities (threshold = 0.5)\n",
    "classifications = (probabilities > 0.5).astype(int)\n",
    "\n",
    "# Step 10: Output the results\n",
    "for i, (prob, classification) in enumerate(zip(probabilities, classifications), 1):\n",
    "    sentiment = \"Positive\" if classification == 1 else \"Negative\"\n",
    "    print(f\"Review {i}: Probability of positive sentiment = {prob:.4f}, Classification = {sentiment}\")\n",
    "\n",
    "# Optional: Display extracted features\n",
    "print(\"\\nExtracted Feature Matrix (Positive Words, Negative Words, Exclamations, Log Word Count):\")\n",
    "print(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Classification Tasks and Features in Logistic Regression\n",
    "- Logistic regression is commonly applied to a variety of NLP tasks.\n",
    "- Any property of the input can become a feature.\n",
    "- Example Task: **Period Disambiguation**  \n",
    "  - Classifying periods as either:\n",
    "    - **EOS (End-of-Sentence)**  \n",
    "    - **Not-EOS** (Part of a word)\n",
    "\n",
    "\n",
    "### Features for Period Disambiguation \n",
    "- **x‚ÇÅ:** Is the current word lowercase?  \n",
    "  - `x‚ÇÅ = 1` if `Case(w·µ¢) = Lower`, else `x‚ÇÅ = 0`\n",
    "- **x‚ÇÇ:** Is the current word an abbreviation?  \n",
    "  - `x‚ÇÇ = 1` if `w·µ¢ ‚àà AcronymDict`, else `x‚ÇÇ = 0`\n",
    "- **x‚ÇÉ:** Is the word \"St.\" with the previous word capitalized?  \n",
    "  - `x‚ÇÉ = 1` if `w·µ¢ = St. & Case(w·µ¢‚Çã‚ÇÅ) = Upper`, else `x‚ÇÉ = 0`\n",
    "\n",
    "\n",
    "\n",
    "### Complex Features and Interactions\n",
    "- Features can express combinations of properties.\n",
    "- Example:  \n",
    "  - **Uppercase + Word \"St.\"**  \n",
    "    - If the word is \"St.\" and the previous word is capitalized, the period is likely part of \"St.\"  \n",
    "    - Otherwise, it‚Äôs likely an end of a sentence (EOS).\n",
    "- Traditional models use `hand-designed features` based on linguistic intuition such as\n",
    "  - Period disambiguation with designed features like acronyms and capitalized words.\n",
    "- **Error analysis** on early systems helps refine features.\n",
    "  \n",
    "\n",
    "### Learning Features Automatically  \n",
    "- Recent NLP models focus on **representation learning**\n",
    "  - ways to learn features automatically in an unsupervised way from the input.\n",
    "- Features can be automatically created using **feature templates**\n",
    "  - abstract specifications of features.\n",
    "**Example:**  \n",
    "- Bigram templates generate features from word pairs before periods in the training set.\n",
    "- The feature is hashed into a unique identifier.\n",
    "\n",
    "\n",
    "\n",
    "### Standardization and Rescaling Input Features \n",
    "- Input features often have varying ranges.  \n",
    "- Rescaling makes features comparable.\n",
    "- **Z-score standardizing** the input features to standard normal distribution:\n",
    "  - $\\displaystyle x'_i = \\frac{x_i - \\mu_i}{\\sigma_i}$\n",
    "  \n",
    "  - $\\displaystyle \\mu_i = \\frac{1}{m} \\sum_{j=1}^{m} x_i^{(j)}$\n",
    "  \n",
    "  - $\\displaystyle \\sigma_i = \\sqrt{\\frac{1}{m} \\sum_{j=1}^{m} (x_i^{(j)} - \\mu_i)^2}$\n",
    "- **Normalizing** the input features values to lie between 0 and 1:\n",
    "  - $\\displaystyle x'_i = \\frac{x_i - \\min(x_i)}{\\max(x_i) - \\min(x_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Many Examples at Once in Logistic Regression \n",
    "- Logistic regression equations above shown for single examples.\n",
    "- In practice, we need to process **entire test sets** at once.\n",
    "- We will use matrix operations to process all examples efficiently.\n",
    "\n",
    "\n",
    "### The For-Loop Approach\n",
    "- For each test example $x^{(i)}$, compute the predicted output:\n",
    "  - $\\hat{y}^{(i)} = \\sigma(w \\cdot x^{(i)} + b)$\n",
    "- Example for 3 test examples:\n",
    "  - $P(y^{(1)} = 1|x^{(1)}) = \\sigma(w \\cdot x^{(1)} + b)$\n",
    "  - $P(y^{(2)} = 1|x^{(2)}) = \\sigma(w \\cdot x^{(2)} + b)$\n",
    "  - $P(y^{(3)} = 1|x^{(3)}) = \\sigma(w \\cdot x^{(3)} + b)$\n",
    "\n",
    "\n",
    "### Efficient Matrix Operation \n",
    "- Instead of looping, we use matrix operations to process all examples at once.\n",
    "- **Input matrix $X$:**  \n",
    "  - Each row $x^{(i)}$ contains the feature vector for example $i$.\n",
    "- **Matrix dimensions:**  \n",
    "  - $X \\in \\mathbb{R}^{m \\times f}$ where:\n",
    "    - $m$ = number of examples\n",
    "    - $f$ = number of features per example.\n",
    "- Matrix Computation for Outputs\n",
    "  - $\\mathbf{y} = Xw + b$\n",
    "  where:\n",
    "  - $X$ is the input matrix of shape $m \\times f$,\n",
    "  - $w$ is the weight vector of shape $f \\times 1$,\n",
    "  - $b$ is the bias term, repeated $m$ times, $b \\in \\mathbb{R}^{m \\times 1}$.\n",
    "- This operation computes the same as looping through examples.\n",
    "- For example, $\\hat{y}^{(1)}$:\n",
    "  - $\\hat{y}^{(1)} = [x_1^{(1)}, x_2^{(1)}, ..., x_f^{(1)}] \\cdot [w_1, w_2, ..., w_f] + b$\n",
    "- Shapes of operations:\n",
    "  - $\\mathbf{y} = Xw + b \\quad (m \\times 1) = (m \\times f) \\cdot (f \\times 1) + (m \\times 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logistic Regression   \n",
    "- Used when we need **more than two classes** such as\n",
    "  - **3-way sentiment classification** (positive, negative, neutral).\n",
    "  - **Part-of-speech tagging** with many possible tags (e.g., 30‚Äì50 POS tags).\n",
    "  - **Named entity recognition** (person, location, organization, etc.).\n",
    "- Multinomial logistic regression, also called **softmax regression** or **maxent classifier**.\n",
    "\n",
    "\n",
    "### One-Hot Encoding for Classification  \n",
    "- Assigns an observation to **one of K classes**.\n",
    "- **One-hot vector** representation:\n",
    "  - If the correct class is $c$, set $y_c = 1$, and all other elements of $y$ to 0.\n",
    "  - Example: For three classes (positive, neutral, negative), one-hot for neutral is $y = [0, 1, 0]$.\n",
    "- The classifier predicts a vector $\\hat{y}$ where $\\hat{y}_k$ represents the probability $p(y_k = 1|x)$.\n",
    "\n",
    "\n",
    "\n",
    "### The Softmax Function  \n",
    "- Generalizes the Sigmoid Function\n",
    "- Computes probabilities for K classes.\n",
    "  - $\\displaystyle \\hat{y}_i = \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$\n",
    "- Given the input a vector $z = [z_1, z_2, ..., z_K]$ of K values,\n",
    "  - $\\displaystyle \\hat{y} =\\text{softmax}(z) = [\\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\text{ for } 1‚â§i‚â§K ]$\n",
    "- Output: probabilities $\\hat{y}_i$ summing to 1, each in the range $[0, 1]$.\n",
    "\n",
    "  \n",
    "### üçé Example of Softmax Calculation   \n",
    "- Given vector $z = [0.6, 1.1, -1.5, 1.2, 3.2, -1.1]$, compute softmax.\n",
    "- Resulting probabilities (rounded):\n",
    "  - $[0.05, 0.09, 0.01, 0.10, 0.74, 0.01]$\n",
    "- **Interpretation:**\n",
    "  - Largest value in $z$ has highest probability (0.74 for $z_5 = 3.2$).\n",
    "  - Softmax squashes other values toward 0.\n",
    "\n",
    "\n",
    "\n",
    "### Applying Softmax in Logistic Regression  \n",
    "- **Input:** Dot product between weight vector $w$ and input $x$ (plus bias).\n",
    "- Now, we need separate weight vectors $w_k$ and biases $b_k$ for each class $k$.\n",
    "- **Class probability:**\n",
    "  - $\\displaystyle p(y_k = 1 | x) = \\dfrac{e^{w_k \\cdot x + b_k}}{\\sum_{j=1}^{K} e^{w_j \\cdot x + b_j}}$\n",
    "\n",
    "\n",
    "\n",
    "### Matrix Representation for Efficient Computation  \n",
    "- Instead of computing each output separately, use **matrix operations**.\n",
    "- **Weight matrix $W$:**\n",
    "  - Each row $k$ corresponds to $w_k$, the weight vector for class $k$.\n",
    "  - Matrix $W$ has shape $[K \\times f]$, with K classes and f input features.\n",
    "- **Bias vector $b$** contains one value for each class.\n",
    "- **Output probabilities:**\n",
    "  - $\\hat{y} = \\text{softmax}(Wx + b)$\n",
    "\n",
    "\n",
    "### Example of Matrix Arithmetic   \n",
    "- For 3 classes and 2 features:\n",
    "  - $W = \\begin{bmatrix} w_{1,1} & w_{1,2} \\\\ w_{2,1} & w_{2,2} \\\\ w_{3,1} & w_{3,2} \\end{bmatrix}, \\quad x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}, \\quad b = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix}$\n",
    "- Compute $Wx + b$, then apply softmax to get $\\hat{y}$.\n",
    "\n",
    "\n",
    "\n",
    "### Features in Multinomial Logistic Regression  \n",
    "- **Separate weight vectors** for each class.\n",
    "- A feature can act as **evidence for or against different classes** such as\n",
    "  - Exclamation mark feature $f_5(x)$:\n",
    "    - $w_{5,+} = 3.5, \\quad w_{5,-} = 3.1, \\quad w_{5,0} = -5.3$\n",
    "    - Positive weights push toward positive or negative classes; negative weights push away.\n",
    "\n",
    "\n",
    "### Feature Representation for Multiclass Tasks  \n",
    "- Features depend on both input text and class such as\n",
    "  - **Sentiment classification (3-way):** A feature like $f_5(x)$ might have different values for each class:\n",
    "    - $f_5(x, +)$, $f_5(x, -)$, $f_5(x, 0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving Binary Logistic Regression from Multinomial Logistic Regression\n",
    "\n",
    "- For binary logistic regression, there are only two classes, \n",
    "  - usually denoted as $y = 0$ and $y = 1$. \n",
    "  - and only one set of weights and bias is needed: $\\mathbf{w}$ and $b$\n",
    "    - since the probability of one class can be derived from the other.\n",
    "\n",
    "- Simplify the multinomial logistic regression formula as follows:\n",
    "\n",
    "  - $\\displaystyle P(y = 1 \\mid \\mathbf{x}) = \\dfrac{e^{(\\mathbf{w}_1 \\cdot \\mathbf{x} + b_1)}}{e^{(\\mathbf{w}_0 \\cdot \\mathbf{x} + b_0)} + e^{(\\mathbf{w}_1 \\cdot \\mathbf{x} + b_1)}}$\n",
    "\n",
    "- Since $P(y = 0 \\mid \\mathbf{x}) = 1 - P(y = 1 \\mid \\mathbf{x})$, we can deduce:\n",
    "\n",
    "  - $\\displaystyle P(y = 1 \\mid \\mathbf{x}) = \\dfrac{1}{1 + \\dfrac{e^{(\\mathbf{w}_0 \\cdot \\mathbf{x} + b_0)}}{e^{(\\mathbf{w}_1 \\cdot \\mathbf{x} + b_1)}}}$\n",
    "\n",
    "- Let‚Äôs define:\n",
    "  - $\\mathbf{w} = \\mathbf{w}_1 - \\mathbf{w}_0$\n",
    "  - $b = b_1 - b_0$\n",
    "\n",
    "- Then:\n",
    "\n",
    "  - $\\displaystyle P(y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + e^{(-(\\mathbf{w} \\cdot \\mathbf{x} + b))}}$\n",
    "\n",
    "- This is the familiar sigmoid function form:\n",
    "\n",
    "  - $\\displaystyle P(y = 1 \\mid \\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)$\n",
    "  - where $\\sigma(z) = \\dfrac{1}{1 + e^{(-z)}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Learning in Logistic Regression**\n",
    "- **Objective**: Learn parameters (weights $w$ and bias $b$) to make the model's predicted output $\\hat{y}$ as close as possible to the true output $y$.\n",
    "- **Supervised Classification**: We know the correct label $y$ for each observation $x$. The goal is to adjust $w$ and $b$ to minimize the distance between $\\hat{y}$ and $y$.\n",
    "- **Two Key Components**:\n",
    "  1. **Loss/cost Function**: Measures how far $\\hat{y}$ is from the true $y$.\n",
    "  2. **Optimization Algorithm**: Iteratively updates parameters to minimize the loss (e.g., gradient descent).\n",
    "\n",
    "\n",
    "### **The Cross-Entropy Loss Function**\n",
    "- **Goal**: Minimize the difference between the classifier's output $\\hat{y}$ and the true output $y$.\n",
    "- **Cross-Entropy Loss** (Negative Log-Likelihood Loss):\n",
    "  - $‚Ñí_{CE}(\\hat{y}, y) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]$\n",
    "  - If $y = 1$, the loss depends on $\\log(\\hat{y})$.\n",
    "  - If $y = 0$, the loss depends on $\\log(1 - \\hat{y})$.\n",
    "\n",
    "\n",
    "### **Deriving the Cross-Entropy Loss**\n",
    "1. **Probability of Correct Label** $p(y|x)$:\n",
    "   - Bernoulli distribution: $p(y|x) = \\hat{y}^y (1 - \\hat{y})^{1-y}$\n",
    "   - If $y = 1$, $p(y|x) = \\hat{y}$.\n",
    "   - If $y = 0$, $p(y|x) = 1 - \\hat{y}$.\n",
    "\n",
    "2. **Log-Likelihood**:\n",
    "   - $\\log(p(y|x)) = y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})$\n",
    "   - We want to **maximize** log-likelihood or **minimize** negative log-likelihood (cross-entropy loss).\n",
    "\n",
    "\n",
    "### **Cross-Entropy Loss in Action**\n",
    "- **Example 1**: If $y = 1$ and $\\hat{y} = 0.7$:\n",
    "  - $‚Ñí_{CE} = -\\log(0.7) \\approx 0.36$\n",
    "  - Loss is small when $\\hat{y}$ is close to 1.\n",
    "- **Example 2**: If $y = 0$ and $\\hat{y} = 0.7$:\n",
    "  - $‚Ñí_{CE} = -\\log(1 - 0.7) = -\\log(0.3) \\approx 1.2$\n",
    "  - Loss is larger when the model is confused.\n",
    "\n",
    "\n",
    "### **Why Minimize Cross-Entropy?**\n",
    "- **Ideal Classifier**:\n",
    "  - For $y = 1$, we want $\\hat{y}$ to be as close to 1 as possible.\n",
    "  - For $y = 0$, we want $\\hat{y}$ to be as close to 0 as possible.\n",
    "- **Properties**:\n",
    "  - **Loss range**: From 0 (perfect prediction) to infinity (highly inaccurate).\n",
    "  - Ensures increasing the probability of the correct class decreases the probability of the wrong class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent   \n",
    "- **Goal:** Find the optimal parameters (weights `Œ∏={w,b}`) that minimize the loss function.  \n",
    "- **Cross-Entropy Loss Function (‚Ñí):**\n",
    "- $\\displaystyle \\hat{\\theta} = \\underset{\\theta}{\\text{argmin}} \\frac{1}{m} \\sum_{i=1}^{m} ‚Ñí(f(x^{(i)}; \\theta), y^{(i)})$  \n",
    "\n",
    "\n",
    "### How Gradient Descent Works   \n",
    "- **Concept:** Gradient descent identifies the steepest slope of the loss function and moves in the opposite direction.  \n",
    "- **Analogy:** Hiking in a canyon‚Äîdescend steeply towards the river.  \n",
    "- **Key Feature:** Logistic regression‚Äôs loss function is **convex**, meaning it has a unique global minimum.  \n",
    "- **Note:** Non-convex functions (e.g., neural networks) may have local minima.  \n",
    "\n",
    "\n",
    "### Gradient Descent - 1D Example   \n",
    "- **Concept:** Move left or right from an initial weight $w_1$ based on the gradient at that point.  \n",
    "- **Gradient:** The slope of the loss function at that point (informally the derivative).  \n",
    "- **Update Rule:**  \n",
    "- $\\displaystyle w_{t+1} = w_t - \\eta \\frac{d}{dw} L(f(x; w), y)$\n",
    "\n",
    "\n",
    "### Gradient Descent in Multiple Dimensions   \n",
    "- **Key Point:** In N-dimensional space, gradient expresses the slope across all dimensions.  \n",
    "- **Components:**  \n",
    "  - $\\displaystyle \\nabla L(f(x; \\theta), y) = \\left[\\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\ldots, \\frac{\\partial L}{\\partial w_n}, \\frac{\\partial L}{\\partial b}\\right]$  \n",
    "\n",
    "\n",
    "### Gradient Descent Update Rule  \n",
    "- **Update Formula:**\n",
    "- $\\displaystyle \\theta_{t+1} = \\theta_t - \\eta \\nabla L(f(x; \\theta), y)$  \n",
    "- **Learning Rate:** $\\eta$ controls step size, impacting convergence speed.  \n",
    "\n",
    "\n",
    "### Gradient for Logistic Regression  \n",
    "- **Cross-Entropy Loss Function:**\n",
    "- $\\displaystyle ‚Ñí(\\hat{y}, y) = -[y \\log \\sigma(w \\cdot x + b) + (1 - y) \\log (1 - \\sigma(w \\cdot x + b))]$\n",
    "- **Gradient for Logistic Regression:**\n",
    "- $\\displaystyle \\frac{\\partial ‚Ñí(\\hat{y}, y)}{\\partial w_j} = (\\hat{y} - y) x_j$\n",
    "- **Note:** Very intuitive: difference between the true label and predicted label times the input feature.\n",
    "\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)   \n",
    "- **Key Idea:** Updates weights based on one randomly chosen training example.  \n",
    "- **Update Rule:**  \n",
    "  - $\\displaystyle \\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(f(x^{(i)}; \\theta), y^{(i)})$\n",
    "- **Advantages:** Faster convergence, ideal for large datasets.  \n",
    "- **Disadvantages:** Noisy updates.   \n",
    "\n",
    "\n",
    "### Example of Gradient Descent  \n",
    "- **Given:** Initial weights $\\theta^{(0)} = (w_1, w_2, b)^T = (0,0,0)^T$, learning rate $\\eta = 0.1$  \n",
    "- **Feature Vector:** $x = [3, 2]$, true label $y = 1$.  \n",
    "- **Compute Gradient:**  \n",
    "  - $\\displaystyle \\nabla_{w_1, w_2, b} L = \\begin{bmatrix}\n",
    "\\frac{\\partial L_{CE}(y)}{\\partial w_1} \\\\\n",
    "\\frac{\\partial L_{CE}(y)}{\\partial w_2} \\\\\n",
    "\\frac{\\partial L_{CE}(y)}{\\partial b}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "(\\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b) - y)x_1 \\\\\n",
    "(\\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b) - y)x_2 \\\\\n",
    "\\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b) - y\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "(\\sigma(0) - 1)x_1 \\\\\n",
    "(\\sigma(0) - 1)x_2 \\\\\n",
    "\\sigma(0) - 1\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "-0.5x_1 \\\\\n",
    "-0.5x_2 \\\\\n",
    "-0.5\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "-1.5 \\\\\n",
    "-1.0 \\\\\n",
    "-0.5\n",
    "\\end{bmatrix}$\n",
    "- **Update One Step:**  \n",
    "  - $\\displaystyle \\theta^{(1)} = \\theta^{(0)} - \\eta \\nabla_{w_1, w_2, b} L\n",
    "= \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix} - \\eta \\begin{bmatrix}\n",
    "-1.5 \\\\\n",
    "-1.0 \\\\\n",
    "-0.5\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "0.15 \\\\\n",
    "0.1 \\\\\n",
    "0.05\n",
    "\\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "- **Stochastic Gradient Descent:**\n",
    "  - Chooses a single random example at a time.\n",
    "  - Moves weights to improve performance on that example.\n",
    "  - Results in choppy movements due to the randomness.\n",
    "\n",
    "- **Batch Gradient Descent:**\n",
    "  - Computes the gradient over the entire dataset.\n",
    "  - Offers a good estimate of the direction to move weights.\n",
    "  - Computationally expensive and slow.\n",
    "\n",
    "- **Mini-Batch Gradient Descent:**\n",
    "  - Compromise between Stochastic and Batch Gradient Descent.\n",
    "  - Uses a group of `m` examples (e.g., 512 or 1024) instead of the whole dataset.\n",
    "  - Computationally efficient and allows parallel processing.\n",
    "\n",
    "\n",
    "\n",
    "### Mini-Batch Training Strategy\n",
    "\n",
    "\n",
    "\n",
    "- **Definition:**\n",
    "  - Divide the dataset into small batches.\n",
    "  - Each batch contains `m` examples.\n",
    "  - For each batch, compute the loss and update weights.\n",
    "\n",
    "- **Benefits:**\n",
    "  - Efficient computation by vectorizing operations.\n",
    "  - Reduces variance in weight updates compared to Stochastic Gradient Descent.\n",
    "  - Faster convergence than Batch Gradient Descent.\n",
    "\n",
    "- **Trade-Off:**\n",
    "  - `m = dataset size`: Batch Gradient Descent.\n",
    "  - `m = 1`: Stochastic Gradient Descent.\n",
    "\n",
    "\n",
    "\n",
    "### Mini-Batch Cross-Entropy Loss\n",
    "\n",
    "\n",
    "\n",
    "- **For one example:**\n",
    "  - $\\displaystyle L_{CE}(\\hat{y}^{(i)}, y^{(i)}) = - y^{(i)} \\log(\\sigma(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)) - (1 - y^{(i)}) \\log(1 - \\sigma(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b))$\n",
    "\n",
    "- **For a mini-batch of size $m $:**\n",
    "  - $\\displaystyle\\text{Cost}(\\hat{y}, y) = \\frac{1}{m} \\sum_{i=1}^{m} L_{CE}(\\hat{y}^{(i)}, y^{(i)})$\n",
    "  - $\\displaystyle = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log \\sigma(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) + (1 - y^{(i)}) \\log(1 - \\sigma(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b)) \\right]$\n",
    "\n",
    "\n",
    "\n",
    "### Gradient of the Mini-Batch Cost Function\n",
    "\n",
    "\n",
    "\n",
    "- **Individual Gradient:**\n",
    "  - $\\displaystyle\\frac{\\partial L_{CE}(\\hat{y}, y)}{\\partial w_j} = \\left( \\sigma(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - y^{(i)} \\right) x^{(i)}_j$\n",
    "\n",
    "- **Mini-Batch Gradient:**\n",
    "  - $\\displaystyle\\frac{\\partial \\text{Cost}(\\hat{y}, y)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ \\sigma(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b) - y^{(i)} \\right] x^{(i)}_j$\n",
    "\n",
    "- **Matrix Form:**\n",
    "  - $\\displaystyle\\frac{\\partial \\text{Cost}(\\hat{ùê≤}, ùê≤)}{\\partial \\mathbf{w}} = \\frac{1}{m} \\left( \\hat{ùê≤} - ùê≤ \\right)^T \\mathbf{X} = \\frac{1}{m}(œÉ(ùêóùê∞+ùêõ) -ùê≤)^Tùêó$\n",
    "  \n",
    "  - where:\n",
    "    - $\\mathbf{X}$: Matrix of inputs (size [m √ó f]).\n",
    "    - $ùê≤$: Vector of correct outputs (size [m √ó 1]).\n",
    "\n",
    "\n",
    "\n",
    "### Mini-Batch Training Algorithm\n",
    "\n",
    "\n",
    "1. Initialize weights $\\mathbf{w}$ and bias $b$.\n",
    "2. **Repeat until convergence:**\n",
    "   - Shuffle the training data.\n",
    "   - Divide data into mini-batches of size $m$.\n",
    "   - For each mini-batch:\n",
    "     - Compute the predictions $\\hat{y}$.\n",
    "     - Calculate the cost function $\\text{Cost}(\\hat{y}, y)$.\n",
    "     - Compute the gradient $\\displaystyle\\frac{\\partial \\text{Cost}}{\\partial \\mathbf{w}}$.\n",
    "     - Update weights: $\\displaystyle\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\frac{\\partial \\text{Cost}}{\\partial \\mathbf{w}}$.\n",
    "     - Update bias: $\\displaystyle b \\leftarrow b - \\eta \\frac{\\partial \\text{Cost}}{\\partial b}$.\n",
    "\n",
    "\n",
    "### Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Cost: 0.6930\n",
      "Epoch 2/10 - Cost: 0.6930\n",
      "Epoch 3/10 - Cost: 0.6935\n",
      "Epoch 4/10 - Cost: 0.6940\n",
      "Epoch 5/10 - Cost: 0.6969\n",
      "Epoch 6/10 - Cost: 0.6927\n",
      "Epoch 7/10 - Cost: 0.6933\n",
      "Epoch 8/10 - Cost: 0.6919\n",
      "Epoch 9/10 - Cost: 0.6910\n",
      "Epoch 10/10 - Cost: 0.6965\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize parameters\n",
    "def initialize_parameters(n_features):\n",
    "    w = np.zeros((n_features, 1))\n",
    "    b = 0\n",
    "    return w, b\n",
    "\n",
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Compute cost and gradient for a mini-batch\n",
    "def compute_cost_and_gradient(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    z = np.dot(X, w) + b\n",
    "    y_hat = sigmoid(z)\n",
    "    \n",
    "    # Compute cost\n",
    "    cost = -(1/m) * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "    \n",
    "    # Compute gradients\n",
    "    dw = (1/m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    \n",
    "    return cost, dw, db\n",
    "\n",
    "# Mini-batch gradient descent\n",
    "def mini_batch_gradient_descent(X, y, batch_size, learning_rate, num_epochs):\n",
    "    n_features = X.shape[1]\n",
    "    w, b = initialize_parameters(n_features)\n",
    "    costs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data\n",
    "        permutation = np.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Compute cost and gradients\n",
    "            cost, dw, db = compute_cost_and_gradient(X_batch, y_batch, w, b)\n",
    "            \n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * dw\n",
    "            b -= learning_rate * db\n",
    "            \n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print cost for every epoch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Cost: {cost:.4f}\")\n",
    "    \n",
    "    return w, b, costs\n",
    "\n",
    "# Example usage with random data\n",
    "X = np.random.rand(1000, 3)  # 1000 examples, 3 features\n",
    "y = np.random.randint(0, 2, (1000, 1))  # Binary target\n",
    "w, b, costs = mini_batch_gradient_descent(X, y, batch_size=64, learning_rate=0.01, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plan for creating the slides:\n",
    "\n",
    "## Overfitting and Regularization in Machine Learning\n",
    "\n",
    "### **What is Overfitting?**\n",
    "- **Phenomenon**: \"Overfitting occurs when a model learns the training data too well, capturing noise or random fluctuations.\"\n",
    "- **Causes**: High weights for features that happen to correlate with the class.\n",
    "- **Consequence**: Poor generalization to new, unseen data.\n",
    "\n",
    "\n",
    "### **Regularization: Controlling Overfitting**\n",
    "- Regularization involves adding a `penalty term` to the loss function to `prevent large weights`.\n",
    "- Helps the model to generalize better by avoiding complex models that fit the training data too well.\n",
    "- Regularized Loss Function:\n",
    "  - $\\displaystyle\\hat{\\theta} = \\arg\\max_{\\theta} \\sum_{i=1}^{m} \\log P(y^{(i)} | x^{(i)}) - \\alpha R(\\theta)$\n",
    "  - $R(\\theta)$ penalizes large weights.\n",
    "- Goal: Optimize weights $\\theta$ while considering regularization term $R(\\theta)$.\n",
    "\n",
    "\n",
    "### **L2 Regularization (Ridge Regression)**\n",
    "- Definition: Penalizes the sum of the squared weights.\n",
    "- Formula: $\\displaystyle R(\\theta) = ||\\theta||_2^2 = \\sum_{j=1}^{n} \\theta_j^2$\n",
    "- Characteristics: Prefers many small weights.\n",
    "- Regularized Loss Function: \n",
    "  - $\\displaystyle\\hat{\\theta} = \\arg\\max_{\\theta} \\sum_{i=1}^{m} \\log P(y^{(i)} | x^{(i)}) - \\alpha \\sum_{j=1}^{n} \\theta_j^2$\n",
    "\n",
    "\n",
    "### **L1 Regularization (Lasso Regression)**\n",
    "- Definition: Penalizes the sum of absolute weights.\n",
    "- Formula: $\\displaystyle R(\\theta) = ||\\theta||_1 = \\sum_{j=1}^{n} |\\theta_j|$\n",
    "- Characteristics: Leads to sparse weight vectors with many zeros.\n",
    "- Regularized Loss Function:\n",
    "  - $\\displaystyle\\hat{\\theta} = \\arg\\max_{\\theta} \\sum_{i=1}^{m} \\log P(y^{(i)} | x^{(i)}) - \\alpha \\sum_{j=1}^{n} |\\theta_j|$\n",
    "\n",
    "\n",
    "### **Comparison of L1 and L2 Regularization**\n",
    "- **L1 Regularization:** Prefers sparse solutions with some weights at zero.\n",
    "- **L2 Regularization:** Prefers solutions with small weights, but not necessarily zero.\n",
    "- **Optimization:** L2 is easier to optimize due to simpler derivative.\n",
    "\n",
    "\n",
    "### **Bayesian Interpretation of Regularization**\n",
    "- L1 Regularization: Laplace prior on weights.\n",
    "  - $\\displaystyle P(\\theta) = \\frac{1}{2b} e^{\\left(-\\frac{|\\theta|}{b}\\right)}$\n",
    "- L2 Regularization: Gaussian prior with mean $\\mu = 0$.\n",
    "  - $\\displaystyle P(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\left(-\\frac{\\theta^2}{2\\sigma^2}\\right)}$\n",
    "- Bayesian View: Regularization constrains the model's complexity based on prior beliefs of parameters.\n",
    "\n",
    "\n",
    "### **Gaussian Prior for L2 Regularization**\n",
    "- Formula: \n",
    "  - $\\displaystyle\\hat{\\theta} = \\arg\\max_{\\theta} \\prod_{i=1}^{m} P(y^{(i)} | x^{(i)}) \\times \\prod_{j=1}^{n} \\sqrt{\\frac{1}{2\\pi\\sigma_j^2}} e^{\\left(-\\frac{(\\theta_j - \\mu_j)^2}{2\\sigma_j^2}\\right)}$\n",
    "- which in `log space`, with $Œº = 0$, and assuming $2œÉ^2 = 1$, simplified to:\n",
    "  - $\\displaystyle\\hat{\\theta} = \\arg\\max_{\\theta} \\sum_{i=1}^{m} \\log P(y^{(i)} | x^{(i)}) - \\alpha \\sum_{j=1}^{n} \\theta_j^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multinomial Logistic Regression Loss Function**\n",
    "- Loss function for $K$ classes generalizes the binary case:\n",
    "  - $\\displaystyle L_{CE}(\\hat{y}, y) = - \\sum_{k=1}^{K} y_k \\log \\hat{y}_k$\n",
    "- Only the correct class $c$ contributes to the **negative log likelihood loss**:\n",
    "  - $\\displaystyle L_{CE}(\\hat{y}, y) = -\\log \\hat{y}_c$\n",
    "- The cross-entropy loss simplifies to:\n",
    "  - $\\displaystyle L_{CE} = - \\log p(y_c = 1 | x) = -\\log \\frac{e^{(w_c \\cdot x + b_c)}}{\\sum_{j=1}^{K} e^{(w_j \\cdot x + b_j)}}$\n",
    "\n",
    "\n",
    "### **Gradient of the Loss Function**\n",
    "- The gradient for multinomial logistic regression is similar to the binary case:\n",
    "  - $\\displaystyle \\frac{\\partial L_{CE}}{\\partial w_{k,i}} = - (y_k - \\hat{y}_k) x_i$\n",
    "- Represents the difference between true class and predicted probability.\n",
    "\n",
    "- For class $k$, the gradient indicates:\n",
    "  - If $y_k = 1$ (true class), the gradient is negative, reducing the loss.\n",
    "  - If $y_k = 0$ (incorrect class), the gradient is positive, increasing the loss.\n",
    "- Gradient guides the update of weights during training.\n",
    "\n",
    "\n",
    "  - $\\displaystyle \\frac{\\partial L_{CE}}{\\partial w_{k,i}} = - \\left( y_k - \\dfrac{e^{(w_k \\cdot x + b_k)}}{\\sum_{j=1}^{K} e^{(w_j \\cdot x + b_j)}}\\right) \\cdot x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Interpretability**\n",
    "- Interpretability means humans can understand why an algorithm reached its conclusions.\n",
    "- For logistic regression, interpretability often involves understanding the role of each feature.\n",
    "  - Features in logistic regression are human-designed.\n",
    "  - The magnitude of weights $|w|$ tells us how much each feature influences the decision.\n",
    "\n",
    "\n",
    "### **Statistical Tests for Feature Significance**\n",
    "- **Likelihood Ratio Test**: Compares two models to see if adding a feature improves fit.\n",
    "- **Wald Test**: Assesses the significance of individual coefficients.\n",
    "- Both tests help determine if a feature is meaningful.\n",
    "- Logistic regression helps `test hypotheses about features` in text, such as:\n",
    "  - Are negative words (e.g., *no*, *not*, *never*) linked to negative sentiment?\n",
    "  - Do negative movie reviews focus more on cinematography?\n",
    "\n",
    "\n",
    "\n",
    "### **Controlling for Confounding Variables**\n",
    "- Logistic regression can isolate the effect of a feature by controlling for others (e.g., genre, review length).\n",
    "- Helps ensure valid conclusions about relationships between features and outcomes.\n",
    "- **Real-World Examples**\n",
    "  - **Healthcare:** Predicting hospital readmissions while controlling for patient age.\n",
    "  - **Politics:** Studying linguistic features and voting patterns, accounting for county demographics.\n",
    "  - **Marketing:** Understanding product sales while considering brand and price."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
