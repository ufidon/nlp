{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/11.apps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/11.apps.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# NLP APPLICATIONS\n",
    "\n",
    "- üìù SALP chapter 13-16\n",
    "- üìù [HuggingFace NLP course](https://huggingface.co/learn/nlp-course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üî≠ NLP Theory in Action: HuggingFace NLP\n",
    "- [Explore Huggingface](https://huggingface.co/)\n",
    "- [Explore its repositories](https://github.com/huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Huggingface core libraries\n",
    "!pip install tokenizers transformers datasets accelerate\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° HuggingFace Transformers\n",
    "### NLP Pipeline\n",
    "- The `pipeline()` function in the ü§ó Transformers library simplifies NLP tasks by \n",
    "  - combining model selection with preprocessing and postprocessing steps, \n",
    "  - enabling easy input of text and retrieval of results.\n",
    "- When text is passed to a pipeline, it goes through `three` main steps: \n",
    "  - preprocessing into a model-compatible format, \n",
    "  - running through the model, \n",
    "  - and post-processing to make predictions understandable.\n",
    "- Available pipelines: \n",
    "  - feature-extraction (get the vector representation of a text)\n",
    "  - fill-mask, question-answering\n",
    "  - ner (named entity recognition), sentiment-analysis by zero-shot-classification\n",
    "  - summarization, text-generation, translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sentiment analysis by zero-shot-classification\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "\n",
    "# multiple statements\n",
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `zero-shot-classification pipeline` enables classifying unlabeled text by specifying custom labels\n",
    "  - which is useful in scenarios where labeling data is difficult or time-consuming.\n",
    "- This pipeline is called \"zero-shot\" because it doesn‚Äôt require model fine-tuning on specific data\n",
    "  - it can assign probability scores for any label set directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Text generation\n",
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, I learned how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The text-generation pipeline allows you to input a prompt, and the model completes it by generating the rest of the text, similar to predictive text on phones.\n",
    "- You can customize the output by setting the number of sequences generated `num_return_sequences` and controlling the output length `max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Customizing the generator with specified model instead of the default\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, I learned how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can choose [specific models](https://huggingface.co/models?pipeline_tag=text-generation) from the Hugging Face Model Hub for various tasks, filtering by task and language to find models suited for specific needs.\n",
    "- The Hub provides an online widget to test models directly, allowing quick previews of model capabilities before downloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. `Mask filling:` `fill-mask` fills in the blanks in a given text:\n",
    "# mask-filling models might have different mask tokens\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Named entity recognition\n",
    "# one type of part-of-speech tagging (POS) \n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"Begin your journey at the majestic Lincoln Memorial, stroll past reflecting pools to the Smithsonian museums, explore Capitol Hill, and end with sunset views of the White House.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setting `grouped_entities=True` in the pipeline groups parts of a sentence that belong to the same entity, \n",
    "  - allowing `multi-word names` like \"Lincoln Memorial\" to be recognized as a single entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Question answering\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Who will be the next president?\",\n",
    "    context=\"The U.S. presidential campaign between Kamala Harris and Donald Trump was intense, highlighting sharp divides on policy and vision. Harris focused on progressive reforms, while Trump emphasized traditional values. Both candidates rallied fervent supporters, underscoring contrasting paths for America's future.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Summarization\n",
    "article = \"\"\"\n",
    "The U.S. presidential race between Kamala Harris and Donald Trump was \n",
    "a highly charged contest that underscored deep ideological divides. \n",
    "Harris championed progressive policies, emphasizing social equity, \n",
    "climate action, and healthcare expansion. She aimed to build on her \n",
    "party‚Äôs recent reforms, appealing to a younger, diverse electorate. \n",
    "Trump, in contrast, doubled down on conservative principles, \n",
    "prioritizing national security, economic growth, and traditional values, \n",
    "rallying his core base with a focus on restoring a sense of American identity. \n",
    "Both candidates energized their supporters with a vision of the country‚Äôs future, \n",
    "setting up a pivotal choice for voters on Election Day.\n",
    "\"\"\"\n",
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Translation: French to English\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Chinese to English\n",
    "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "pipe(\"\"\"\n",
    "„ÄäÂ§úÂÆøÂ±±ÂØ∫„Äã\n",
    "     Âîê‰ª£ÊùéÁôΩ\n",
    "Âç±Ê•ºÈ´òÁôæÂ∞∫ÔºåÊâãÂèØÊëòÊòüËæ∞„ÄÇ\n",
    "‰∏çÊï¢È´òÂ£∞ËØ≠ÔºåÊÅêÊÉäÂ§©‰∏ä‰∫∫„ÄÇ     \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer mechanism\n",
    "- Transformer models can be broadly grouped into three categories:\n",
    "  - `Auto-regressive` models like GPT\n",
    "  - `Auto-encoding` models like BERT\n",
    "  - `Sequence-to-sequence` models like BART/T5\n",
    "\n",
    "### Transformers are language models\n",
    "- Transformer models are trained using `self-supervised learning` on raw text, \n",
    "  - developing a statistical understanding without human-labeled data.\n",
    "- For practical tasks, these pretrained models undergo `transfer learning` \n",
    "  - fine-tuned with human-labeled data to perform specific tasks,\n",
    "  - such as `causal or masked` language modeling.\n",
    "\n",
    "### Transformers are big models\n",
    "- Increasing model size and data generally improves performance but is costly in time, compute resources, and environmental impact.\n",
    "- Sharing pretrained models helps reduce global costs by avoiding the need to retrain from scratch for each project.\n",
    "- Tools like `ML CO2 Impact` and `Code Carbon` can help measure a model's carbon footprint, promoting awareness and efficiency in model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "- Pretraining initializes a model from scratch with random weights on large datasets, requiring significant time and resources.\n",
    "- Fine-tuning builds on pretrained models, needing less data, time, and cost by adapting knowledge to specific tasks.\n",
    "- Fine-tuning is efficient and yields better results than training from scratch, especially for models closely aligned with the task.\n",
    "\n",
    "### General Architecture\n",
    "- The Transformer model architecture has two main components: \n",
    "  - an encoder for understanding input and \n",
    "  - a decoder for generating output based on this understanding.\n",
    "- Specific configurations include encoder-only for tasks like classification, \n",
    "  - decoder-only for generation, and encoder-decoder for tasks like translation.\n",
    "- Attention layers in Transformers focus on relevant words for each prediction, crucial for tasks like translation.\n",
    "  - Context impacts word meaning, requiring the model to focus on nearby or distant words as needed.\n",
    "- Transformers‚Äô encoder can consider the full input sentence, while the decoder sequentially builds the output.\n",
    "  - During training, the decoder sees limited past outputs to improve prediction difficulty and accuracy.\n",
    "  - Attention masking manages word relevance, helping with padding and structural differences between languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures vs. checkpoints\n",
    "- **Architecture**: Defines the model structure, including layers and operations.\n",
    "- **Checkpoints**: Saved weights trained on specific data for an architecture.\n",
    "- **Model**: Broad term referring to either architecture, checkpoint, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder models\n",
    "- Known for `bi-directional` attention, focus on understanding the entire sentence context and are typically used for tasks like classification and question answering.\n",
    "- They are pretrained by `reconstructing corrupted sentences`, with models like BERT, ALBERT, and RoBERTa as key examples.\n",
    "\n",
    "### Decoder models\n",
    "- Using only the Transformer decoder, are `auto-regressive` and `predict the next word` based on previous words, making them ideal for text generation tasks.\n",
    "- Examples include models like GPT and Transformer XL, which excel in generating coherent text by training on sequential word prediction.\n",
    "\n",
    "### Encoder-decoder models \n",
    "- or sequence-to-sequence models, use both Transformer parts, \n",
    "  - with the encoder capturing full input context and the decoder focusing on sequential output generation.\n",
    "- These models, like T5 and BART, are well-suited for tasks such as summarization, translation, and question answering by generating responses based on specific inputs.\n",
    "\n",
    "### Bias and limitations\n",
    "- Transformer models are trained on large datasets that include both high- and low-quality content, leading to potential biases in generated responses.\n",
    "- Despite fine-tuning, models may still produce biased, offensive outputs, so users should be cautious of these limitations in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias and limitations\n",
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "| Model     | Examples   | Tasks       |\n",
    "|------------------|-------|-----------------|\n",
    "| **Encoder**      | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Sentence classification, named entity recognition, extractive question answering |\n",
    "| **Decoder**      | CTRL, GPT, GPT-2, Transformer XL        | Text generation           |\n",
    "| **Encoder-decoder** | BART, T5, Marian, mBART      | Summarization, translation, generative question answering |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ Explore and practice \n",
    "- The rest of the [ü§ó NLP Course](https://huggingface.co/learn/nlp-course/chapter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [‚öîÔ∏è Chatbot Arena (formerly LMSYS): Free AI Chat to Compare & Test Best AI Chatbots](https://lmarena.ai/)\n",
    "  - [openplayground: An LLM playground you can run on your laptop](https://github.com/nat/openplayground)\n",
    "- [LLM Zoo: democratizing ChatGPT](https://github.com/FreedomIntelligence/LLMZoo)\n",
    "- [DSPy: Programming‚Äînot prompting‚ÄîFoundation Models](https://github.com/stanfordnlp/dspy)\n",
    "  - [LLM Engine: fine-tuning and serving large language models](https://github.com/scaleapi/llm-engine)\n",
    "- [vLLM: a fast and easy-to-use library for LLM inference and serving](https://github.com/vllm-project/vllm)\n",
    "  - [ollama: Get up and running with large language models](https://github.com/ollama/ollama)\n",
    "  - [llama.cpp: LLM inference in C/C++](https://github.com/ggerganov/llama.cpp)\n",
    "- [Browse State-of-the-Art](https://paperswithcode.com/sota)\n",
    "  - [Practical Deep Learning](https://course.fast.ai/)\n",
    "  - [deeplearning.ai](https://www.deeplearning.ai/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
