{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/qair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/qair.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "**Question Answering, Information Retrieval, and Retrieval Augmented Generation**\n",
    "\n",
    "- üìù SALP chapter 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- `Question answering (QA) systems` have supported human information needs since the 1960s, advancing over time with innovations like IBM‚Äôs Watson, which surpassed human performance on \"Jeopardy!\" in 2011.\n",
    "  - They are integrated with search engines and modern large language models (LLMs), enhancing their ability to answer diverse queries.\n",
    "\n",
    "- `Prompt-based methods` in modern QA use pretrained LLMs to directly answer fact-based questions from stored knowledge.\n",
    "  - LLMs often produce `hallucinations`, confidently giving incorrect answers, especially in specialized domains, due to poor calibration.\n",
    "  - `Simple prompting LLMs` limits their ability to access proprietary or real-time data, as they cannot update information after training.\n",
    "  - `Retrieval-augmented generation (RAG)` overcomes these issues by combining `document retrieval` with LLMs, grounding answers in relevant, curated data and enabling responses based on proprietary information.\n",
    "  - RAG uses `information retrieval techniques`, from tf-idf to neural models like BERT, to select relevant documents, enhancing answer reliability and context-awareness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "- `Information retrieval (IR)` is the field focused on retrieving media based on user needs,\n",
    "  -  with IR systems often functioning as `search engines`; \n",
    "  -  `ad hoc retrieval` is a core IR task where users enter queries to find ordered sets of relevant documents.\n",
    "\n",
    "![The architecture of an ad hoc IR system](./images/qa/ir.png)\n",
    "\n",
    "- Key components of IR include \n",
    "  - documents: text units like web pages or articles, \n",
    "  - collections: sets of documents, \n",
    "  - terms: words or phrases in documents, and \n",
    "  - queries: user-expressed needs.\n",
    "\n",
    "- IR systems commonly use the `vector space` model, \n",
    "  - representing documents and queries as vectors based on word counts and ranking them by cosine similarity, \n",
    "  - following a bag-of-words approach where word positions are disregarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term weighting and document scoring\n",
    "- IR scores a document‚Äôs match with a query by assigning term weights to each word, commonly using weighting schemes:\n",
    "  - [tf-idf](https://scikit-learn.org/1.5/modules/feature_extraction.html): term-frequency ‚Äî inverse document-frequency\n",
    "  - [BM25](https://huggingface.co/blog/xhluca/bm25s): Best Match 25\n",
    "\n",
    "- **Tf-idf** is the product of term frequency (**tf**) and inverse document frequency (**idf**): \n",
    "  - **Term frequency (tf)** measures word frequency in a document, using a logarithmic scale to avoid overemphasis on high counts:\n",
    "    - $tf_{t, d} = \n",
    "    \\begin{cases} \n",
    "      1 + \\log_{10}(\\text{count}(t, d)) & \\text{if count}(t, d) > 0 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "   \\end{cases}$\n",
    "\n",
    "  - **Inverse Document Frequency (idf)** measures how informative a term is across documents, `decreasing` for terms appearing in many documents:\n",
    "    - $idf_t = \\log_{10} \\dfrac{N}{df_t}$\n",
    "    where $N$ is the total number of documents, and $df_t$ is the number of documents containing term $t$.\n",
    "- The **tf-idf** score for a word $t$ in document $d$ is computed by multiplying `tf` and `idf`:\n",
    "  - $\\text{tf-idf}(t, d) = tf_{t, d} \\cdot idf_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document scoring\n",
    "- Document scoring uses the **cosine similarity** between the query vector $ùê™$ and document vector $ùêù$, computed as:\n",
    "  - $\\text{score}(ùê™, ùêù) = \\cos(ùê™, ùêù) = \\dfrac{ùê™ \\cdot ùêù}{|ùê™||ùêù|}$\n",
    "\n",
    "  - Cosine similarity can also be viewed as the **dot product of unit vectors** by normalizing both vectors to unit length before calculation:\n",
    "    - $\\text{score}(ùê™, ùêù) = \\cos(ùê™, ùêù) = \\left(\\dfrac{ùê™}{|ùê™|}\\right) \\cdot \\left(\\dfrac{ùêù}{|ùêù|}\\right)$\n",
    "  - In `tf-idf`:\n",
    "    - $\\text{score}(ùê™, ùêù) = \\sum_{t \\in ùê™} \\frac{\\text{tf-idf}(t, ùê™)}{\\sqrt{\\sum_{q_i \\in ùê™} \\text{tf-idf}^2(q_i, ùê™)}} \\cdot \\frac{\\text{tf-idf}(t, ùêù)}{\\sqrt{\\sum_{d_i \\in ùêù} \\text{tf-idf}^2(d_i, ùêù)}}$\n",
    "\n",
    "- Simplifications and **variants of tf-idf cosine** scoring are used in practice, such as omitting the **idf term** in the document to enhance performance.\n",
    "\n",
    "  - **BM25** is an advanced variant of tf-idf that introduces two parameters\n",
    "    - **k** (balancing term frequency and IDF) \n",
    "    - **b** (adjusting document length normalization)‚Äîto refine scoring.\n",
    "\n",
    "- **Stop lists** (lists of high-frequency words like \"the,\" \"a\") were traditionally used to exclude common terms from queries and documents, reducing index size; \n",
    "  - modern IR systems now rarely use stop lists due to advancements in IDF weighting and processing efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Inverted Index](https://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html)\n",
    "- In information retrieval, the goal is to find documents containing query terms, ignoring those with none of the terms.\n",
    "\n",
    "- An `inverted index` structure, with a dictionary and postings lists, efficiently identifies relevant documents and stores term frequencies and positions.\n",
    "\n",
    "- The dictionary links terms to postings lists, which provide document IDs and term-related data for fast score computation.\n",
    "\n",
    "- Alternatives, such as bigram indexing and hashing, can improve efficiency in tasks like finding Wikipedia pages for question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval with Dense Vectors\n",
    "- Traditional IR methods like tf-idf and BM25 have the **Vocabulary Mismatch Problem**\n",
    "  - They require `exact word overlap` between query and document, which limits retrieval when `synonyms` are used.\n",
    "\n",
    "  - `Dense embeddings`, such as those generated by BERT, address this issue by encoding semantic meaning, rather than relying on exact word matches.\n",
    "\n",
    "- **Single encoder (a)**: The query and document are fed into a BERT model, with a linear layer on the [CLS] token to predict similarity, allowing context-sensitive matching.\n",
    "  - $\\text{score}(ùê™,ùêù) = \\text{softmax}(ùêî(ùê≥))$\n",
    "    - $ùê≥ = \\text{BERT}(ùê™;[\\text{SEP}];ùêù)[\\text{CLS}]$\n",
    "  - Documents are split into short passages (e.g., 100 tokens) to fit within BERT's 512-token limit, and the model is fine-tuned on a relevance dataset for better retrieval accuracy.\n",
    "  - `Single encoder` uses a `full BERT encoder` for each query-document pair, which is computationally expensive, \n",
    "    - as it requires encoding each document in the collection with every new query.\n",
    "\n",
    "![Two ways to do dense retrieval](./images/qa/qdbert.png)\n",
    "\n",
    "\n",
    "- **Bi-Encoder (b)** encodes each document only once and precomputes document vectors, enabling `fast` query processing by computing dot products between the query vector and precomputed document vectors.\n",
    "  - $\\text{score}(ùê™,ùêù) = ùê≥_ùê™ ‚ãÖ ùê≥_ùêù$\n",
    "    - $ùê≥_ùê™ = \\text{BERT}_Q(ùê™)[\\text{CLS}]$\n",
    "    - $ùê≥_ùêù = \\text{BERT}_D(ùêù)[\\text{CLS}]$\n",
    "  - However, it sacrifices some accuracy by not capturing detailed token-level interactions between the query and document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numerous approaches between the full encoder and the bi-encoder balance accuracy and efficiency by \n",
    "  - using cheaper ranking methods (e.g., BM25) to initially rank documents \n",
    "  - then applying costly BERT scoring to rerank only the top-ranked ones.\n",
    "\n",
    "- [ColBERT (Contextualized Late Interaction over BERT)](https://huggingface.co/colbert-ir/colbertv2.0) encodes the query and document separately into token-level representations, allowing it to pre-store document representations for faster scoring.\n",
    "\n",
    "- ![A sketch of the ColBERT algorithm at inference time](./images/qa/colbert.png)\n",
    "\n",
    "  - It computes relevance by summing the maximum similarity between each query token and the most contextually similar document token, highlighting `token-level contextual similarity`.\n",
    "  - Query and document tokens are processed with BERT, including special `[Q]` and `[D]` tokens, and then scaled to unit length, optimizing vector size for storage efficiency.\n",
    "  - It requires end-to-end training, fine-tuning BERT encoders and linear layers using positive and negative document pairs, optimizing relevance scoring through cross-entropy loss.\n",
    "\n",
    "- Training data for supervised methods like ColBERT often includes labeled positive and negative passages, with semi-supervised approaches or iterative methods used when labeled data is sparse.\n",
    "\n",
    "- Efficient ranking of dense vectors uses `approximate nearest neighbor search algorithms` (e.g., [Faiss](https://huggingface.co/docs/datasets/en/faiss_es)) to quickly find the most similar document vectors for a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Questions with Retrieval-Augmented Generation\n",
    "- The retrieval-augmented generation (RAG) approach to question answering involves \n",
    "  - retrieving relevant text segments (retriever) \n",
    "  - generating an answer based on these documents (reader).\n",
    "\n",
    "![Retrieval-based question answering](./images/qa/rag.png)\n",
    "\n",
    "- This two-stage model first uses dense retrievers to find supportive passages and then employs a large language model to generate the answer from the retrieved content, one token at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "- RAG uses retrieved passages to help a language model generate answers, addressing limitations of simple conditional generation.\n",
    "  - simple autoregressive language modeling: \n",
    "    - $\\displaystyle p(x_1,‚ãØ,x_n)=‚àè_{i=1}^n p([Q:]; q; [A:]; x_{<i})$ \n",
    "- In RAG, a language model is conditioned on both the question and retrieved passages to reduce issues like hallucinations and provide evidence-based answers.\n",
    "  - Simple prompting can work for basic fact-based questions, but RAG `improves reliability` by using `specific prompts` like \"Based on these texts, answer this question.\"\n",
    "  - $\\displaystyle p(x_1,‚ãØ,x_n)=‚àè_{i=1}^n p(x_i|R(q);\\text{prompt};[Q:]; q; [A:]; x_{<i})$ \n",
    "- Effective RAG requires a well-performing retriever, often utilizing a `two-stage process` that ranks retrieved passages to improve accuracy.\n",
    "  - For complex questions, RAG may use `multi-hop retrieval`, combining initial retrieval results with follow-up searches for more context.\n",
    "- `Privacy and prompt engineering` are important considerations, especially in combining private and public data, with ongoing research focusing on refining the integration of retrieval and generation stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering Datasets\n",
    "- `Question-answering (QA) datasets` support both training and evaluating language models.  They vary by purpose, \n",
    "  - targeting natural information-seeking `questions`\n",
    "  - `probing` system knowledge and reasoning.\n",
    "- Natural question datasets include \n",
    "  - [Natural Questions](https://huggingface.co/datasets/google-research-datasets/natural_questions): based on Google queries, with answers derived from Wikipedia\n",
    "  - [MS MARCO (Microsoft Machine Reading Comprehension)](https://huggingface.co/datasets/microsoft/ms_marco): Bing queries with human-generated answers and passages.\n",
    "- Non-English natural question datasets include \n",
    "  - [DuReader](https://paperswithcode.com/dataset/dureader): Chinese search engine queries\n",
    "  - [TyDi QA](https://huggingface.co/datasets/google-research-datasets/tydiqa): questions in 11 languages from Wikipedia passages.\n",
    "- Probing datasets, such as [MMLU (Massive Multitask Language Understanding)](https://paperswithcode.com/dataset/mmlu), assess knowledge across multiple fields like medicine and computer science, using questions sourced from exams.\n",
    "- Some datasets augment questions with passages, enabling reading comprehension tasks that require extracting answers from provided texts.\n",
    "\n",
    "- QA tasks vary: \n",
    "  - `open book` QA uses retrieval-augmented generation, \n",
    "  - `closed book` QA answers directly from the model without retrieval.\n",
    "- Answer formats differ, including multiple-choice and freeform, affecting model requirements for generating or selecting responses.\n",
    "\n",
    "- Prompting styles in QA, impacting model performance on complex questions, can be \n",
    "  - zero-shot (only the question) \n",
    "  - few-shot (with examples).\n",
    "- `MMLU` supports both zero-shot and few-shot prompting, making it suitable for testing various model capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Question Answering\n",
    "- **Exact Match**: For multiple-choice questions (like in MMLU), QA systems are evaluated by the `percentage of answers that exactly match the correct answer`.\n",
    "\n",
    "- **Token F1 Score**: For free-text questions (e.g., Natural Questions), token F1 is used to measure `partial overlap between the predicted answer and the reference`, treating them as bags of tokens and averaging scores across all questions.\n",
    "\n",
    "- **Mean Reciprocal Rank (MRR)**: For systems providing ranked answers, MRR evaluates the rank of the first correct answer, scoring each question based on `the reciprocal of this rank` and averaging scores across questions.\n",
    "  - If no correct answer is given, the score for that question is zero, with some versions of MRR excluding these zero-score questions from the final calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-üèÉ Practice [Question answering from HuggingFace NLP](https://huggingface.co/learn/nlp-course/en/chapter7/7?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install required libraries\n",
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of the dataset\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. check the first element\n",
    "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
    "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
    "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])\n",
    "# In the Answer, The text field is rather obvious, \n",
    "# and the answer_start field contains the `starting character index` of each answer in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is only one possible answer in each training record\n",
    "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but there may be several answers in each evaluation record\n",
    "print(raw_datasets[\"validation\"][0][\"context\"])\n",
    "print(raw_datasets[\"validation\"][0][\"question\"])\n",
    "print(raw_datasets[\"validation\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_datasets[\"validation\"][2][\"context\"])\n",
    "print(raw_datasets[\"validation\"][2][\"question\"])\n",
    "print(raw_datasets[\"validation\"][2][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Processing the training data\n",
    "# 4.1 prepare a [fast tokenizer](https://huggingface.co/docs/transformers/index)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the tokenizer is really fast\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = raw_datasets[\"train\"][0][\"context\"]\n",
    "question = raw_datasets[\"train\"][0][\"question\"]\n",
    "\n",
    "# 4.2 The tokenizer will properly insert the special tokens to form a sentence like this:\n",
    "# [CLS] question [SEP] context [SEP]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])\n",
    "# The labels are the index of the tokens starting and ending the answer, \n",
    "# and the model will be tasked to predicted one start and end logit \n",
    "# per token in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 deal long contexts with a sliding windows\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    ")\n",
    "\n",
    "for ids in inputs[\"input_ids\"]:\n",
    "    print(tokenizer.decode(ids))\n",
    "\n",
    "# The example is split into four inputs \n",
    "# each contains the question and some part of the context \n",
    "# some do not contain the answer\n",
    "#   the labels will be start_position = end_position = 0 \n",
    "#   so we predict the [CLS] token\n",
    "# nonzero (start_position, end_position) if the answer is in the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 overflow_to_sample_mapping and offset mappings\n",
    "# https://huggingface.co/learn/nlp-course/chapter6/4\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"overflow_to_sample_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more examples give more useful result\n",
    "inputs = tokenizer(\n",
    "    raw_datasets[\"train\"][2:6][\"question\"],\n",
    "    raw_datasets[\"train\"][2:6][\"context\"],\n",
    "    max_length=100,\n",
    "    truncation=\"only_second\",\n",
    "    stride=50,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "\n",
    "print(f\"The 4 examples gave {len(inputs['input_ids'])} features.\")\n",
    "print(f\"Here is where each comes from: {inputs['overflow_to_sample_mapping']}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 find the answer\n",
    "answers = raw_datasets[\"train\"][2:6][\"answers\"]\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "\n",
    "for i, offset in enumerate(inputs[\"offset_mapping\"]):\n",
    "    sample_idx = inputs[\"overflow_to_sample_mapping\"][i]\n",
    "    answer = answers[sample_idx]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label is (0, 0)\n",
    "    if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer in the context\n",
    "idx = 0\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "start = start_positions[idx]\n",
    "end = end_positions[idx]\n",
    "labeled_answer = tokenizer.decode(inputs[\"input_ids\"][idx][start : end + 1])\n",
    "\n",
    "print(f\"Theoretical answer: {answer}, labels give: {labeled_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer not in the context\n",
    "idx = 4\n",
    "sample_idx = inputs[\"overflow_to_sample_mapping\"][idx]\n",
    "answer = answers[sample_idx][\"text\"][0]\n",
    "\n",
    "decoded_example = tokenizer.decode(inputs[\"input_ids\"][idx])\n",
    "print(f\"Theoretical answer: {answer}, decoded example: {decoded_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 put all preprocess together\n",
    "max_length = 384\n",
    "stride = 128\n",
    "\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\", #!!!!\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.6 apply the preprocessing to the whole training set\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"].map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"train\"]), len(train_dataset)\n",
    "# the preprocessing added roughly 1,000 samples\n",
    "# i.e. many contexts are quite long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Processing the validation data\n",
    "# preprocessing function\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the preprocessing function\n",
    "validation_dataset = raw_datasets[\"validation\"].map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")\n",
    "len(raw_datasets[\"validation\"]), len(validation_dataset)\n",
    "# a couple of hundred samples are added\n",
    "# i.e. most of the contexts in the validation set are short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Fine-tuning the model with the Trainer API\n",
    "# 6.1 a taste on a small validation set\n",
    "small_eval_set = raw_datasets[\"validation\"].select(range(100))\n",
    "trained_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "eval_set = small_eval_set.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"validation\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "# remove columns not needed by the model\n",
    "eval_set_for_model = eval_set.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "eval_set_for_model.set_format(\"torch\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "batch = {k: eval_set_for_model[k].to(device) for k in eval_set_for_model.column_names}\n",
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(trained_checkpoint).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format conversion\n",
    "start_logits = outputs.start_logits.cpu().numpy()\n",
    "end_logits = outputs.end_logits.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "example_to_features = collections.defaultdict(list)\n",
    "for idx, feature in enumerate(eval_set):\n",
    "    example_to_features[feature[\"example_id\"]].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 pick the answer with the best logit score\n",
    "import numpy as np\n",
    "\n",
    "n_best = 20\n",
    "max_answer_length = 30\n",
    "predicted_answers = []\n",
    "\n",
    "for example in small_eval_set:\n",
    "    example_id = example[\"id\"]\n",
    "    context = example[\"context\"]\n",
    "    answers = []\n",
    "\n",
    "    for feature_index in example_to_features[example_id]:\n",
    "        start_logit = start_logits[feature_index]\n",
    "        end_logit = end_logits[feature_index]\n",
    "        offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "\n",
    "        start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "        for start_index in start_indexes:\n",
    "            for end_index in end_indexes:\n",
    "                # Skip answers that are not fully in the context\n",
    "                if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                    continue\n",
    "                # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                if (\n",
    "                    end_index < start_index\n",
    "                    or end_index - start_index + 1 > max_answer_length\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                answers.append(\n",
    "                    {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "    predicted_answers.append({\"id\": example_id, \"prediction_text\": best_answer[\"text\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 evaluate\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretical/reference answer format\n",
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in small_eval_set\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the predicted answer\n",
    "print(predicted_answers[0])\n",
    "print(theoretical_answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the score\n",
    "metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all together `compute_metrics`\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(features):\n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "\n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # Loop through all features associated with that example\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            start_indexes = np.argsort(start_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logit)[-1 : -n_best - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    answer = {\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                    }\n",
    "                    answers.append(answer)\n",
    "\n",
    "        # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "\n",
    "    theoretical_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    return metric.compute(predictions=predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate our predictions\n",
    "compute_metrics(start_logits, end_logits, eval_set, small_eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Fine-tuning the model\n",
    "# 7.1 load a pretrained the model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 prepare TrainingArguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-squad\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 prepare the Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch the training:\n",
    "# https://wandb.ai/ API token needed\n",
    "# The API fails in the middle, use the custom training loop below\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "predictions, _, _ = trainer.predict(validation_dataset)\n",
    "start_logits, end_logits = predictions\n",
    "compute_metrics(start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. A custom training loop\n",
    "# 8.1 Preparing everything for training\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    validation_set, collate_fn=default_data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinstantiate our model, \n",
    "# starting from the BERT pretrained model again:\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "# accelerator = Accelerator()\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Training loop\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    accelerator.print(\"Evaluation!\")\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(validation_dataset)]\n",
    "    end_logits = end_logits[: len(validation_dataset)]\n",
    "\n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, validation_dataset, raw_datasets[\"validation\"]\n",
    "    )\n",
    "    print(f\"epoch {epoch}:\", metrics)\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Training in progress epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model checkpoint from the current folder\n",
    "model_checkpoint = \"./\"\n",
    "question_answerer = pipeline(\"question-answering\", model=model_checkpoint, device='cuda')\n",
    "\n",
    "context = \"\"\"\n",
    "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ü§ó Transformers?\"\n",
    "print(question_answerer(question=question, context=context))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
