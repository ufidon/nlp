{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/qair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/qair.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "**Question Answering, Information Retrieval, and Retrieval Augmented Generation**\n",
    "\n",
    "- 📝 SALP chapter 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- `Question answering (QA) systems` have supported human information needs since the 1960s, advancing over time with innovations like IBM’s Watson, which surpassed human performance on \"Jeopardy!\" in 2011.\n",
    "  - They are integrated with search engines and modern large language models (LLMs), enhancing their ability to answer diverse queries.\n",
    "\n",
    "- `Prompt-based methods` in modern QA use pretrained LLMs to directly answer fact-based questions from stored knowledge.\n",
    "  - LLMs often produce `hallucinations`, confidently giving incorrect answers, especially in specialized domains, due to poor calibration.\n",
    "  - `Simple prompting LLMs` limits their ability to access proprietary or real-time data, as they cannot update information after training.\n",
    "  - `Retrieval-augmented generation (RAG)` overcomes these issues by combining `document retrieval` with LLMs, grounding answers in relevant, curated data and enabling responses based on proprietary information.\n",
    "  - RAG uses `information retrieval techniques`, from tf-idf to neural models like BERT, to select relevant documents, enhancing answer reliability and context-awareness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "- `Information retrieval (IR)` is the field focused on retrieving media based on user needs,\n",
    "  -  with IR systems often functioning as `search engines`; \n",
    "  -  `ad hoc retrieval` is a core IR task where users enter queries to find ordered sets of relevant documents.\n",
    "\n",
    "![The architecture of an ad hoc IR system](./images/qa/ir.png)\n",
    "\n",
    "- Key components of IR include \n",
    "  - documents: text units like web pages or articles, \n",
    "  - collections: sets of documents, \n",
    "  - terms: words or phrases in documents, and \n",
    "  - queries: user-expressed needs.\n",
    "\n",
    "- IR systems commonly use the `vector space` model, \n",
    "  - representing documents and queries as vectors based on word counts and ranking them by cosine similarity, \n",
    "  - following a bag-of-words approach where word positions are disregarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term weighting and document scoring\n",
    "- IR scores a document’s match with a query by assigning term weights to each word, commonly using weighting schemes:\n",
    "  - [tf-idf](https://scikit-learn.org/1.5/modules/feature_extraction.html): term-frequency — inverse document-frequency\n",
    "  - [BM25](https://huggingface.co/blog/xhluca/bm25s): Best Match 25\n",
    "\n",
    "- **Tf-idf** is the product of term frequency (**tf**) and inverse document frequency (**idf**): \n",
    "  - **Term frequency (tf)** measures word frequency in a document, using a logarithmic scale to avoid overemphasis on high counts:\n",
    "    - $tf_{t, d} = \n",
    "    \\begin{cases} \n",
    "      1 + \\log_{10}(\\text{count}(t, d)) & \\text{if count}(t, d) > 0 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "   \\end{cases}$\n",
    "\n",
    "  - **Inverse Document Frequency (idf)** measures how informative a term is across documents, `decreasing` for terms appearing in many documents:\n",
    "    - $idf_t = \\log_{10} \\dfrac{N}{df_t}$\n",
    "    where $N$ is the total number of documents, and $df_t$ is the number of documents containing term $t$.\n",
    "- The **tf-idf** score for a word $t$ in document $d$ is computed by multiplying `tf` and `idf`:\n",
    "  - $\\text{tf-idf}(t, d) = tf_{t, d} \\cdot idf_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document scoring\n",
    "- Document scoring uses the **cosine similarity** between the query vector $𝐪$ and document vector $𝐝$, computed as:\n",
    "  - $\\text{score}(𝐪, 𝐝) = \\cos(𝐪, 𝐝) = \\dfrac{𝐪 \\cdot 𝐝}{|𝐪||𝐝|}$\n",
    "\n",
    "  - Cosine similarity can also be viewed as the **dot product of unit vectors** by normalizing both vectors to unit length before calculation:\n",
    "    - $\\text{score}(𝐪, 𝐝) = \\cos(𝐪, 𝐝) = \\left(\\dfrac{𝐪}{|𝐪|}\\right) \\cdot \\left(\\dfrac{𝐝}{|𝐝|}\\right)$\n",
    "  - In `tf-idf`:\n",
    "    - $\\text{score}(𝐪, 𝐝) = \\sum_{t \\in 𝐪} \\frac{\\text{tf-idf}(t, 𝐪)}{\\sqrt{\\sum_{q_i \\in 𝐪} \\text{tf-idf}^2(q_i, 𝐪)}} \\cdot \\frac{\\text{tf-idf}(t, 𝐝)}{\\sqrt{\\sum_{d_i \\in 𝐝} \\text{tf-idf}^2(d_i, 𝐝)}}$\n",
    "\n",
    "- Simplifications and **variants of tf-idf cosine** scoring are used in practice, such as omitting the **idf term** in the document to enhance performance.\n",
    "\n",
    "  - **BM25** is an advanced variant of tf-idf that introduces two parameters\n",
    "    - **k** (balancing term frequency and IDF) \n",
    "    - **b** (adjusting document length normalization)—to refine scoring.\n",
    "\n",
    "- **Stop lists** (lists of high-frequency words like \"the,\" \"a\") were traditionally used to exclude common terms from queries and documents, reducing index size; \n",
    "  - modern IR systems now rarely use stop lists due to advancements in IDF weighting and processing efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Inverted Index](https://nlp.stanford.edu/IR-book/html/htmledition/a-first-take-at-building-an-inverted-index-1.html)\n",
    "- In information retrieval, the goal is to find documents containing query terms, ignoring those with none of the terms.\n",
    "\n",
    "- An `inverted index` structure, with a dictionary and postings lists, efficiently identifies relevant documents and stores term frequencies and positions.\n",
    "\n",
    "- The dictionary links terms to postings lists, which provide document IDs and term-related data for fast score computation.\n",
    "\n",
    "- Alternatives, such as bigram indexing and hashing, can improve efficiency in tasks like finding Wikipedia pages for question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval with Dense Vectors\n",
    "- Traditional IR methods like tf-idf and BM25 have the **Vocabulary Mismatch Problem**\n",
    "  - They require `exact word overlap` between query and document, which limits retrieval when `synonyms` are used.\n",
    "\n",
    "  - `Dense embeddings`, such as those generated by BERT, address this issue by encoding semantic meaning, rather than relying on exact word matches.\n",
    "\n",
    "- **Single encoder (a)**: The query and document are fed into a BERT model, with a linear layer on the [CLS] token to predict similarity, allowing context-sensitive matching.\n",
    "  - $\\text{score}(𝐪,𝐝) = \\text{softmax}(𝐔(𝐳))$\n",
    "    - $𝐳 = \\text{BERT}(𝐪;[\\text{SEP}];𝐝)[\\text{CLS}]$\n",
    "  - Documents are split into short passages (e.g., 100 tokens) to fit within BERT's 512-token limit, and the model is fine-tuned on a relevance dataset for better retrieval accuracy.\n",
    "  - `Single encoder` uses a `full BERT encoder` for each query-document pair, which is computationally expensive, \n",
    "    - as it requires encoding each document in the collection with every new query.\n",
    "\n",
    "![Two ways to do dense retrieval](./images/qa/qdbert.png)\n",
    "\n",
    "\n",
    "- **Bi-Encoder (b)** encodes each document only once and precomputes document vectors, enabling `fast` query processing by computing dot products between the query vector and precomputed document vectors.\n",
    "  - $\\text{score}(𝐪,𝐝) = 𝐳_𝐪 ⋅ 𝐳_𝐝$\n",
    "    - $𝐳_𝐪 = \\text{BERT}_Q(𝐪)[\\text{CLS}]$\n",
    "    - $𝐳_𝐝 = \\text{BERT}_D(𝐝)[\\text{CLS}]$\n",
    "  - However, it sacrifices some accuracy by not capturing detailed token-level interactions between the query and document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numerous approaches between the full encoder and the bi-encoder balance accuracy and efficiency by \n",
    "  - using cheaper ranking methods (e.g., BM25) to initially rank documents \n",
    "  - then applying costly BERT scoring to rerank only the top-ranked ones.\n",
    "\n",
    "- [ColBERT (Contextualized Late Interaction over BERT)](https://huggingface.co/colbert-ir/colbertv2.0) encodes the query and document separately into token-level representations, allowing it to pre-store document representations for faster scoring.\n",
    "\n",
    "- ![A sketch of the ColBERT algorithm at inference time](./images/qa/colbert.png)\n",
    "\n",
    "  - It computes relevance by summing the maximum similarity between each query token and the most contextually similar document token, highlighting `token-level contextual similarity`.\n",
    "  - Query and document tokens are processed with BERT, including special `[Q]` and `[D]` tokens, and then scaled to unit length, optimizing vector size for storage efficiency.\n",
    "  - It requires end-to-end training, fine-tuning BERT encoders and linear layers using positive and negative document pairs, optimizing relevance scoring through cross-entropy loss.\n",
    "\n",
    "- Training data for supervised methods like ColBERT often includes labeled positive and negative passages, with semi-supervised approaches or iterative methods used when labeled data is sparse.\n",
    "\n",
    "- Efficient ranking of dense vectors uses `approximate nearest neighbor search algorithms` (e.g., [Faiss](https://huggingface.co/docs/datasets/en/faiss_es)) to quickly find the most similar document vectors for a query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering Questions with Retrieval-Augmented Generation\n",
    "- The retrieval-augmented generation (RAG) approach to question answering involves \n",
    "  - retrieving relevant text segments (retriever) \n",
    "  - generating an answer based on these documents (reader).\n",
    "\n",
    "![Retrieval-based question answering](./images/qa/rag.png)\n",
    "\n",
    "- This two-stage model first uses dense retrievers to find supportive passages and then employs a large language model to generate the answer from the retrieved content, one token at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "- RAG uses retrieved passages to help a language model generate answers, addressing limitations of simple conditional generation.\n",
    "  - simple autoregressive language modeling: \n",
    "    - $\\displaystyle p(x_1,⋯,x_n)=∏_{i=1}^n p([Q:]; q; [A:]; x_{<i})$ \n",
    "- In RAG, a language model is conditioned on both the question and retrieved passages to reduce issues like hallucinations and provide evidence-based answers.\n",
    "  - Simple prompting can work for basic fact-based questions, but RAG `improves reliability` by using `specific prompts` like \"Based on these texts, answer this question.\"\n",
    "  - $\\displaystyle p(x_1,⋯,x_n)=∏_{i=1}^n p(x_i|R(q);\\text{prompt};[Q:]; q; [A:]; x_{<i})$ \n",
    "- Effective RAG requires a well-performing retriever, often utilizing a `two-stage process` that ranks retrieved passages to improve accuracy.\n",
    "  - For complex questions, RAG may use `multi-hop retrieval`, combining initial retrieval results with follow-up searches for more context.\n",
    "- `Privacy and prompt engineering` are important considerations, especially in combining private and public data, with ongoing research focusing on refining the integration of retrieval and generation stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answering Datasets\n",
    "- `Question-answering (QA) datasets` support both training and evaluating language models.  They vary by purpose, \n",
    "  - targeting natural information-seeking `questions`\n",
    "  - `probing` system knowledge and reasoning.\n",
    "- Natural question datasets include \n",
    "  - [Natural Questions](https://huggingface.co/datasets/google-research-datasets/natural_questions): based on Google queries, with answers derived from Wikipedia\n",
    "  - [MS MARCO (Microsoft Machine Reading Comprehension)](https://huggingface.co/datasets/microsoft/ms_marco): Bing queries with human-generated answers and passages.\n",
    "- Non-English natural question datasets include \n",
    "  - [DuReader](https://paperswithcode.com/dataset/dureader): Chinese search engine queries\n",
    "  - [TyDi QA](https://huggingface.co/datasets/google-research-datasets/tydiqa): questions in 11 languages from Wikipedia passages.\n",
    "- Probing datasets, such as [MMLU (Massive Multitask Language Understanding)](https://paperswithcode.com/dataset/mmlu), assess knowledge across multiple fields like medicine and computer science, using questions sourced from exams.\n",
    "- Some datasets augment questions with passages, enabling reading comprehension tasks that require extracting answers from provided texts.\n",
    "\n",
    "- QA tasks vary: \n",
    "  - `open book` QA uses retrieval-augmented generation, \n",
    "  - `closed book` QA answers directly from the model without retrieval.\n",
    "- Answer formats differ, including multiple-choice and freeform, affecting model requirements for generating or selecting responses.\n",
    "\n",
    "- Prompting styles in QA, impacting model performance on complex questions, can be \n",
    "  - zero-shot (only the question) \n",
    "  - few-shot (with examples).\n",
    "- `MMLU` supports both zero-shot and few-shot prompting, making it suitable for testing various model capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Question Answering\n",
    "- **Exact Match**: For multiple-choice questions (like in MMLU), QA systems are evaluated by the `percentage of answers that exactly match the correct answer`.\n",
    "\n",
    "- **Token F1 Score**: For free-text questions (e.g., Natural Questions), token F1 is used to measure `partial overlap between the predicted answer and the reference`, treating them as bags of tokens and averaging scores across all questions.\n",
    "\n",
    "- **Mean Reciprocal Rank (MRR)**: For systems providing ranked answers, MRR evaluates the rank of the first correct answer, scoring each question based on `the reciprocal of this rank` and averaging scores across questions.\n",
    "  - If no correct answer is given, the score for that question is zero, with some versions of MRR excluding these zero-score questions from the final calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-🏃 Practice [Question answering from HuggingFace NLP](https://huggingface.co/learn/nlp-course/en/chapter7/7?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install required libraries\n",
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features of the dataset\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. check the first element\n",
    "print(\"Context: \", raw_datasets[\"train\"][0][\"context\"])\n",
    "print(\"Question: \", raw_datasets[\"train\"][0][\"question\"])\n",
    "print(\"Answer: \", raw_datasets[\"train\"][0][\"answers\"])\n",
    "# In the Answer, The text field is rather obvious, \n",
    "# and the answer_start field contains the `starting character index` of each answer in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is only one possible answer in each training record\n",
    "raw_datasets[\"train\"].filter(lambda x: len(x[\"answers\"][\"text\"]) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but there may be several answers in each evaluation record\n",
    "print(raw_datasets[\"validation\"][0][\"context\"])\n",
    "print(raw_datasets[\"validation\"][0][\"question\"])\n",
    "print(raw_datasets[\"validation\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_datasets[\"validation\"][2][\"context\"])\n",
    "print(raw_datasets[\"validation\"][2][\"question\"])\n",
    "print(raw_datasets[\"validation\"][2][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Processing the training data\n",
    "# 4.1 prepare a [fast tokenizer](https://huggingface.co/docs/transformers/index)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the tokenizer is really fast\n",
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = raw_datasets[\"train\"][0][\"context\"]\n",
    "question = raw_datasets[\"train\"][0][\"question\"]\n",
    "\n",
    "# 4.2 The tokenizer will properly insert the special tokens to form a sentence like this:\n",
    "# [CLS] question [SEP] context [SEP]\n",
    "\n",
    "inputs = tokenizer(question, context)\n",
    "tokenizer.decode(inputs[\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
