{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/09.llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/09.llms.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Large Language Models\n",
    "\n",
    "üìù SALP chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î How do we learn?\n",
    "- Fluent speakers' vast vocabulary is key for language comprehension and production, aiding in studying knowledge acquisition.\n",
    "\n",
    "- Adult vocabulary estimates range from 30,000 to 100,000 words, mostly learned early through spoken interactions.\n",
    "\n",
    "- Children need to learn 7-10 words daily to reach adult vocabulary levels by age 20, a consistent rate across studies.\n",
    "\n",
    "- Reading drives vocabulary growth, with children learning words faster than they encounter them in texts.\n",
    "\n",
    "- The distributional hypothesis suggests word meanings are learned from co-occurrences in text, with real-world interactions enhancing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Could machines learn similarly?\n",
    "- Pretraining large language models (LLMs) involves learning language and world knowledge from vast text data, enabling them to excel in various natural language tasks.\n",
    "\n",
    "- LLMs have transformed tasks like summarization, translation, question answering, and chatbots by using the knowledge gained during pretraining.\n",
    "\n",
    "- LLMs are often `autoregressive or casual`, predicting the next word from previous ones in text flowing sequence (mostly, left-to-right) during training.\n",
    "\n",
    "- Text generation with LLMs is central to generative AI, which includes text, code, and image generation, using specific algorithms like greedy decoding and sampling.\n",
    "\n",
    "- Almost any NLP task, such as summarization, can be framed as `word prediction` in LLMs, demonstrating their versatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of LLMs\n",
    "- **Architecture**: Large Language Models (LLMs) are deep neural networks, often based on transformer architectures, that can process and generate human-like text by learning patterns in massive datasets.\n",
    "\n",
    "- **Training**: LLMs are trained on vast amounts of text data from the internet, books, and other sources. This enables them to understand grammar, facts, and relationships between words to perform a variety of natural language tasks.\n",
    "\n",
    "- **Scale**: These models are \"large\" due to their immense size, often containing billions or even trillions of parameters (weights and biases), which allows them to capture nuanced information and context in text.\n",
    "\n",
    "- **Generalization**: LLMs can generalize across a wide range of language tasks like text completion, translation, summarization, and answering questions without task-specific training, relying on their broad training data.\n",
    "\n",
    "- **Adaptability**: LLMs can be fine-tuned for specific applications, such as customer support, writing assistants, or domain-specific text generation, making them versatile across industries and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs Conditional Text Generation\n",
    "- LLMs generate text token-by-token, using both the `input prompt` and `previously generated tokens`.\n",
    "  - Long context windows (thousands of tokens) make transformers effective for this task.\n",
    "- üçé Text completion ‚Äì LLMs predict the next word based on context, leading to coherent outputs.\n",
    "\n",
    "![Left-to-right (also called autoregressive) text completion](./images/llm/lrtextcomp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Prediction for NLP Tasks**\n",
    "- Many practical NLP tasks can be cast as `word prediction`.\n",
    "- **Sentiment Analysis**: Predict sentiment by comparing probabilities of words like \"positive\" vs. \"negative.\"\n",
    "  - e.g. The sentiment analysis of `I like NLP` can be cast to\n",
    "    - P(positive | The sentiment of `I like NLP` is:), and\n",
    "    - P(negative | The sentiment of `I like NLP` is:)\n",
    "- **Question Answering**: Predict the next word after a question to generate factual answers.\n",
    "  - e.g: \"Who wrote *The Origin of Species*?\" ‚Üí Answer: \"Charles Darwin.\" can be cast to\n",
    "    - P(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species\"?  A:) over all possible next words\n",
    "      - It is very likely we get `Charles`, add it to the context and continue the prediction\n",
    "    - P(w|Q: Who wrote the book ‚Äò‚ÄòThe Origin of Species\"?  A: Charles)\n",
    "      - then it is very likely we get `Darwin`\n",
    "- **Summarization**: LLMs generate summaries using prompt like long articles appended with `tl;dr` to condense it.\n",
    "  - Transformers handle large context windows, using the entire article and generated text to produce concise summaries.\n",
    "\n",
    "![Summarization with large language models using the tl;dr token and context-based autore-\n",
    "gressive generation](./images/llm/texsum.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decoding Strategies in LLMs**\n",
    "- Choosing a word to generate based on the model‚Äôs probabilities is called `decoding`. There are 3 popular decoding strategies:\n",
    "- **Greedy decoding**: Always selects the most probable next word $\\hat{w}_t$ from the vocabulary $V$, but results in repetitive, generic text.\n",
    "  - $\\hat{w}_t = \\arg \\max_{w‚ààV} P(w|ùê∞_{<t})$\n",
    "  - Extremely predictable, identical contexts result in same output.\n",
    "- **Beam search**: Extension of greedy decoding, works well for highly constrained tasks like machine translation.\n",
    "  - It is expected generating a text in one language conditioned on a very specific text in another language.\n",
    "- **Sampling methods**: repeatedly randomly samples words according to their probability\n",
    "until a pre-determined length is reached or the end-of-sentence token is selected. \n",
    "  - Introduce diversity by generating less predictable outputs, improving text variation over greedy decoding.\n",
    "  - It is the most common method for decoding in LLMs with a bit of `generalization`:\n",
    "    - Conditioned on `prompts and previous selections`, words are sampled based on their conditional probabilities determined by a transformer language model.\n",
    "    - Three popular sampling schemes: `random` sampling, `top-k` sampling, `nucleus or top-p` sampling, and `temperature` sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling \n",
    "- Generates a sequence of words $W = w_1, w_2, \\cdots, w_N$, until the end-of-sequence token is hit:\n",
    "\n",
    "  $\n",
    "  i ‚Üê 1\\\\\n",
    "  w·µ¢ ‚àº p(w)\\\\\n",
    "  while\\ w·µ¢ != EOS\\\\\n",
    "    i ‚Üê i+1\\\\\n",
    "    w·µ¢ ‚àº p(w·µ¢ | ùê∞_{<i})\n",
    "  $\n",
    "  - `x ‚àº p(x)`: choose x by sampling from the distribution p(x)\n",
    "\n",
    "- Random sampling may generate `strange or incoherent sentences` due to the large amount of low-probability words.\n",
    "- Alternative sampling methods reduce the chance of selecting unlikely words.\n",
    "  - by trading off between quality (favoring more probable words) and diversity (including middle-probability words for creativity).\n",
    "- High-probability words lead to coherent but repetitive text, while middle-probability words enhance creativity at the cost of coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-k Sampling\n",
    "- Top-k sampling generalizes greedy decoding by `selecting from the top k most likely words` instead of the single most probable word.\n",
    "- At `each word generation`, the vocabulary is `truncated` to the top k words based on their likelihood, and the distribution is `renormalized`.\n",
    "- A word is randomly sampled from these k words based on their `renormalized probabilities`.\n",
    "- When `k = 1`, top-k sampling behaves the `same as greedy` decoding.\n",
    "- Larger k values introduce more diverse text while maintaining quality by selecting words that are still sufficiently probable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nucleus or top-p sampling\n",
    "- Top-k sampling has a fixed k, but `word probability distributions vary by context`, making it less adaptable.\n",
    "- `Top-p` sampling (nucleus sampling) selects words based on `covering a fixed p percent of the probability mass` instead of a fixed number of words.\n",
    "- This approach aims to remove unlikely words while being more flexible across different contexts.\n",
    "- Top-p sampling `dynamically adjusts the pool of candidate words`, ensuring better adaptability to varying probability distributions.\n",
    "- Given a distribution $P(w_t |ùê∞_{<t} )$, the top-p vocabulary $V^{(p)}$ is the smallest set of words such that \n",
    "  - $\\displaystyle \\sum_{w‚ààV^{(p)}}  P(w|ùê∞_{<t} ) ‚â• p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature sampling\n",
    "- Temperature sampling `reshapes the probability distribution` instead of truncating it, `adjusting word probabilities` based on a `temperature parameter` $œÑ$.\n",
    "- In `low-temperature` sampling $œÑ ‚àà (0,1]$, the probabilities of common words increase, making the distribution more `focused on high-probability` words.\n",
    "  - The logits are divided by œÑ before being passed through softmax, enhancing the probability of the most likely words.\n",
    "  - As œÑ approaches 0, the model becomes more \"greedy,\" favoring the most probable word, while œÑ close to 1 leaves the distribution mostly unchanged.\n",
    "- `High-temperature` sampling $œÑ > 1$ flattens the distribution, encouraging `more exploration and diversity` in word selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training LLMs with Self-Supervision\n",
    "- A transformer language model is trained using `self-supervision`, predicting the next word in a text sequence `without needing additional labels`.\n",
    "- The model minimizes prediction errors by using `cross-entropy loss`, which measures the difference between the predicted $\\bm{\\hat{y}}_t$ and actual $\\bm{y}_t$ probability distributions.\n",
    "  - $\\displaystyle L_{CE} = -\\sum_{w\\in V} \\bm{y}_t[w]\\log \\bm{\\hat{y}}_t[w]$\n",
    "- Cross-entropy loss is simplified by focusing on the probability assigned to the correct next word in the sequence.\n",
    "  - $\\displaystyle L_{CE}(\\bm{\\hat{y}}_t, \\bm{y}_t) = -\\log \\bm{\\hat{y}}_t[w_{t+1}]$\n",
    "- At each time step, the model computes a probability distribution for the next word based on the correct input sequence.\n",
    "- `Teacher forcing` is used, where the correct sequence of tokens is always fed to the model for prediction, rather than using its previous predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Training a transformer as a language model](./images/llm/trainllm.png)\n",
    "\n",
    "- The average cross-entropy loss over the entire sequence is calculated, and weights are adjusted via gradient descent to minimize this loss.\n",
    "- Unlike RNNs, transformers `process each item in the sequence in parallel`, as there‚Äôs no recurrence in hidden state calculations.\n",
    "- Large models `fill the full context window` with text, packing multiple documents with special end-of-text tokens if necessary.\n",
    "- Batch sizes for gradient descent are typically large, with GPT-3 models using up to 3.2 million tokens per batch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
