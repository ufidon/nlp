{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/09.llms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/09.llms.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Large Language Models\n",
    "\n",
    "üìù SALP chapter 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î How do we learn?\n",
    "- Fluent speakers' vast vocabulary is key for language comprehension and production, aiding in studying knowledge acquisition.\n",
    "\n",
    "- Adult vocabulary estimates range from 30,000 to 100,000 words, mostly learned early through spoken interactions.\n",
    "\n",
    "- Children need to learn 7-10 words daily to reach adult vocabulary levels by age 20, a consistent rate across studies.\n",
    "\n",
    "- Reading drives vocabulary growth, with children learning words faster than they encounter them in texts.\n",
    "\n",
    "- The distributional hypothesis suggests word meanings are learned from co-occurrences in text, with real-world interactions enhancing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Could machines learn similarly?\n",
    "- Pretraining large language models (LLMs) involves learning language and world knowledge from vast text data, enabling them to excel in various natural language tasks.\n",
    "\n",
    "- LLMs have transformed tasks like summarization, translation, question answering, and chatbots by using the knowledge gained during pretraining.\n",
    "\n",
    "- LLMs are often `autoregressive or casual`, predicting the next word from previous ones in text flowing sequence (mostly, left-to-right) during training.\n",
    "\n",
    "- Text generation with LLMs is central to generative AI, which includes text, code, and image generation, using specific algorithms like greedy decoding and sampling.\n",
    "\n",
    "- Almost any NLP task, such as summarization, can be framed as `word prediction` in LLMs, demonstrating their versatility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of LLMs\n",
    "- **Architecture**: Large Language Models (LLMs) are deep neural networks, often based on transformer architectures, that can process and generate human-like text by learning patterns in massive datasets.\n",
    "\n",
    "- **Training**: LLMs are trained on vast amounts of text data from the internet, books, and other sources. This enables them to understand grammar, facts, and relationships between words to perform a variety of natural language tasks.\n",
    "\n",
    "- **Scale**: These models are \"large\" due to their immense size, often containing billions or even trillions of parameters (weights and biases), which allows them to capture nuanced information and context in text.\n",
    "\n",
    "- **Generalization**: LLMs can generalize across a wide range of language tasks like text completion, translation, summarization, and answering questions without task-specific training, relying on their broad training data.\n",
    "\n",
    "- **Adaptability**: LLMs can be fine-tuned for specific applications, such as customer support, writing assistants, or domain-specific text generation, making them versatile across industries and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs Conditional Text Generation\n",
    "- LLMs generate text token-by-token, using both the `input prompt` and `previously generated tokens`.\n",
    "  - Long context windows (thousands of tokens) make transformers effective for this task.\n",
    "- üçé Text completion ‚Äì LLMs predict the next word based on context, leading to coherent outputs.\n",
    "\n",
    "![Left-to-right (also called autoregressive) text completion](./images/llm/lrtextcomp.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Word Prediction for NLP Tasks**\n",
    "- Many practical NLP tasks can be cast as `word prediction`.\n",
    "- **Sentiment Analysis**: Predict sentiment by comparing probabilities of words like \"positive\" vs. \"negative.\"\n",
    "- **Question Answering**: Predict the next word after a question to generate factual answers.\n",
    "- Example: \"Who wrote *The Origin of Species*?\" ‚Üí Answer: \"Charles Darwin.\"\n",
    "- **Summarization**: LLMs generate summaries using prompts like \"tl;dr\" to condense long articles.\n",
    "  - Transformers handle large context windows, using the entire article and generated text to produce concise summaries."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
