{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/09.mlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/09.mlm.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Masked Language Models\n",
    "\n",
    "üìù SALP chapter 11\n",
    "\n",
    "- Introduction to `bidirectional transformer` encoders, focusing on the `BERT` model and its masked language modeling technique.\n",
    "  - [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert): Bidirectional Encoder Representations from Transformers\n",
    "- Explanation of `masked language modeling`, where a word is masked in the middle of a sentence and the model predicts it based on surrounding context.\n",
    "- Discussion of finetuning pretrained models by `adding a classifier for downstream tasks` like named entity tagging or natural language inference.\n",
    "- Introduction to `transfer learning`, where knowledge from pretraining is applied to new tasks.\n",
    "- Overview of `contextual embeddings`, which represent words differently based on their context, contrasting with earlier static embeddings like word2vec or GloVe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Transformer Encoders\n",
    "- Underlie BERT, RoBERTa, and SpanBERT, which differ from causal (left-to-right) transformers.\n",
    "- Used for contextual token representation while causal models for generative tasks like question answering and summarization.\n",
    "- Utilize information from the entire input sequence\n",
    "  - useful for tasks like part-of-speech tagging, parsing, and named entity recognition.\n",
    "- Focus on creating contextualized token representations using self-attention across the entire input sequence."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
