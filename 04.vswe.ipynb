{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/04.vswe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/04.vswe.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Vector Semantics and Embeddings\n",
    "\n",
    "ðŸ“ SALP chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” **How Do We Determine Word Meanings?**\n",
    "- Words appearing in `similar contexts` tend to have `similar meanings`.\n",
    "  - **Synonyms like \"oculist\" and \"eye-doctor\"** tend to occur near words like *eye* or *examined*.\n",
    "  - **Meaning Similarity = Context Similarity:**\n",
    "    - The amount of difference in meaning corresponds to differences in their contexts.\n",
    "- **Distributional Hypothesis:** Words that occur in similar contexts have similar meanings.\n",
    "\n",
    "\n",
    "### **Vector Semantics and Representation Learning**\n",
    "- **Vector Semantics:**\n",
    "  - Represents word meanings based on their distribution in text.\n",
    "  - Widely used in NLP tasks that involve understanding word meaning.\n",
    "- **Representation Learning:** \n",
    "  - Automatically learns useful representations of input data, instead of using handcrafted features.\n",
    "\n",
    "\n",
    "### **Static vs. Dynamic Embeddings**\n",
    "\n",
    "| **Feature**    | **Static Embeddings**   | **Dynamic Embeddings**    |\n",
    "|---------|-------------|---------------|\n",
    "| **Definition**    | Fixed representation for each word   | Context-dependent representation      |\n",
    "| **Examples**     | Word2Vec, GloVe     | BERT, GPT       |\n",
    "| **Context Awareness**     | Ignores context differences    | Adapts based on sentence context      |\n",
    "| **Flexibility**      | Limited to one meaning per word   | Captures multiple meanings            |\n",
    "| **Use Case**     | Simpler tasks (e.g., word similarity)  | Complex tasks (e.g., sentiment analysis, QA) |\n",
    "\n",
    "- Dynamic embeddings offer more nuanced and context-sensitive word representations, enhancing performance in complex NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Semantics\n",
    "- `Classical/dictionary representation` of word meaning: \n",
    "  - Words as strings of letters or symbols (e.g., DOG for â€œdogâ€).\n",
    "  - This approach is unsatisfactory because it doesn't capture relationships between words.\n",
    "- **Desirable Features of Word Meaning Representation:**\n",
    "  - **Similarity:** Cat is similar to dog.\n",
    "  - **Antonymy:** Cold is the opposite of hot.\n",
    "  - **Connotation:** Happy (positive) vs. sad (negative).\n",
    "  - **Relatedness:** Buy, sell, and pay represent different perspectives on the same event.\n",
    "- **Goal:**\n",
    "  - Develop models that capture these nuances to handle tasks like question-answering and dialogue.\n",
    "\n",
    "\n",
    "### **Lemmas, Senses, and Synonymy**\n",
    "- **Lemmas and Word Senses:**\n",
    "  - **Lemma:** The base form of a word\n",
    "    - e.g., *mouse* for both mouse and mice\n",
    "  - **Word Sense:** Different meanings of a lemma\n",
    "    - e.g., *mouse* as a rodent vs. a computer device.\n",
    "- **Synonymy:**\n",
    "  - Words with `nearly identical meanings` in context \n",
    "  - e.g., car/automobile, couch/sofa.\n",
    "- **Principle of Contrast:** \n",
    "  - No two words are exactly identical in meaning. \n",
    "  - Even near-synonyms differ in usage or connotation.\n",
    "- ðŸŽ **Examples:**\n",
    "  - Scientific vs. informal context: \n",
    "    - *Hâ‚‚O* (scientific) vs. *water* (informal).\n",
    "  - Genre differences are part of the meaning.\n",
    "\n",
    "\n",
    "### **Word Similarity, Relatedness, and Connotation**\n",
    "- **Word Similarity:**\n",
    "  - Not necessarily synonyms, but share `related features` (e.g., cat/dog).\n",
    "  - Important for tasks like paraphrasing and summarization.\n",
    "- **Word Relatedness:**\n",
    "  - Words can be related without being similar\n",
    "    - e.g., coffee/cup, surgeon/scalpel.\n",
    "  - **Semantic Fields:** Words grouped by topics\n",
    "    - e.g., hospital: surgeon, nurse, anesthetic.\n",
    "\n",
    "\n",
    "### **Semantic Frames and Roles**\n",
    "- A **semantic frame** is a set of words that represent perspectives or participants in a specific event.\n",
    "- ðŸŽ**Example: Commercial Transaction Frame**\n",
    "  - **Event:** Trading money for goods/services.\n",
    "  - **Verbs:** \n",
    "    - *Buy* (from the buyer's perspective)\n",
    "    - *Sell* (from the seller's perspective)\n",
    "    - *Pay* (focuses on the monetary aspect)\n",
    "  - **Roles:**\n",
    "    - *Buyer*: Entity providing money\n",
    "    - *Seller*: Entity providing goods/services\n",
    "    - *Goods*: The item being exchanged\n",
    "    - *Money*: The currency used in the transaction\n",
    "- **Practical Application:**\n",
    "  - Understanding that \"Sam bought the book from Ling\" is equivalent to \"Ling sold the book to Sam.\"\n",
    "  - Essential for tasks like **question-answering** and **machine translation**.\n",
    "\n",
    "\n",
    "### **Connotation and Sentiment Analysis**\n",
    "**Connotation** is the `affective meanings or emotions` associated with a word.\n",
    "- **Positive Connotations:** positive sentiment\n",
    "  - *Wonderful*, *Love, Great*\n",
    "- **Negative Connotations:** negative sentiment\n",
    "  - *Dreary*, *Terrible, Hate*\n",
    "- **Contextual Differences:**\n",
    "  - Words with similar meanings can have different connotations:\n",
    "    - *Fake, Knockoff, Forgery* (negative) vs. *Copy, Replica, Reproduction* (neutral/positive).\n",
    "    - *Innocent* (positive) vs. *Naive* (negative).\n",
    "- **Application:**\n",
    "  - Important for **sentiment analysis**, understanding user opinions in reviews, and political language analysis.\n",
    "\n",
    "\n",
    "### **Dimensions of Affective Meaning**\n",
    "- **Three Key Dimensions (Book: Osgood et al., The Measurement of Meaning):**\n",
    "  1. **Valence:** Pleasantness of the word (e.g., *happy* = high, *unhappy* = low).\n",
    "  2. **Arousal:** Intensity of the emotion (e.g., *excited* = high, *calm* = low).\n",
    "  3. **Dominance:** Control exerted by the word (e.g., *controlling* = high, *awed* = low).\n",
    "- **Examples:**\n",
    "  - **Courageous:** [Valence: 8.05, Arousal: 5.5, Dominance: 7.38]\n",
    "  - **Heartbreak:** [Valence: 2.45, Arousal: 5.65, Dominance: 3.58]\n",
    "- **Implications:**\n",
    "  - Words can be represented as points in a 3D space.\n",
    "  - Foundation for **vector semantics** and understanding complex emotional nuances in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Semantics\n",
    "- Each word is represented as a **vector** in a `semantic space`.\n",
    "  - These vectors are called `embeddings`\n",
    "- **Types of Vectors:**\n",
    "  1. **Sparse Vectors:**\n",
    "     - Derived from traditional methods like \n",
    "       - **tf-idf** or **PPMI** (positive pointwise mutual information)\n",
    "     - Typically very long and contain mostly zeros.\n",
    "  2. **Dense Vectors:**\n",
    "     - Generated using models like **word2vec**.\n",
    "     - Shorter, compact representations with useful semantic properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vector/Distributional Models of Meaning**\n",
    "- Represent word meanings based on `co-occurrence patterns` in text.\n",
    "- Capture how often words appear together in **Co-occurrence Matrix:** \n",
    "  1. **Term-Document Matrix**\n",
    "  2. **Term-Term Matrix**\n",
    "\n",
    "\n",
    "### **Term-Document Matrix**\n",
    "- Rows represent words, and columns represent documents.\n",
    "- Each cell shows the frequency of a word in a specific document.\n",
    "- ðŸŽ **Example:** a term-document matrix for 4 words across 4 Shakespeare plays.\n",
    "\n",
    "|               | As You Like It | Twelfth Night | Julius Caesar | Henry V  |\n",
    "|---------------|----------------|---------------|---------------|---------|\n",
    "| **battle**    | 1              | 0             | 7             | 13      |\n",
    "| **good**      | 114            | 80            | 62            | 89      |\n",
    "| **fool**      | 36             | 58            | 1             | 4       |\n",
    "| **wit**       | 20             | 15            | 2             | 3       |\n",
    "\n",
    "\n",
    "### **Document Representation as Vectors**\n",
    "- Each column is called a document **vector** \n",
    "  - Which is an array of numbers representing a documentâ€™s word frequencies.\n",
    "  - e.g. *As You Like It* is represented by the vector **[1, 114, 36, 20]**.\n",
    "- Each document is a point in a high-dimensional vector space.\n",
    "- This vector space `dimensionality` is equal to the `vocabulary size (|V|)`.\n",
    "\n",
    "\n",
    "### **Document Similarity**\n",
    "- Documents with similar vectors have similar content.\n",
    "- ðŸŽ **Example:**\n",
    "  - *As You Like It* and *Twelfth Night* are closer in the vector space because they share more similar words like \"fool\" and \"wit\".\n",
    "  - In contrast, *Julius Caesar* and *Henry V* have higher frequencies of \"battle\".\n",
    "\n",
    "\n",
    "\n",
    "### **Words as Vectors: Document Dimensions**\n",
    "- Words can be represented as vectors too, based on their distribution across documents.\n",
    "- Each word vector is a row in the term-document matrix.\n",
    "- ðŸŽ **Example:**\n",
    "  - The vector for \"fool\" is **[36, 58, 1, 4]**.\n",
    "  - Dimensions correspond to the four Shakespeare plays.\n",
    "- **Observation:**\n",
    "  - Related words (e.g., \"fool\" and \"wit\") have similar vectors because they occur in similar documents.\n",
    "\n",
    "\n",
    "\n",
    "### **Words as Vectors: Word Dimensions**\n",
    "- **Term-Term Matrix (Word-Word Matrix):** Columns and rows represent words instead of documents.\n",
    "  - Each cell records the number of times `two words co-occur` in a defined context\n",
    "    - e.g., within a Â±4 word window.\n",
    "- ðŸŽ **Example:** Four words in the Wikipedia corpus\n",
    "\n",
    "|               | computer  | data     | result  | pie   | sugar | count(w)  |\n",
    "|---------------|-----------|----------|---------|-------|-------|------|\n",
    "| **cherry**    | 2         | 8        | 9       | 442   | 25    | 486  |\n",
    "| **strawberry**| 0         | 0        | 1       | 60    | 19    | 80  |\n",
    "| **digital**   | 1670      | 1683     | 85      | 5     | 4     | 3447  |\n",
    "| **information**| 3325     | 3982     | 378     | 5     | 13    | 7703  |\n",
    "| **count(context)**| 4997  | 5673     | 473     | 512   | 61    | 11716 |\n",
    "\n",
    "- **Word Similarity:**\n",
    "  - Words like \"cherry\" and \"strawberry\" are more similar because they share contexts like \"pie\" and \"sugar\".\n",
    "\n",
    "\n",
    "\n",
    "### **Dimensionality of Word Vectors**\n",
    "- The number of dimensions is generally the size of the vocabulary $|V|$\n",
    "  - often between 10,000 and 50,000 words, based on the most frequent words in the training corpus.\n",
    "- Most cells are zeros, resulting in `sparse` vector representations.\n",
    "- There are efficient algorithms for storing and computing with sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine for Measuring Similarity\n",
    "- `Cosine similarity` measures how similar two vectors are by calculating the cosine of the angle between them.\n",
    "- Commonly used to measure similarity between word or document vectors in NLP.\n",
    "- Requires two vectors of the same dimensionality, either:\n",
    "  - Both with words as dimensions (length |V|).\n",
    "  - Both with documents as dimensions (length |D|).\n",
    "\n",
    "\n",
    "### **The Dot Product**\n",
    "- The **dot product** of two vectors $v$ and $w$ is calculated as:\n",
    "  - $\\displaystyle\\text{dot product}(v, w) = v \\cdot w = \\sum_{i=1}^{N} v_i w_i = v_1w_1 + v_2w_2 + ... + v_N w_N$\n",
    "- Acts as a `similarity metric`\n",
    "  - high when vectors have large values in the same dimensions.\n",
    "  - 0 if vectors are orthogonal (`unrelated`)\n",
    "- **Issue:**\n",
    "  - The dot product favors longer vectors, resulting in higher similarity scores for frequent words.\n",
    "  - âˆµ Frequent words have longer vectors because they co-occur with more words, leading to inflated similarity scores.\n",
    "- **Solution:**\n",
    "  - Normalize the dot product by dividing by the lengths of both vectors.\n",
    "    - $\\displaystyle\\cos(v, w) = \\frac{v \\cdot w}{|v||w|}$\n",
    "    - Where:\n",
    "      - $\\displaystyle|v| = \\sqrt{\\sum_{i=1}^{N} v_i^2} \\quad \\text{and} \\quad |w| = \\sqrt{\\sum_{i=1}^{N} w_i^2}$\n",
    "\n",
    "\n",
    "### **Properties of Cosine Similarity**\n",
    "- Cosine similarity ranges from 0 to 1 for non-negative frequency vectors:\n",
    "  - **1:** Vectors are identical.\n",
    "  - **0:** Vectors are orthogonal (completely dissimilar).\n",
    "- **Advantages:**\n",
    "  - Measures similarity regardless of vector length (frequency).\n",
    "  - Useful in comparing high-dimensional, sparse vectors like word embeddings.\n",
    "\n",
    "\n",
    "### ðŸŽ **Example: Comparing Words with Cosine Similarity**\n",
    "- Given the term-term matrix:\n",
    "\n",
    "|         | pie | data | computer |\n",
    "|---------|-----|------|----------|\n",
    "| **cherry**     | 442 | 8    | 2        |\n",
    "| **digital**    | 5   | 1683 | 1670     |\n",
    "| **information**| 5   | 3982 | 3325     |\n",
    "\n",
    "\n",
    "- **Calculation: Cosine Similarity of Cherry and Information**\n",
    "  - $\\displaystyle\\cos(\\text{cherry}, \\text{information}) = \\frac{442 \\times 5 + 8 \\times 3982 + 2 \\times 3325}{\\sqrt{442^2 + 8^2 + 2^2} \\sqrt{5^2 + 3982^2 + 3325^2}}$\n",
    "  - $= 0.018$\n",
    "\n",
    "- **Calculation: Cosine Similarity of Digital and Information**\n",
    "  - $\\displaystyle\\cos(\\text{digital}, \\text{information}) = \\frac{5 \\times 5 + 1683 \\times 3982 + 1670 \\times 3325}{\\sqrt{5^2 + 1683^2 + 1670^2} \\sqrt{5^2 + 3982^2 + 3325^2}}$\n",
    "  - $= 0.996$\n",
    "\n",
    "- **Digital** is much closer in meaning to **information** than **cherry** is, based on cosine similarity.\n",
    "  - This result makes intuitive sense as \"digital\" and \"information\" frequently co-occur in similar contexts, unlike \"cherry.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "- A technique used to weigh terms in a document to evaluate how important a word is relative to a document or a collection.\n",
    "- Used to handle the limitations of raw frequency which is skewed and not discriminative.\n",
    "  - Words like **the**, **it**, and **they** appear frequently but are not informative.\n",
    "  - Frequent words across documents can obscure meaningful associations\n",
    "  - e.g. Words like \"good\" in Shakespeare's plays are frequent across all plays and don't help in distinguishing between them.\n",
    "\n",
    "\n",
    "### **Balancing Frequency with TF-IDF**\n",
    "- **The Paradox:**\n",
    "  - Rare words are important, but overly frequent words are not useful.\n",
    "- **TF-IDF Solution:**\n",
    "  - Combines **Term Frequency (TF)** and **Inverse Document Frequency (IDF)** to balance word frequency across documents.\n",
    "\n",
    "\n",
    "### **Term Frequency (TF)**\n",
    "- Measures the frequency of a word $t$ in a document $d$.\n",
    "  - $\\text{tf}_{t, d} = \\text{count}(t, d)$\n",
    "- **Log Scaling** is often applied to squash the frequency values:\n",
    "  - $\\text{tf}_{t, d} = \\begin{cases} \n",
    "  1 + \\log_{10} \\text{count}(t, d) & \\text{if } \\text{count}(t, d) > 0 \\\\ \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}$\n",
    "  - **Example Values:**\n",
    "    - 1 occurrence: $\\text{tf} = 1$\n",
    "    - 10 occurrences: $\\text{tf} = 2$\n",
    "    - 100 occurrences: $\\text{tf} = 3$\n",
    "- The **collection frequency** of a term is its total number of occurrences in the whole collection of documents. \n",
    "\n",
    "\n",
    "### **Inverse Document Frequency (IDF)**\n",
    "- Measures the importance of a term across all documents.\n",
    "- $\\text{idf}_t = \\log_{10} \\left( \\dfrac{N}{\\text{df}_t} \\right)$\n",
    "  - $N$ = Total number of documents.\n",
    "  - Document frequency $\\text{df}_t$ of a term $t$ = Number of documents containing term $t$.\n",
    "- Gives higher weight to terms appearing in fewer documents.\n",
    "\n",
    "\n",
    "### **Example of IDF in Shakespeare's Plays**\n",
    "\n",
    "| Word     | Document Frequency | IDF  |\n",
    "|----------|--------------------|------|\n",
    "| Romeo    | 1                  | 1.57 |\n",
    "| Salad    | 2                  | 1.27 |\n",
    "| Falstaff | 4                  | 0.967 |\n",
    "| Forest   | 12                 | 0.489 |\n",
    "| Battle   | 21                 | 0.246 |\n",
    "| Wit      | 34                 | 0.037 |\n",
    "| Fool     | 36                 | 0.012 |\n",
    "| Good     | 37                 | 0    |\n",
    "\n",
    "- Words like \"Romeo\" have high IDF and are discriminative, whereas \"good\" has an IDF of 0 and is non-informative.\n",
    "\n",
    "\n",
    "### **Calculating TF-IDF**\n",
    "- The tf-idf weighted value $w_{t, d}$ for word $t$ in document $d$ combines term\n",
    "frequency $tf_{t, d}$\n",
    "  - $w_{t, d} = \\text{tf}_{t, d} \\times \\text{idf}_t$\n",
    "- **Example Calculation:**\n",
    "  - **Word:** *Wit* in *As You Like It*\n",
    "  - **Term Frequency:** $1 + \\log_{10}(20) = 2.301$\n",
    "  - **IDF:** $0.037$\n",
    "  - **TF-IDF Weight:** $2.301 \\times 0.037 = 0.085$\n",
    "- The weight of \"wit\" is reduced significantly, reflecting its high frequency across many plays.\n",
    "\n",
    "\n",
    "### **Benefits of TF-IDF Weighting**\n",
    "- **Advantages:**\n",
    "  - Reduces the impact of common, non-informative words.\n",
    "  - Highlights unique terms that distinguish documents from each other.\n",
    "- **Applications:**\n",
    "  - Commonly used in information retrieval and text mining.\n",
    "  - Provides a strong baseline for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pointwise Mutual Information (PMI)**\n",
    "- A statistical measure used to evaluate the association between two events $x$ and $y$ (e.g., words) by comparing their observed co-occurrence to what would be expected if they were independent.\n",
    "  - $\\displaystyle \\text{PMI}(x, y) = \\log_2 \\left( \\frac{P(x, y)}{P(x)P(y)} \\right)$\n",
    "- **Intuition:**\n",
    "  - If two words co-occur frequently but occur independently with low probability, PMI will be high.\n",
    "  - For example, in a text corpus, if \"cat\" and \"fur\" appear together more often than independently, PMI will indicate a strong association.\n",
    "- PMI is frequently used to quantify the relationship between a **target word** $w$ (e.g., \"cat\") and a **context word** $c$ (e.g., \"fur\").\n",
    "  - $\\displaystyle \\text{PMI}(w, c) = \\log_2 \\left( \\frac{P(w, c)}{P(w)P(c)} \\right)$\n",
    "  1. $P(w, c)$: Probability of observing both the target word $w$ and context word $c$ together.\n",
    "  2. $P(w)P(c)$: Probability of observing $w$ and $c$ independently.\n",
    "- **Interpretation:**\n",
    "  - PMI > 0: Words co-occur more often than by chance (positive association).\n",
    "  - PMI = 0: Words co-occur as expected by chance.\n",
    "  - PMI < 0: Words co-occur less often than by chance (negative association).\n",
    "\n",
    "\n",
    "\n",
    "### ðŸŽ**Calculating PMI for 'information' and 'data':**\n",
    "1. **Co-occurrence Probability:**\n",
    "   - $\\displaystyle P(w=\\text{information}, c=\\text{data}) = \\frac{3982}{11716} = 0.3399$\n",
    "\n",
    "2. **Individual Probabilities:**\n",
    "   - $\\displaystyle P(w=\\text{information}) = \\frac{7703}{11716} = 0.6575$\n",
    "   - $\\displaystyle P(c=\\text{data}) = \\frac{5673}{11716} = 0.4842$\n",
    "\n",
    "3. **PMI Calculation:**\n",
    "   - $\\displaystyle \\text{PMI}(\\text{information}, \\text{data}) = \\log_2 \\left( \\frac{0.3399}{0.6575 \\times 0.4842} \\right) = 0.0944$\n",
    "\n",
    "- The PMI value of 0.0944 suggests a slight positive association between \"information\" and \"data\" in this context.\n",
    "\n",
    "\n",
    "\n",
    "### **Challenges with PMI**\n",
    "\n",
    "- **Issues with Negative PMI:**\n",
    "  - Negative PMI values suggest that two words co-occur less often than expected by chance, but these values can be unreliable unless the corpus is extremely large.\n",
    "  - For example, distinguishing whether two words with very low probabilities occur together less often than by chance would require vast amounts of data.\n",
    "\n",
    "- **Bias Towards Infrequent Events:**\n",
    "  - PMI tends to give high values for rare word pairs even if their co-occurrence is not meaningful.\n",
    "  - Example: A rare term like \"quantum\" paired with \"entanglement\" might have an inflated PMI due to their low individual frequencies.\n",
    "\n",
    "- **Impact on NLP Models:**\n",
    "  - This bias can lead to incorrect conclusions about word associations and affect downstream tasks such as topic modeling or semantic similarity.\n",
    "\n",
    "\n",
    "\n",
    "### **Positive Pointwise Mutual Information (PPMI)**\n",
    "\n",
    "- **Positive PMI (PPMI)** sets all negative PMI values to zero, making it more robust against unreliable associations.\n",
    "\n",
    "  - $\\displaystyle \\text{PPMI}(w, c) = \\max \\left( \\log_2 \\left( \\frac{P(w, c)}{P(w)P(c)} \\right), 0 \\right)$\n",
    "\n",
    "- **Why Use PPMI?**\n",
    "  - It focuses only on positive associations, eliminating misleading negative values.\n",
    "  - PPMI is commonly used in word embeddings and vector space models, where negative PMI values do not contribute meaningful information.\n",
    "\n",
    "- **Example:**\n",
    "  - If PMI for (word1, word2) is -2, PPMI sets it to 0.\n",
    "  - If PMI for (word1, word2) is 4, PPMI retains the value of 4.\n",
    "\n",
    "\n",
    "\n",
    "### **Constructing the PPMI Matrix**\n",
    "\n",
    "- A co-occurrence matrix $F$ has:\n",
    "  - **W rows:** Represent words in the vocabulary.\n",
    "  - **C columns** Represent contexts (surrounding words).\n",
    "  - Each cell $f_{ij}$ in $F$ indicates the number of times word $w_i$ appears within context $c_j$.\n",
    "\n",
    "- **Constructing the PPMI Matrix:**\n",
    "1. Calculate the joint probability $p_{ij}$ of $w_i$ and $c_j$:\n",
    "   - $\\displaystyle p_{ij}=\\frac{f_{ij}}{âˆ‘_{i=1}^W âˆ‘_{j=1}^C f_{ij}}$\n",
    "2. Calculate the marginal probabilities of word $p_{i*}$ and context $p_{*j}$:\n",
    "   - $\\displaystyle p_{i*} = \\dfrac{âˆ‘_{j=1}^C f_{ij}}{ âˆ‘_{i=1}^W âˆ‘_{j=1}^C f_{ij}} â€ƒ\\quad  p_{*j} = \\dfrac{âˆ‘_{i=1}^W f_{ij}}{ âˆ‘_{i=1}^W âˆ‘_{j=1}^C f_{ij}}$ \n",
    "3. Compute: $\\displaystyle \\text{PPMI}_{ij} = \\max \\left( \\log_2 \\left( \\frac{p_{ij}}{p_{i*} \\times p_{*j}} \\right), 0 \\right)$\n",
    "\n",
    "- The resulting PPMI matrix highlights the strength of association between words and their contexts.\n",
    "\n",
    "\n",
    "\n",
    "### **Example PPMI Calculations**\n",
    "\n",
    "| `w\\P(w,c)\\c`| Computer | Data   | Result | Pie    | Sugar  | p(w)  |\n",
    "|-------------|----------|--------|--------|--------|--------|-------|\n",
    "| Cherry      | 0.0002   | 0.0007 | 0.0008 | 0.0377 | 0.0021 | 0.0415|\n",
    "| Strawberry  | 0.0000   | 0.0000 | 0.0001 | 0.0051 | 0.0016 | 0.0068|\n",
    "| Digital     | 0.1425   | 0.1436 | 0.0073 | 0.0004 | 0.0003 | 0.2942|\n",
    "| Information | 0.2838   | 0.3399 | 0.0323 | 0.0004 | 0.0011 | 0.6575|\n",
    "| p(c)        | 0.4265   | 0.4842 | 0.0404 | 0.0437 | 0.0052 |       |\n",
    "\n",
    "\n",
    "| Word       | Computer | Data | Result | Pie  | Sugar |\n",
    "|------------|------|------|--------|------|-------|\n",
    "| Cherry     | 0    | 0    | 0      | 4.38 | 3.30  |\n",
    "| Strawberry | 0    | 0    | 0      | 4.10 | 5.51  |\n",
    "| Digital    | 0.18 | 0.01 | 0      | 0    | 0     |\n",
    "| Information| 0.02 | 0.09 | 0.28   | 0    | 0     |\n",
    "\n",
    "- Cherry and strawberry are highly associated with \"pie\" and \"sugar,\" indicating strong co-occurrence.\n",
    "- Digital has a weaker association with \"data,\" possibly due to its broader usage context.\n",
    "\n",
    "\n",
    "\n",
    "### **Handling PMI Bias with $\\alpha$-Smoothing**\n",
    "- Rare contexts can disproportionately inflate PMI values.\n",
    "- **Solution:**\n",
    "  - Modify context probability using $\\alpha$-smoothing:\n",
    "  - $\\displaystyle \\text{PPM}I_{\\alpha}(w, c) = \\max \\left( \\log_2 \\left( \\frac{P(w, c)}{P(w)P^{\\alpha}(c)} \\right), 0 \\right)$\n",
    "  - **$P^{\\alpha}(c) = \\dfrac{count(c)^Î±}{Î£_{c}count(c)^Î±}$:** Raises context probability $P(c)$ to a power $\\alpha$.\n",
    "  - Setting $\\alpha = 0.75$ gives a balanced reduction in bias, particularly for rare contexts.\n",
    "- Reduces high PMI values for rare word-context pairs, providing more meaningful associations.\n",
    "\n",
    "\n",
    "\n",
    "### **Another Solution:Laplace Smoothing for PMI**\n",
    "- **Laplace Smoothing**: Add a small constant $k$ (e.g., 0.1 to 3) to each count before calculating PMI.\n",
    "- **Why Laplace Smoothing?**\n",
    "  - Helps reduce the impact of zero or low-frequency counts in co-occurrence matrices.\n",
    "  - Larger $k$ values discount more, reducing PMI bias further.\n",
    "- **Modified PMI Formula:**\n",
    "  - $\\displaystyle \\text{PMI}(w, c) = \\log_2 \\left( \\frac{f(w, c) + k}{(f(w) + k)(f(c) + k)} \\right)$\n",
    "- **Choosing $k$:**\n",
    "  - Smaller values maintain original PMI properties; larger values decrease variability in low-frequency pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of tf-idf and PPMI Vector Models\n",
    "\n",
    "- **Vector Representation:**\n",
    "  - Target word represented as a vector.\n",
    "  - Dimensions correspond to either:\n",
    "    - Documents in a collection (term-document matrix).\n",
    "    - Counts of neighboring words (term-term matrix).\n",
    "  - Values in dimensions weighted by:\n",
    "    - **tf-idf** (for term-document matrices).\n",
    "    - **PPMI** (for term-term matrices).\n",
    "  - Vectors are sparse (mostly zeros).\n",
    "\n",
    "- **Similarity Computation:**\n",
    "  - Similarity between two words $x$ and $y$ computed using cosine similarity.\n",
    "  - High cosine value indicates high similarity.\n",
    "  - Referred to as the **tf-idf model** or **PPMI model** based on the weighting function.\n",
    "\n",
    "\n",
    "### **Applications of tf-idf and PPMI Models in Document and Word Similarity**\n",
    "\n",
    "- **Document Similarity:**\n",
    "  - **Document Vector:** \n",
    "    - Represented by the centroid of word vectors in the document.\n",
    "    - Centroid minimizes the sum of squared distances to each vector.\n",
    "    - Formula: $\\mathbf{d} = \\dfrac{\\mathbf{w}_1 + \\mathbf{w}_2 + ... + \\mathbf{w}_k}{k}$\n",
    "  - Compute similarity between two documents $\\mathbf{d}_1$ and $\\mathbf{d}_2$ using cosine similarity.\n",
    "  - Applications:\n",
    "    - Information retrieval.\n",
    "    - Plagiarism detection.\n",
    "    - News recommendation.\n",
    "    - Digital humanities (comparing texts).\n",
    "\n",
    "- **Word Similarity:**\n",
    "  - Use PPMI or tf-idf models to find word paraphrases.\n",
    "  - Track changes in word meanings.\n",
    "  - Discover meanings in different corpora.\n",
    "  - Find top 10 similar words by cosine similarity.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
