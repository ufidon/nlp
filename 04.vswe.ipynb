{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/04.vswe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/04.vswe.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Vector Semantics and Embeddings\n",
    "\n",
    "ðŸ“ SALP chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” **How Do We Determine Word Meanings?**\n",
    "- Words appearing in `similar contexts` tend to have `similar meanings`.\n",
    "  - **Synonyms like \"oculist\" and \"eye-doctor\"** tend to occur near words like *eye* or *examined*.\n",
    "  - **Meaning Similarity = Context Similarity:**\n",
    "    - The amount of difference in meaning corresponds to differences in their contexts.\n",
    "- **Distributional Hypothesis:** Words that occur in similar contexts have similar meanings.\n",
    "\n",
    "\n",
    "### **Vector Semantics and Representation Learning**\n",
    "- **Vector Semantics:**\n",
    "  - Represents word meanings based on their distribution in text.\n",
    "  - Widely used in NLP tasks that involve understanding word meaning.\n",
    "- **Representation Learning:** \n",
    "  - Automatically learns useful representations of input data, instead of using handcrafted features.\n",
    "\n",
    "\n",
    "### **Static vs. Dynamic Embeddings**\n",
    "\n",
    "| **Feature**    | **Static Embeddings**   | **Dynamic Embeddings**    |\n",
    "|---------|-------------|---------------|\n",
    "| **Definition**    | Fixed representation for each word   | Context-dependent representation      |\n",
    "| **Examples**     | Word2Vec, GloVe     | BERT, GPT       |\n",
    "| **Context Awareness**     | Ignores context differences    | Adapts based on sentence context      |\n",
    "| **Flexibility**      | Limited to one meaning per word   | Captures multiple meanings            |\n",
    "| **Use Case**     | Simpler tasks (e.g., word similarity)  | Complex tasks (e.g., sentiment analysis, QA) |\n",
    "\n",
    "- Dynamic embeddings offer more nuanced and context-sensitive word representations, enhancing performance in complex NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Semantics\n",
    "- `Classical/dictionary representation` of word meaning: \n",
    "  - Words as strings of letters or symbols (e.g., DOG for â€œdogâ€).\n",
    "  - This approach is unsatisfactory because it doesn't capture relationships between words.\n",
    "- **Desirable Features of Word Meaning Representation:**\n",
    "  - **Similarity:** Cat is similar to dog.\n",
    "  - **Antonymy:** Cold is the opposite of hot.\n",
    "  - **Connotation:** Happy (positive) vs. sad (negative).\n",
    "  - **Relatedness:** Buy, sell, and pay represent different perspectives on the same event.\n",
    "- **Goal:**\n",
    "  - Develop models that capture these nuances to handle tasks like question-answering and dialogue.\n",
    "\n",
    "\n",
    "### **Lemmas, Senses, and Synonymy**\n",
    "- **Lemmas and Word Senses:**\n",
    "  - **Lemma:** The base form of a word\n",
    "    - e.g., *mouse* for both mouse and mice\n",
    "  - **Word Sense:** Different meanings of a lemma\n",
    "    - e.g., *mouse* as a rodent vs. a computer device.\n",
    "- **Synonymy:**\n",
    "  - Words with `nearly identical meanings` in context \n",
    "  - e.g., car/automobile, couch/sofa.\n",
    "- **Principle of Contrast:** \n",
    "  - No two words are exactly identical in meaning. \n",
    "  - Even near-synonyms differ in usage or connotation.\n",
    "- ðŸŽ **Examples:**\n",
    "  - Scientific vs. informal context: \n",
    "    - *Hâ‚‚O* (scientific) vs. *water* (informal).\n",
    "  - Genre differences are part of the meaning.\n",
    "\n",
    "\n",
    "### **Word Similarity, Relatedness, and Connotation**\n",
    "- **Word Similarity:**\n",
    "  - Not necessarily synonyms, but share `related features` (e.g., cat/dog).\n",
    "  - Important for tasks like paraphrasing and summarization.\n",
    "- **Word Relatedness:**\n",
    "  - Words can be related without being similar\n",
    "    - e.g., coffee/cup, surgeon/scalpel.\n",
    "  - **Semantic Fields:** Words grouped by topics\n",
    "    - e.g., hospital: surgeon, nurse, anesthetic.\n",
    "\n",
    "\n",
    "### **Semantic Frames and Roles**\n",
    "- A **semantic frame** is a set of words that represent perspectives or participants in a specific event.\n",
    "- ðŸŽ**Example: Commercial Transaction Frame**\n",
    "  - **Event:** Trading money for goods/services.\n",
    "  - **Verbs:** \n",
    "    - *Buy* (from the buyer's perspective)\n",
    "    - *Sell* (from the seller's perspective)\n",
    "    - *Pay* (focuses on the monetary aspect)\n",
    "  - **Roles:**\n",
    "    - *Buyer*: Entity providing money\n",
    "    - *Seller*: Entity providing goods/services\n",
    "    - *Goods*: The item being exchanged\n",
    "    - *Money*: The currency used in the transaction\n",
    "- **Practical Application:**\n",
    "  - Understanding that \"Sam bought the book from Ling\" is equivalent to \"Ling sold the book to Sam.\"\n",
    "  - Essential for tasks like **question-answering** and **machine translation**.\n",
    "\n",
    "\n",
    "### **Connotation and Sentiment Analysis**\n",
    "**Connotation** is the `affective meanings or emotions` associated with a word.\n",
    "- **Positive Connotations:** positive sentiment\n",
    "  - *Wonderful*, *Love, Great*\n",
    "- **Negative Connotations:** negative sentiment\n",
    "  - *Dreary*, *Terrible, Hate*\n",
    "- **Contextual Differences:**\n",
    "  - Words with similar meanings can have different connotations:\n",
    "    - *Fake, Knockoff, Forgery* (negative) vs. *Copy, Replica, Reproduction* (neutral/positive).\n",
    "    - *Innocent* (positive) vs. *Naive* (negative).\n",
    "- **Application:**\n",
    "  - Important for **sentiment analysis**, understanding user opinions in reviews, and political language analysis.\n",
    "\n",
    "\n",
    "### **Dimensions of Affective Meaning**\n",
    "- **Three Key Dimensions (Book: Osgood et al., The Measurement of Meaning):**\n",
    "  1. **Valence:** Pleasantness of the word (e.g., *happy* = high, *unhappy* = low).\n",
    "  2. **Arousal:** Intensity of the emotion (e.g., *excited* = high, *calm* = low).\n",
    "  3. **Dominance:** Control exerted by the word (e.g., *controlling* = high, *awed* = low).\n",
    "- **Examples:**\n",
    "  - **Courageous:** [Valence: 8.05, Arousal: 5.5, Dominance: 7.38]\n",
    "  - **Heartbreak:** [Valence: 2.45, Arousal: 5.65, Dominance: 3.58]\n",
    "- **Implications:**\n",
    "  - Words can be represented as points in a 3D space.\n",
    "  - Foundation for **vector semantics** and understanding complex emotional nuances in text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Semantics\n",
    "- Each word is represented as a **vector** in a `semantic space`.\n",
    "  - These vectors are called `embeddings`\n",
    "- **Types of Vectors:**\n",
    "  1. **Sparse Vectors:**\n",
    "     - Derived from traditional methods like \n",
    "       - **tf-idf** or **PPMI** (positive pointwise mutual information)\n",
    "     - Typically very long and contain mostly zeros.\n",
    "  2. **Dense Vectors:**\n",
    "     - Generated using models like **word2vec**.\n",
    "     - Shorter, compact representations with useful semantic properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vector/Distributional Models of Meaning**\n",
    "- Represent word meanings based on `co-occurrence patterns` in text.\n",
    "- Capture how often words appear together in **Co-occurrence Matrix:** \n",
    "  1. **Term-Document Matrix**\n",
    "  2. **Term-Term Matrix**\n",
    "\n",
    "\n",
    "### **Term-Document Matrix**\n",
    "- Rows represent words, and columns represent documents.\n",
    "- Each cell shows the frequency of a word in a specific document.\n",
    "- ðŸŽ **Example:** a term-document matrix for 4 words across 4 Shakespeare plays.\n",
    "\n",
    "|               | As You Like It | Twelfth Night | Julius Caesar | Henry V  |\n",
    "|---------------|----------------|---------------|---------------|---------|\n",
    "| **battle**    | 1              | 0             | 7             | 13      |\n",
    "| **good**      | 114            | 80            | 62            | 89      |\n",
    "| **fool**      | 36             | 58            | 1             | 4       |\n",
    "| **wit**       | 20             | 15            | 2             | 3       |\n",
    "\n",
    "\n",
    "### **Document Representation as Vectors**\n",
    "- Each column is called a document **vector** \n",
    "  - Which is an array of numbers representing a documentâ€™s word frequencies.\n",
    "  - e.g. *As You Like It* is represented by the vector **[1, 114, 36, 20]**.\n",
    "- Each document is a point in a high-dimensional vector space.\n",
    "- This vector space `dimensionality` is equal to the `vocabulary size (|V|)`.\n",
    "\n",
    "\n",
    "### **Document Similarity**\n",
    "- Documents with similar vectors have similar content.\n",
    "- ðŸŽ **Example:**\n",
    "  - *As You Like It* and *Twelfth Night* are closer in the vector space because they share more similar words like \"fool\" and \"wit\".\n",
    "  - In contrast, *Julius Caesar* and *Henry V* have higher frequencies of \"battle\".\n",
    "\n",
    "\n",
    "\n",
    "### **Words as Vectors: Document Dimensions**\n",
    "- Words can be represented as vectors too, based on their distribution across documents.\n",
    "- Each word vector is a row in the term-document matrix.\n",
    "- ðŸŽ **Example:**\n",
    "  - The vector for \"fool\" is **[36, 58, 1, 4]**.\n",
    "  - Dimensions correspond to the four Shakespeare plays.\n",
    "- **Observation:**\n",
    "  - Related words (e.g., \"fool\" and \"wit\") have similar vectors because they occur in similar documents.\n",
    "\n",
    "\n",
    "\n",
    "### **Words as Vectors: Word Dimensions**\n",
    "- **Term-Term Matrix (Word-Word Matrix):** Columns and rows represent words instead of documents.\n",
    "  - Each cell records the number of times `two words co-occur` in a defined context\n",
    "    - e.g., within a Â±4 word window.\n",
    "- ðŸŽ **Example:** Four words in the Wikipedia corpus\n",
    "\n",
    "|               | computer  | data     | result  | pie   | sugar | count(w)  |\n",
    "|---------------|-----------|----------|---------|-------|-------|------|\n",
    "| **cherry**    | 2         | 8        | 9       | 442   | 25    | 486  |\n",
    "| **strawberry**| 0         | 0        | 1       | 60    | 19    | 80  |\n",
    "| **digital**   | 1670      | 1683     | 85      | 5     | 4     | 3447  |\n",
    "| **information**| 3325     | 3982     | 378     | 5     | 13    | 7703  |\n",
    "| **count(context)**| 4997  | 5673     | 473     | 512   | 61    | 11716 |\n",
    "\n",
    "- **Word Similarity:**\n",
    "  - Words like \"cherry\" and \"strawberry\" are more similar because they share contexts like \"pie\" and \"sugar\".\n",
    "\n",
    "\n",
    "\n",
    "### **Dimensionality of Word Vectors**\n",
    "- The number of dimensions is generally the size of the vocabulary $|V|$\n",
    "  - often between 10,000 and 50,000 words, based on the most frequent words in the training corpus.\n",
    "- Most cells are zeros, resulting in `sparse` vector representations.\n",
    "- There are efficient algorithms for storing and computing with sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine for Measuring Similarity\n",
    "- `Cosine similarity` measures how similar two vectors are by calculating the cosine of the angle between them.\n",
    "- Commonly used to measure similarity between word or document vectors in NLP.\n",
    "- Requires two vectors of the same dimensionality, either:\n",
    "  - Both with words as dimensions (length |V|).\n",
    "  - Both with documents as dimensions (length |D|).\n",
    "\n",
    "\n",
    "### **The Dot Product**\n",
    "- The **dot product** of two vectors $v$ and $w$ is calculated as:\n",
    "  - $\\displaystyle\\text{dot product}(v, w) = v \\cdot w = \\sum_{i=1}^{N} v_i w_i = v_1w_1 + v_2w_2 + ... + v_N w_N$\n",
    "- Acts as a `similarity metric`\n",
    "  - high when vectors have large values in the same dimensions.\n",
    "  - 0 if vectors are orthogonal (`unrelated`)\n",
    "- **Issue:**\n",
    "  - The dot product favors longer vectors, resulting in higher similarity scores for frequent words.\n",
    "  - âˆµ Frequent words have longer vectors because they co-occur with more words, leading to inflated similarity scores.\n",
    "- **Solution:**\n",
    "  - Normalize the dot product by dividing by the lengths of both vectors.\n",
    "    - $\\displaystyle\\cos(v, w) = \\frac{v \\cdot w}{|v||w|}$\n",
    "    - Where:\n",
    "      - $\\displaystyle|v| = \\sqrt{\\sum_{i=1}^{N} v_i^2} \\quad \\text{and} \\quad |w| = \\sqrt{\\sum_{i=1}^{N} w_i^2}$\n",
    "\n",
    "\n",
    "### **Properties of Cosine Similarity**\n",
    "- Cosine similarity ranges from 0 to 1 for non-negative frequency vectors:\n",
    "  - **1:** Vectors are identical.\n",
    "  - **0:** Vectors are orthogonal (completely dissimilar).\n",
    "- **Advantages:**\n",
    "  - Measures similarity regardless of vector length (frequency).\n",
    "  - Useful in comparing high-dimensional, sparse vectors like word embeddings.\n",
    "\n",
    "\n",
    "### ðŸŽ **Example: Comparing Words with Cosine Similarity**\n",
    "- Given the term-term matrix:\n",
    "\n",
    "|         | pie | data | computer |\n",
    "|---------|-----|------|----------|\n",
    "| **cherry**     | 442 | 8    | 2        |\n",
    "| **digital**    | 5   | 1683 | 1670     |\n",
    "| **information**| 5   | 3982 | 3325     |\n",
    "\n",
    "\n",
    "- **Calculation: Cosine Similarity of Cherry and Information**\n",
    "  - $\\displaystyle\\cos(\\text{cherry}, \\text{information}) = \\frac{442 \\times 5 + 8 \\times 3982 + 2 \\times 3325}{\\sqrt{442^2 + 8^2 + 2^2} \\sqrt{5^2 + 3982^2 + 3325^2}}$\n",
    "  - $= 0.018$\n",
    "\n",
    "- **Calculation: Cosine Similarity of Digital and Information**\n",
    "  - $\\displaystyle\\cos(\\text{digital}, \\text{information}) = \\frac{5 \\times 5 + 1683 \\times 3982 + 1670 \\times 3325}{\\sqrt{5^2 + 1683^2 + 1670^2} \\sqrt{5^2 + 3982^2 + 3325^2}}$\n",
    "  - $= 0.996$\n",
    "\n",
    "- **Digital** is much closer in meaning to **information** than **cherry** is, based on cosine similarity.\n",
    "  - This result makes intuitive sense as \"digital\" and \"information\" frequently co-occur in similar contexts, unlike \"cherry.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "- A technique used to weigh terms in a document to evaluate how important a word is relative to a document or a collection.\n",
    "- Used to handle the limitations of raw frequency which is skewed and not discriminative.\n",
    "  - Words like **the**, **it**, and **they** appear frequently but are not informative.\n",
    "  - Frequent words across documents can obscure meaningful associations\n",
    "  - e.g. Words like \"good\" in Shakespeare's plays are frequent across all plays and don't help in distinguishing between them.\n",
    "\n",
    "\n",
    "### **Balancing Frequency with TF-IDF**\n",
    "- **The Paradox:**\n",
    "  - Rare words are important, but overly frequent words are not useful.\n",
    "- **TF-IDF Solution:**\n",
    "  - Combines **Term Frequency (TF)** and **Inverse Document Frequency (IDF)** to balance word frequency across documents.\n",
    "\n",
    "\n",
    "### **Term Frequency (TF)**\n",
    "- Measures the frequency of a word $t$ in a document $d$.\n",
    "  - $\\text{tf}_{t, d} = \\text{count}(t, d)$\n",
    "- **Log Scaling** is often applied to squash the frequency values:\n",
    "  - $\\text{tf}_{t, d} = \\begin{cases} \n",
    "  1 + \\log_{10} \\text{count}(t, d) & \\text{if } \\text{count}(t, d) > 0 \\\\ \n",
    "  0 & \\text{otherwise} \n",
    "  \\end{cases}$\n",
    "  - **Example Values:**\n",
    "    - 1 occurrence: $\\text{tf} = 1$\n",
    "    - 10 occurrences: $\\text{tf} = 2$\n",
    "    - 100 occurrences: $\\text{tf} = 3$\n",
    "- The **collection frequency** of a term is its total number of occurrences in the whole collection of documents. \n",
    "\n",
    "\n",
    "### **Inverse Document Frequency (IDF)**\n",
    "- Measures the importance of a term across all documents.\n",
    "- $\\text{idf}_t = \\log_{10} \\left( \\dfrac{N}{\\text{df}_t} \\right)$\n",
    "  - $N$ = Total number of documents.\n",
    "  - Document frequency $\\text{df}_t$ of a term $t$ = Number of documents containing term $t$.\n",
    "- Gives higher weight to terms appearing in fewer documents.\n",
    "\n",
    "\n",
    "### **Example of IDF in Shakespeare's Plays**\n",
    "\n",
    "| Word     | Document Frequency | IDF  |\n",
    "|----------|--------------------|------|\n",
    "| Romeo    | 1                  | 1.57 |\n",
    "| Salad    | 2                  | 1.27 |\n",
    "| Falstaff | 4                  | 0.967 |\n",
    "| Forest   | 12                 | 0.489 |\n",
    "| Battle   | 21                 | 0.246 |\n",
    "| Wit      | 34                 | 0.037 |\n",
    "| Fool     | 36                 | 0.012 |\n",
    "| Good     | 37                 | 0    |\n",
    "\n",
    "- Words like \"Romeo\" have high IDF and are discriminative, whereas \"good\" has an IDF of 0 and is non-informative.\n",
    "\n",
    "\n",
    "### **Calculating TF-IDF**\n",
    "- The tf-idf weighted value $w_{t, d}$ for word $t$ in document $d$ combines term\n",
    "frequency $tf_{t, d}$\n",
    "  - $w_{t, d} = \\text{tf}_{t, d} \\times \\text{idf}_t$\n",
    "- **Example Calculation:**\n",
    "  - **Word:** *Wit* in *As You Like It*\n",
    "  - **Term Frequency:** $1 + \\log_{10}(20) = 2.301$\n",
    "  - **IDF:** $0.037$\n",
    "  - **TF-IDF Weight:** $2.301 \\times 0.037 = 0.085$\n",
    "- The weight of \"wit\" is reduced significantly, reflecting its high frequency across many plays.\n",
    "\n",
    "\n",
    "### **Benefits of TF-IDF Weighting**\n",
    "- **Advantages:**\n",
    "  - Reduces the impact of common, non-informative words.\n",
    "  - Highlights unique terms that distinguish documents from each other.\n",
    "- **Applications:**\n",
    "  - Commonly used in information retrieval and text mining.\n",
    "  - Provides a strong baseline for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pointwise Mutual Information (PMI)**\n",
    "- A statistical measure used to evaluate the association between two events $x$ and $y$ (e.g., words) by comparing their observed co-occurrence to what would be expected if they were independent.\n",
    "  - $\\displaystyle \\text{PMI}(x, y) = \\log_2 \\left( \\frac{P(x, y)}{P(x)P(y)} \\right)$\n",
    "- **Intuition:**\n",
    "  - If two words co-occur frequently but occur independently with low probability, PMI will be high.\n",
    "  - For example, in a text corpus, if \"cat\" and \"fur\" appear together more often than independently, PMI will indicate a strong association.\n",
    "- PMI is frequently used to quantify the relationship between a **target word** $w$ (e.g., \"cat\") and a **context word** $c$ (e.g., \"fur\").\n",
    "  - $\\displaystyle \\text{PMI}(w, c) = \\log_2 \\left( \\frac{P(w, c)}{P(w)P(c)} \\right)$\n",
    "  1. $P(w, c)$: Probability of observing both the target word $w$ and context word $c$ together.\n",
    "  2. $P(w)P(c)$: Probability of observing $w$ and $c$ independently.\n",
    "- **Interpretation:**\n",
    "  - PMI > 0: Words co-occur more often than by chance (positive association).\n",
    "  - PMI = 0: Words co-occur as expected by chance.\n",
    "  - PMI < 0: Words co-occur less often than by chance (negative association).\n",
    "\n",
    "\n",
    "\n",
    "### ðŸŽ**Calculating PMI for 'information' and 'data':**\n",
    "1. **Co-occurrence Probability:**\n",
    "   - $\\displaystyle P(w=\\text{information}, c=\\text{data}) = \\frac{3982}{11716} = 0.3399$\n",
    "\n",
    "2. **Individual Probabilities:**\n",
    "   - $\\displaystyle P(w=\\text{information}) = \\frac{7703}{11716} = 0.6575$\n",
    "   - $\\displaystyle P(c=\\text{data}) = \\frac{5673}{11716} = 0.4842$\n",
    "\n",
    "3. **PMI Calculation:**\n",
    "   - $\\displaystyle \\text{PMI}(\\text{information}, \\text{data}) = \\log_2 \\left( \\frac{0.3399}{0.6575 \\times 0.4842} \\right) = 0.0944$\n",
    "\n",
    "- The PMI value of 0.0944 suggests a slight positive association between \"information\" and \"data\" in this context.\n",
    "\n",
    "\n",
    "\n",
    "### **Challenges with PMI**\n",
    "\n",
    "- **Issues with Negative PMI:**\n",
    "  - Negative PMI values suggest that two words co-occur less often than expected by chance, but these values can be unreliable unless the corpus is extremely large.\n",
    "  - For example, distinguishing whether two words with very low probabilities occur together less often than by chance would require vast amounts of data.\n",
    "\n",
    "- **Bias Towards Infrequent Events:**\n",
    "  - PMI tends to give high values for rare word pairs even if their co-occurrence is not meaningful.\n",
    "  - Example: A rare term like \"quantum\" paired with \"entanglement\" might have an inflated PMI due to their low individual frequencies.\n",
    "\n",
    "- **Impact on NLP Models:**\n",
    "  - This bias can lead to incorrect conclusions about word associations and affect downstream tasks such as topic modeling or semantic similarity.\n",
    "\n",
    "\n",
    "\n",
    "### **Positive Pointwise Mutual Information (PPMI)**\n",
    "\n",
    "- **Positive PMI (PPMI)** sets all negative PMI values to zero, making it more robust against unreliable associations.\n",
    "\n",
    "  - $\\displaystyle \\text{PPMI}(w, c) = \\max \\left( \\log_2 \\left( \\frac{P(w, c)}{P(w)P(c)} \\right), 0 \\right)$\n",
    "\n",
    "- **Why Use PPMI?**\n",
    "  - It focuses only on positive associations, eliminating misleading negative values.\n",
    "  - PPMI is commonly used in word embeddings and vector space models, where negative PMI values do not contribute meaningful information.\n",
    "\n",
    "- **Example:**\n",
    "  - If PMI for (word1, word2) is -2, PPMI sets it to 0.\n",
    "  - If PMI for (word1, word2) is 4, PPMI retains the value of 4.\n",
    "\n",
    "\n",
    "\n",
    "### **Constructing the PPMI Matrix**\n",
    "\n",
    "- A co-occurrence matrix $F$ has:\n",
    "  - **W rows:** Represent words in the vocabulary.\n",
    "  - **C columns** Represent contexts (surrounding words).\n",
    "  - Each cell $f_{ij}$ in $F$ indicates the number of times word $w_i$ appears within context $c_j$.\n",
    "\n",
    "- **Constructing the PPMI Matrix:**\n",
    "1. Calculate the joint probability $p_{ij}$ of $w_i$ and $c_j$:\n",
    "   - $\\displaystyle p_{ij}=\\frac{f_{ij}}{âˆ‘_{i=1}^W âˆ‘_{j=1}^C f_{ij}}$\n",
    "2. Calculate the marginal probabilities of word $p_{i*}$ and context $p_{*j}$:\n",
    "   - $\\displaystyle p_{i*} = \\dfrac{âˆ‘_{j=1}^C f_{ij}}{ âˆ‘_{i=1}^W âˆ‘_{j=1}^C f_{ij}} â€ƒ\\quad  p_{*j} = \\dfrac{âˆ‘_{i=1}^W f_{ij}}{ âˆ‘_{i=1}^W âˆ‘_{j=1}^C f_{ij}}$ \n",
    "3. Compute: $\\displaystyle \\text{PPMI}_{ij} = \\max \\left( \\log_2 \\left( \\frac{p_{ij}}{p_{i*} \\times p_{*j}} \\right), 0 \\right)$\n",
    "\n",
    "- The resulting PPMI matrix highlights the strength of association between words and their contexts.\n",
    "\n",
    "\n",
    "\n",
    "### **Example PPMI Calculations**\n",
    "\n",
    "| `w\\P(w,c)\\c`| Computer | Data   | Result | Pie    | Sugar  | p(w)  |\n",
    "|-------------|----------|--------|--------|--------|--------|-------|\n",
    "| Cherry      | 0.0002   | 0.0007 | 0.0008 | 0.0377 | 0.0021 | 0.0415|\n",
    "| Strawberry  | 0.0000   | 0.0000 | 0.0001 | 0.0051 | 0.0016 | 0.0068|\n",
    "| Digital     | 0.1425   | 0.1436 | 0.0073 | 0.0004 | 0.0003 | 0.2942|\n",
    "| Information | 0.2838   | 0.3399 | 0.0323 | 0.0004 | 0.0011 | 0.6575|\n",
    "| p(c)        | 0.4265   | 0.4842 | 0.0404 | 0.0437 | 0.0052 |       |\n",
    "\n",
    "\n",
    "| Word       | Computer | Data | Result | Pie  | Sugar |\n",
    "|------------|------|------|--------|------|-------|\n",
    "| Cherry     | 0    | 0    | 0      | 4.38 | 3.30  |\n",
    "| Strawberry | 0    | 0    | 0      | 4.10 | 5.51  |\n",
    "| Digital    | 0.18 | 0.01 | 0      | 0    | 0     |\n",
    "| Information| 0.02 | 0.09 | 0.28   | 0    | 0     |\n",
    "\n",
    "- Cherry and strawberry are highly associated with \"pie\" and \"sugar,\" indicating strong co-occurrence.\n",
    "- Digital has a weaker association with \"data,\" possibly due to its broader usage context.\n",
    "\n",
    "\n",
    "\n",
    "### **Handling PMI Bias with $\\alpha$-Smoothing**\n",
    "- Rare contexts can disproportionately inflate PMI values.\n",
    "- **Solution:**\n",
    "  - Modify context probability using $\\alpha$-smoothing:\n",
    "  - $\\displaystyle \\text{PPM}I_{\\alpha}(w, c) = \\max \\left( \\log_2 \\left( \\frac{P(w, c)}{P(w)P^{\\alpha}(c)} \\right), 0 \\right)$\n",
    "  - **$P^{\\alpha}(c) = \\dfrac{count(c)^Î±}{Î£_{c}count(c)^Î±}$:** Raises context probability $P(c)$ to a power $\\alpha$.\n",
    "  - Setting $\\alpha = 0.75$ gives a balanced reduction in bias, particularly for rare contexts.\n",
    "- Reduces high PMI values for rare word-context pairs, providing more meaningful associations.\n",
    "\n",
    "\n",
    "\n",
    "### **Another Solution:Laplace Smoothing for PMI**\n",
    "- **Laplace Smoothing**: Add a small constant $k$ (e.g., 0.1 to 3) to each count before calculating PMI.\n",
    "- **Why Laplace Smoothing?**\n",
    "  - Helps reduce the impact of zero or low-frequency counts in co-occurrence matrices.\n",
    "  - Larger $k$ values discount more, reducing PMI bias further.\n",
    "- **Modified PMI Formula:**\n",
    "  - $\\displaystyle \\text{PMI}(w, c) = \\log_2 \\left( \\frac{f(w, c) + k}{(f(w) + k)(f(c) + k)} \\right)$\n",
    "- **Choosing $k$:**\n",
    "  - Smaller values maintain original PMI properties; larger values decrease variability in low-frequency pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of tf-idf and PPMI Vector Models\n",
    "\n",
    "- **Vector Representation:**\n",
    "  - Target word represented as a vector.\n",
    "  - Dimensions correspond to either:\n",
    "    - Documents in a collection (term-document matrix).\n",
    "    - Counts of neighboring words (term-term matrix).\n",
    "  - Values in dimensions weighted by:\n",
    "    - **tf-idf** (for term-document matrices).\n",
    "    - **PPMI** (for term-term matrices).\n",
    "  - Vectors are sparse (mostly zeros).\n",
    "\n",
    "- **Similarity Computation:**\n",
    "  - Similarity between two words $x$ and $y$ computed using cosine similarity.\n",
    "  - High cosine value indicates high similarity.\n",
    "  - Referred to as the **tf-idf model** or **PPMI model** based on the weighting function.\n",
    "\n",
    "\n",
    "### **Applications of tf-idf and PPMI Models in Document and Word Similarity**\n",
    "\n",
    "- **Document Similarity:**\n",
    "  - **Document Vector:** \n",
    "    - Represented by the centroid of word vectors in the document.\n",
    "    - Centroid minimizes the sum of squared distances to each vector.\n",
    "    - Formula: $\\mathbf{d} = \\dfrac{\\mathbf{w}_1 + \\mathbf{w}_2 + ... + \\mathbf{w}_k}{k}$\n",
    "  - Compute similarity between two documents $\\mathbf{d}_1$ and $\\mathbf{d}_2$ using cosine similarity.\n",
    "  - Applications:\n",
    "    - Information retrieval.\n",
    "    - Plagiarism detection.\n",
    "    - News recommendation.\n",
    "    - Digital humanities (comparing texts).\n",
    "\n",
    "- **Word Similarity:**\n",
    "  - Use PPMI or tf-idf models to find word paraphrases.\n",
    "  - Track changes in word meanings.\n",
    "  - Discover meanings in different corpora.\n",
    "  - Find top 10 similar words by cosine similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Problems of Traditional Word Representations**\n",
    "- **Sparse Vectors:**\n",
    "  - Words represented as long vectors with many zeros.\n",
    "  - Dimensions correspond to either words in the vocabulary or documents in a collection.\n",
    "- **Challenges:**\n",
    "  - High dimensionality (e.g., 50,000+ dimensions).\n",
    "  - Difficult to capture the relationship between similar words.\n",
    "  - Computationally inefficient due to high sparsity and large size.\n",
    "\n",
    "\n",
    "\n",
    "### **The Solution: Embeddings**\n",
    "- **Dense Vectors:**\n",
    "  - Represent words as short vectors with real-valued numbers.\n",
    "  - Typically 50-1000 dimensions instead of large vocabulary size.\n",
    "- **Characteristics:**\n",
    "  - Values can be positive or negative, capturing subtle nuances.\n",
    "  - Dense and compact, unlike sparse vectors.\n",
    "- **Implication:**\n",
    "  - Easier for models to learn and generalize from, leading to better performance in NLP tasks.\n",
    "\n",
    "\n",
    "### **Word2Vec and SGNS**\n",
    "- Word2Vec is a tool that generates word embeddings using the Skip-Gram with Negative Sampling (SGNS) model.\n",
    "- **Static Embeddings:**\n",
    "  - Each word has one fixed vector (static embedding), regardless of context.\n",
    "\n",
    "\n",
    "\n",
    "### **How Does Word2Vec Work?**\n",
    "- **Intuition:**\n",
    "  - Instead of counting word co-occurrences, we train a classifier to predict if two words are likely to appear together.\n",
    "- **Self-Supervision:**\n",
    "  - No labeled data is needed; the context of each word in natural text serves as training data.\n",
    "- **Output:**\n",
    "  - The classifier weights become the word embeddings, representing each word in a dense vector form.\n",
    "\n",
    "\n",
    "\n",
    "### **Simplification of Word2Vec**\n",
    "- **Task Simplification:**\n",
    "  - Binary classification (\"Is word A likely to appear near word B?\") rather than predicting the next word in a sequence.\n",
    "- **Architecture Simplification:**\n",
    "  - Uses logistic regression (a single-layer model) instead of complex multi-layer neural networks.\n",
    "- **Advantages:**\n",
    "  - Easier to train and understand.\n",
    "  - Requires fewer computational resources compared to more complex models.\n",
    "\n",
    "\n",
    "\n",
    "### **Skip-Gram Algorithm Steps**\n",
    "1. **Positive Examples:**\n",
    "   - Pair a target word with its context words (e.g., \"apple\" with \"fruit\").\n",
    "2. **Negative Sampling:**\n",
    "   - Randomly select words from the vocabulary that do not appear in the context to create negative examples.\n",
    "3. **Classifier Training:**\n",
    "   - Use logistic regression to train the classifier to distinguish between positive and negative samples.\n",
    "4. **Embedding Learning:**\n",
    "   - The weights learned by the classifier for each word pair are used as the final word embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Classification Task**\n",
    "- **Example Sentence:** \"... lemon, a tablespoon of apricot jam, a pinch ...\"\n",
    "- **Target Word $w$:** \"apricot\"\n",
    "- **Context Words $c_1, c_2, c_3, c_4$:** \"tablespoon,\" \"of,\" \"jam,\" \"a\"\n",
    "- **Objective:** Train a classifier to predict if a given word $c$ is a real context word for the target word (w).\n",
    "- **Example Tuples:** \n",
    "  - Target word: â€œapricotâ€\n",
    "  - Positive examples (words in context): (apricot, jam) â†’ **True Context**\n",
    "  - Negative examples (random words): (apricot, aardvark) â†’ **False Context**\n",
    "\n",
    "\n",
    "\n",
    "### **Calculating Probability of Context**\n",
    "- Estimate the probability that a word $c$ is a real context word for $w$, denoted as $P(+|w, c)$.\n",
    "  - $P(âˆ’|w, c) = 1 âˆ’ P(+|w, c)$\n",
    "- The probability is based on the similarity between the embeddings of $w$ and $c$.\n",
    "- The similarity can be measured by the dot product of the embeddings of $w$ and $c$:\n",
    "  - $\\text{Similarity}(w, c) â‰ˆ c Â· w$\n",
    "    - Ranges from $âˆ’âˆž$ to $âˆž$.\n",
    "    - Positive values indicate similar vectors, negative values indicate dissimilar vectors.\n",
    "- Convert similarity to probability with sigmoid (logistic) function \n",
    "  - $P(+|w, c) = Ïƒ(c Â· w)$\n",
    "  - $Ïƒ(x) = \\dfrac{1}{1 + e^{-x}}$\n",
    "\n",
    "\n",
    "### **Handling Multiple Context Words with Skip-Gram**\n",
    "- **Assumption:** Context words are independent.\n",
    "  - **Probability of Entire Context Window:**\n",
    "    - $\\displaystyle P(+|w, c_{1:L}) = âˆ_{i=1}^L (Ïƒ(c_i Â· w))$, for $i = 1$ to $L$\n",
    "  - **Log Probability:**\n",
    "    - $\\displaystyle \\log P(+|w, c_{1:L}) = Î£_{i=1}^L (\\log Ïƒ(c_i Â· w))$, for $i = 1$ to $L$\n",
    "\n",
    "\n",
    "### **Learning the Classifier Parameters of the Skip-Gram Model**\n",
    "  - Skip-gram stores two embeddings for each word\n",
    "    - one for the word as a target: the `target embedding` or `input embedding`\n",
    "    - one for the word considered as context: the `context embedding` or `output embedding`\n",
    "  - The parameters need to learn are two matrices:\n",
    "    - the target embeddings $W$ and the context+noise embeddings  $C$, \n",
    "    - each containing an embedding for every one of the $|V|$ words in the vocabulary $V$\n",
    "\n",
    "![skipgram](./images/skipgram.png)\n",
    "\n",
    "- The parameter $Î¸$ is a matrix of $2|V|$ vectors formed by concatenating $W$ and $C$,\n",
    "  - each of dimension d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Skip-Gram with Negative Sampling (SGNS)**\n",
    "- For each positive example $(w, c_{+}), k$ negative samples are generated.\n",
    "- Noise words are chosen from the lexicon based on a weighted unigram frequency \n",
    "  - $\\displaystyle p_{\\alpha}(w) = \\dfrac{count(w)^{\\alpha}}{\\Sigma_{w'}count(w')^{\\alpha}}$.\n",
    "  - Weighting parameter $Î± = 0.75$ increases the probability of selecting rare words as noise words.\n",
    "  - ðŸŽ For two events, P(a) = 0.99 and P(b) = 0.01.\n",
    "  - With Î± = 0.75, the probability of the rare event (b) increases to 0.03.\n",
    "    - $\\displaystyle p_{\\alpha}(a) = \\dfrac{0.99^{0.75}}{0.99^{0.75} + 0.01^{0.75}} = 0.97$\n",
    "    - $\\displaystyle p_{\\alpha}(a) = \\dfrac{0.01^{0.75}}{0.99^{0.75} + 0.01^{0.75}} = 0.03$\n",
    "- This increases the chance of selecting rare noise words to improve performance.\n",
    "\n",
    "\n",
    "### **Loss Function for Skip-Gram**\n",
    "- $\\displaystyle L = - \\log \\left( P(+|w, c_{+}) \\times \\prod_{i=1}^{k} P(-|w, c_{-_i}) \\right)$\n",
    "- $\\displaystyle = - \\left( \\log \\sigma(c_{+} \\cdot w) + \\Sigma_{i=1}^k \\log\\sigma(-c_{-_i} \\cdot w) \\right)$\n",
    "\n",
    "  - Maximize similarity between target word and context words.\n",
    "  - Minimize similarity between target word and negative samples (non-neighbors).\n",
    "\n",
    "\n",
    "\n",
    "### **Optimizing Skip-Gram with SGD**\n",
    "- Loss function is minimized using Stochastic Gradient Descent (SGD).\n",
    "\n",
    "![sgd one step](./images/sgdonestep.png)\n",
    "\n",
    "- Embedding space adjustment\n",
    "  - Positive example: Move â€œapricotâ€ and â€œjamâ€ closer.\n",
    "  - Negative example: Move â€œapricotâ€ and â€œmatrixâ€ further apart.\n",
    "\n",
    "\n",
    "\n",
    "### **Derivatives and Updates**\n",
    "- Derivatives of the loss function:\n",
    "  - $\\displaystyle \\frac{\\partial L}{\\partial c_{+}} = [\\sigma(c_{+} \\cdot w) - 1] w$\n",
    "  - $\\displaystyle \\frac{\\partial L}{\\partial c_{-}} = [\\sigma(c_{-} \\cdot w)] w$\n",
    "  - $\\displaystyle \\frac{\\partial L}{\\partial w} = [\\sigma(c_{+} \\cdot w) - 1]c_{+} + \\sum_{i=1}^k [\\sigma(c_{-_i} \\cdot w)]c_{-_i}$\n",
    "- Update rules for gradient descent (with learning rate Î·):\n",
    "  - - $w^{t+1} = w^t - \\eta \\left( [\\sigma(c_{+}^t \\cdot w^t) - 1] c_{+}^t + \\sum_{i=1}^k [\\sigma(c_{-_i}^t \\cdot w^t)] c_{-_i}^t \\right)$\n",
    "\n",
    "- Common approach: \n",
    "  - representing word i with the vector: $w_i + c_i$, \n",
    "  - or use only the target embedding: $w_i$.\n",
    "\n",
    "\n",
    "### **Effect of Context Window Size**\n",
    "- The size of the context window (L) affects the quality of embeddings.\n",
    "- Larger windows capture broader contexts but may include unrelated words.\n",
    "- Optimal window size is often tuned on a development dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Kinds of Static embeddings: FastText and GloVe.\n",
    "\n",
    "### **FastText: Handling Unknown Words and Word Sparsity**\n",
    "- **Problem with Word2Vec**:\n",
    "  - **Unknown words**: Word2Vec struggles with unseen words in a test corpus.\n",
    "  - **Word sparsity**: In languages with rich morphology, some word forms appear rarely.\n",
    "- **FastText Approach**:\n",
    "  - Extends Word2Vec by representing words as a combination of the word itself and its subword units (n-grams).\n",
    "  - Example with n = 3 for the word \"where\":\n",
    "    - Word: `<where>`\n",
    "    - n-grams: `<wh, whe, her, ere, re>`\n",
    "- **How FastText Works**:\n",
    "  - Learns skip-gram embeddings for each constituent n-gram.\n",
    "  - Word representation = sum of all its n-gram embeddings.\n",
    "  - **Unknown words** are represented by their n-grams, even if the full word is unseen.\n",
    "- **FastText Availability**:\n",
    "  - Open-source library with pretrained embeddings for 157 languages: [https://fasttext.cc](https://fasttext.cc)\n",
    "\n",
    "\n",
    "\n",
    "### **GloVe: Capturing Global Word Co-occurrence Statistics**\n",
    "\n",
    "  - **GloVe Overview**:\n",
    "    - Short for **Global Vectors**, GloVe captures **global corpus statistics**.\n",
    "    - Unlike Word2Vec, GloVe focuses on co-occurrence probabilities between words.\n",
    "  - **How GloVe Works**:\n",
    "    - Builds a **word-word co-occurrence matrix** from the corpus.\n",
    "    - Uses **probability ratios** to capture word relationships, blending ideas from count-based models like **PPMI** and prediction-based models like Word2Vec.\n",
    "  - **Mathematical Connection**:\n",
    "    - Dense embeddings from Word2Vec have an **elegant relationship** with sparse embeddings like PPMI.\n",
    "    - Word2Vec can be viewed as implicitly optimizing a function of the PPMI matrix.\n",
    "  - **Use Cases**:\n",
    "    - GloVe has been widely adopted in tasks requiring robust static embeddings for semantic similarity and global word relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Embeddings\n",
    "- **Challenge**: Embedding vectors have high dimensions (e.g., 100D). Visualizing them helps us understand word relationships.\n",
    "- **Similarity-based Visualization**:\n",
    "  - Most similar words to a target word (e.g., **frog**):\n",
    "    - Closest words: **frogs, toad, litoria, rana, lizard**\n",
    "  - Based on sorting word vectors by cosine similarity.\n",
    "- **Clustering Visualization**:\n",
    "  - Use **hierarchical clustering** to show relationships between nouns in the embedding space.\n",
    "  - Groups similar words together in a tree-like structure.\n",
    "- **Projection Visualization (t-SNE)**:\n",
    "  - `t-SNE`: t-distributed Stochastic Neighbor Embedding\n",
    "  - Most common method: Project high-dimensional vectors into 2D using **t-SNE**.\n",
    "  - t-SNE captures local and global structure of the embeddings in a 2D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Properties of Embeddings\n",
    "- **Context Window Size**:\n",
    "  - Smaller windows (e.g., Â±2 words) capture **syntactic similarity**\n",
    "    - e.g., similar parts of speech\n",
    "  - Larger windows (e.g., Â±5 words) capture **topical or semantic similarity**.\n",
    "- **Examples**:\n",
    "  - **Window Â±2**: *Hogwarts* â†’ *Sunnydale*, *Evernight* \n",
    "    - similar fictional schools\n",
    "  - **Window Â±5**: *Hogwarts* â†’ *Dumbledore*, *Malfoy*, *half-blood*\n",
    "    - related to Harry Potter\n",
    "- **Types of Word Association**:\n",
    "  - **First-order co-occurrence**: Words that occur nearby\n",
    "    - e.g., *wrote* with *book* or *poem*\n",
    "  - **Second-order co-occurrence**: Words with similar neighbors \n",
    "    - e.g., *wrote* with *said* or *remarked*\n",
    "\n",
    "\n",
    "### **Analogy and Relational Similarity**\n",
    "- **Parallelogram Model**:\n",
    "  - Used to solve analogies: *apple:tree::grape:vine*.\n",
    "    - Given $a,b,a^*$ in $a : b :: a^* : b^*$, $b^*$ can be found by\n",
    "      - $\\hat{b^*} = \\arg\\max_{x} \\text{distance}(x, b-a+a^*)$\n",
    "  - The vector from *apple* to *tree* is applied to *grape* to find *vine*.\n",
    "- **Word2Vec/GloVe Success**:\n",
    "    - **king** - **man** + **woman** â‰ˆ **queen**.\n",
    "    - **Paris** - **France** + **Italy** â‰ˆ **Rome**.\n",
    "  - Embeddings capture relations like **MALE/FEMALE**, **CAPITAL/CITY**.\n",
    "- **Limitations**:\n",
    "  - May return input words or morphological variants \n",
    "    - e.g., *cherry:red* :: *potato:x* â†’ *potato*\n",
    "  - Performs well on frequent words and common relations but struggles with more complex associations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings and Historical Semantics\n",
    "- **Analyzing Semantic Shifts**:\n",
    "  - Embeddings from different time periods show how word meanings evolve.\n",
    "  - Separate embedding spaces are built for each decade from historical corpora (e.g., Google N-grams, Corpus of Historical American English).\n",
    "- **t-SNE Visualization Example**:\n",
    "  - Changes in word meanings over two centuries (e.g., *gay*, *broadcast*, *awful*).\n",
    "  - **Gay**: Shift from \"cheerful\" to referring to homosexuality.\n",
    "  - **Broadcast**: Shift from \"casting seeds\" to \"transmitting signals.\"\n",
    "  - **Awful**: Shift from \"full of awe\" to \"terrible.\"\n",
    "- **Tools**:\n",
    "  - t-SNE (dimensionality reduction) used to visualize these shifts in embedding spaces."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
