{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/02.nglm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/02.nglm.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# N-gram `Language Models (LMs)`\n",
    "\n",
    "üìù SALP chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î An intriguing question\n",
    "- What will you `likely` fill in the blank below?\n",
    "- **I dreamed ___________.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Purpose of Language Models**\n",
    "   - **Probability Assignment:** LMs assign a `probability` to `each possible next word, phrase, or sentence` based on `the context provided by the preceding words`.\n",
    "     - computes $P(w|c)$, the `probability` of a word $w$ given some context $c$.\n",
    "   - **Applications:** Used in various natural language processing (NLP) tasks like \n",
    "     - speech recognition, \n",
    "     - machine translation, \n",
    "     - text generation and autocomplete,\n",
    "     - augmentative and alternative communication (aac) systems, etc.\n",
    "\n",
    "## **Types of Language Models**\n",
    "   - **Unigram Model:**\n",
    "     - **Description:** Assumes each word is `independent` of the previous words.\n",
    "       - Which is unlikely in NLP.\n",
    "     - **Example:** Probability of the sentence \"I love NLP\" is $P(\\text{\"I\"}) \\times P(\\text{\"love\"}) \\times P(\\text{\"NLP\"})$.\n",
    "     - **Limitation:** Ignores context, leading to unrealistic predictions.\n",
    "   \n",
    "   - **N-gram Model:**\n",
    "     - **Description:** Considers the context of the previous $N-1$ words.\n",
    "     - **Example:**\n",
    "       - **Bigram Model (N=2):** $P(\\text{\"love\"} | \\text{\"I\"}) \\times P(\\text{\"NLP\"} | \\text{\"love\"})$\n",
    "       - **Trigram Model (N=3):** $P(\\text{\"NLP\"} | \\text{\"I love\"})$\n",
    "     - **Usage:** \n",
    "       - N-gram statistics capture diverse linguistic phenomena, including: \n",
    "         - syntactic patterns, task-specific tendencies, and cultural preferences. \n",
    "       - These probabilities reflect: \n",
    "         - grammatical rules, domain-specific language use, and societal trends in word co-occurrences.\n",
    "     - **Padding**: Extra context marks are needed at the start and end of sentences for longer n-grams\n",
    "       - e.g with trigrams, we use two special markers `<s>` before the first word to calculate `ùêè(\"I\"|<s><s>)`\n",
    "     - **Limitation:** The model might struggle with `rare or unseen` word sequences due to `data sparsity`.\n",
    "\n",
    "   - **Neural Language Models:**\n",
    "     - **Recurrent Neural Networks (RNNs):**\n",
    "       - **Description:** Uses a feedback loop to take into account all previous words, maintaining context across varying lengths.\n",
    "       - **Example:** Predicts the next word in a sequence like \"I love\" by using hidden states that summarize previous words.\n",
    "       - **Limitation:** Can suffer from issues like vanishing gradients, especially with long dependencies.\n",
    "     \n",
    "     - **Transformers:**\n",
    "       - **Description:** Uses attention mechanisms to capture long-range dependencies without the sequential nature of RNNs.\n",
    "       - **Example:** Models like GPT, BERT use transformers to predict the next word or even fill in masked words in a sentence.\n",
    "       - **Advantage:** Handles long-term dependencies more effectively and is highly parallelizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Probability of a sentence**\n",
    "- A sentence $W$ is a sequence of words appearing together in a specific order: $w_1w_2w_3\\cdots w_n$\n",
    "- The probability of a sentence is the `joint probability` of its words: $P(W)=P(w_1,w_2,w_3,\\cdots, w_n)$\n",
    "  - $P(\\text{\"I love NLP\"}) = P(\\text{\"I\"} \\cap \\text{\"love\"} \\cap \\text{\"NLP\"})$\n",
    "- $P(W)$ can be calculated with the `Chain Rule of Probability`:\n",
    "  - $P(w_1, w_2, \\dots, w_n) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1, w_2) \\times \\dots \\times P(w_n | w_1, w_2, \\dots, w_{n-1})$\n",
    "  - ü§î How to calculate each term on the right side? \n",
    "  - ü§î How to simplify this formula?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating N-gram Probabilities with Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "- **Maximum Likelihood Estimation (MLE)** estimates the probability of an n-grams based on its `frequencies` in a corpus, \n",
    "  - by dividing the frequency or count of that n-gram by the frequency of the (n-1)-gram that precedes it.\n",
    "  - The resulting parameter set of MLE `maximizes the likelihood` of the training set $T$ given the model $M$ (i.e., $P(T|M)$).\n",
    "- **Unigram Probability:** The probability of a single word (unigram) =\n",
    "  -  the frequency $C(w_i)$ of that word $w_i$ √∑ the total number $N$ of words in the corpus.\n",
    "  -  $\\displaystyle P(w_i) = \\frac{C(w_i)}{N}$\n",
    "- **Bigram Probability:** The probability of a word given the previous word (bigram) = \n",
    "  - the frequency $C(w_{i-1}, w_i)$ of the bigram $(w_{i-1}, w_i)$ √∑ the frequency $C(w_{i-1})$ of the first word $w_{i-1}$ in the bigram.\n",
    "  - $\\displaystyle P(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{‚àë_{w} C(w_{i-1}, w)} = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$\n",
    "  -  This ratio is called a `relative frequency`\n",
    "- **Trigram Probability:** The probability of a word given the previous two words (trigram) =\n",
    "  - the frequency $C(w_{i-2}, w_{i-1}, w_i)$ of the trigram $(w_{i-2}, w_{i-1}, w_i)$ √∑ the frequency $C(w_{i-2}, w_{i-1})$ of the first two words $(w_{i-2}, w_{i-1})$ in the trigram.\n",
    "  - $\\displaystyle P(w_i \\mid w_{i-2}, w_{i-1}) = \\frac{C(w_{i-2}, w_{i-1}, w_i)}{C(w_{i-2}, w_{i-1})}$\n",
    "\n",
    "\n",
    "### üçé **Example**\n",
    "Given a simple corpus:\n",
    "\n",
    "```\n",
    "corpus = [\"I love NLP\", \"I love coding\", \"NLP is fun\", \"I love Python\", \"Python is great\"]\n",
    "```\n",
    "\n",
    "For the bigram \"I love\", the bigram count $C(\"I\", \"love\")$ might be 3, and the unigram count $C(\"I\")$ might be 3 as well. The bigram probability $P(\"love\" \\mid \"I\")$ would then be:\n",
    "\n",
    "$\\displaystyle P(\"love\" \\mid \"I\") = \\frac{C(\"I\", \"love\")}{C(\"I\")} = \\frac{3}{3} = 1.0$\n",
    "\n",
    "### üí° **A Simple Python Demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_corpus=[['i', 'love', 'nlp'], ['i', 'love', 'coding'], ['nlp', 'is', 'fun'], ['i', 'love', 'python'], ['python', 'is', 'great']]\n",
      "unigram_counts=Counter({'i': 3, 'love': 3, 'nlp': 2, 'is': 2, 'python': 2, 'coding': 1, 'fun': 1, 'great': 1})\n",
      "bigram_counts=Counter({('i', 'love'): 3, ('love', 'nlp'): 1, ('love', 'coding'): 1, ('nlp', 'is'): 1, ('is', 'fun'): 1, ('love', 'python'): 1, ('python', 'is'): 1, ('is', 'great'): 1})\n",
      "trigram_counts=Counter({('i', 'love', 'nlp'): 1, ('i', 'love', 'coding'): 1, ('nlp', 'is', 'fun'): 1, ('i', 'love', 'python'): 1, ('python', 'is', 'great'): 1})\n",
      "P(love) = 0.2\n",
      "P(love | i) = 1.0\n",
      "P(nlp | ('i', 'love')) = 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qingshan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Small corpus\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"I love coding\",\n",
    "    \"NLP is fun\",\n",
    "    \"I love Python\",\n",
    "    \"Python is great\",\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "print(f\"{tokenized_corpus=}\")\n",
    "\n",
    "# Calculate unigram and bigram counts\n",
    "unigram_counts = Counter([word for sentence in tokenized_corpus for word in sentence])\n",
    "bigram_counts = Counter([(sentence[i], sentence[i+1]) for sentence in tokenized_corpus for i in range(len(sentence)-1)])\n",
    "trigram_counts = Counter([(sentence[i], sentence[i+1], sentence[i+2]) for sentence in tokenized_corpus for i in range(len(sentence)-2)])\n",
    "\n",
    "\n",
    "# Total number of unigrams in the corpus\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "# Function to calculate unigram probability\n",
    "def unigram_probability(word):\n",
    "    return unigram_counts[word] / total_unigrams\n",
    "\n",
    "# Function to calculate bigram probability\n",
    "def bigram_probability(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1]\n",
    "\n",
    "# Function to calculate bigram probability\n",
    "def trigram_probability(w1, w2, w3):\n",
    "    return trigram_counts[(w1, w2, w3)] / bigram_counts[(w1, w2)]\n",
    "\n",
    "print(f\"{unigram_counts=}\")\n",
    "print(f\"{bigram_counts=}\")\n",
    "print(f\"{trigram_counts=}\")\n",
    "\n",
    "# Example of unigram probability\n",
    "word = \"love\"\n",
    "print(f\"P({word}) = {unigram_probability(word)}\")\n",
    "\n",
    "# Example of bigram probability\n",
    "w1, w2 = \"i\", \"love\"\n",
    "print(f\"P({w2} | {w1}) = {bigram_probability(w1, w2)}\")\n",
    "\n",
    "# Example of bigram probability\n",
    "w1, w2, w3 = \"i\", \"love\", \"nlp\"\n",
    "print(f\"P({w3} | {w1,w2}) = {trigram_probability(w1, w2, w3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the probability of a sentence using n-grams\n",
    "- Calculate the product of the conditional probabilities of each word given its preceding $n-1$ words. \n",
    "- This process involves breaking down the sentence into a series of `n-grams` and \n",
    "  - then using their respective probabilities to compute the `overall sentence probability` $P(W)$.\n",
    "- **Unigram Model:**\n",
    "  - $P(W)$ = the product of the probabilities of each individual word.\n",
    "  - $P(W) = P(w_1) \\times P(w_2) \\times \\dots \\times P(w_T)$\n",
    "- **Bigram Model:** \n",
    "  - $P(W)$ = the product of the probabilities of each word given the previous word.\n",
    "  - $P(W) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\times \\dots \\times P(w_T \\mid w_{T-1})$\n",
    "- **Trigram Model:** \n",
    "  - $P(W)$ = the product of the probabilities of each word given the previous two words.\n",
    "  - $P(W) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_1, w_2) \\times \\dots \\times P(w_T \\mid w_{T-2}, w_{T-1})$\n",
    "- ‚ö†Ô∏è These are `approximations` to probabilities of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üçé Example\n",
    "Let's assume a simple corpus and bigram model:\n",
    "\n",
    "```\n",
    "corpus = [\"I love NLP\", \"I love coding\", \"NLP is fun\", \"I love Python\", \"Python is great\"]\n",
    "```\n",
    "\n",
    "We want to calculate the probability of the sentence \"I love NLP\" using the bigram model.\n",
    "\n",
    "### **Step-by-Step Calculation**\n",
    "1. **Break down the sentence into bigrams:**\n",
    "   - Bigrams: (\"I\", \"love\"), (\"love\", \"NLP\")\n",
    "\n",
    "2. **Compute the sentence probability:**\n",
    "   $P(W) = P(\"I\") \\times P(\"love\" \\mid \"I\") \\times P(\"NLP\" \\mid \"love\")$\n",
    "\n",
    "   Assume we have the following probabilities:\n",
    "   - $P(\"I\") = 0.2$\n",
    "   - $P(\"love\" \\mid \"I\") = 1.0$\n",
    "   - $P(\"NLP\" \\mid \"love\") = 0.33$\n",
    "\n",
    "   Then the sentence probability:\n",
    "   $P(W) = 0.3 \\times 1.0 \\times 0.33 = 0.099$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram_counts=Counter({'i': 3, 'love': 3, 'nlp': 2, 'is': 2, 'python': 2, 'coding': 1, 'fun': 1, 'great': 1})\n",
      "bigram_counts=Counter({('i', 'love'): 3, ('love', 'nlp'): 1, ('love', 'coding'): 1, ('nlp', 'is'): 1, ('is', 'fun'): 1, ('love', 'python'): 1, ('python', 'is'): 1, ('is', 'great'): 1})\n",
      "P('I') = 0.2\n",
      "P('I love') = 1.0\n",
      "P('love NLP') = 0.3333333333333333\n",
      "P('I love NLP') = 0.06666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qingshan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Small corpus\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"I love coding\",\n",
    "    \"NLP is fun\",\n",
    "    \"I love Python\",\n",
    "    \"Python is great\",\n",
    "]\n",
    "\n",
    "# Tokenize sentences\n",
    "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Calculate unigram and bigram counts\n",
    "unigram_counts = Counter([word for sentence in tokenized_corpus for word in sentence])\n",
    "bigram_counts = Counter([(sentence[i], sentence[i+1]) for sentence in tokenized_corpus for i in range(len(sentence)-1)])\n",
    "\n",
    "# Total number of unigrams in the corpus\n",
    "total_unigrams = sum(unigram_counts.values())\n",
    "\n",
    "# Functions to calculate probabilities\n",
    "def unigram_probability(word):\n",
    "    return unigram_counts[word] / total_unigrams\n",
    "\n",
    "def bigram_probability(w1, w2):\n",
    "    return bigram_counts[(w1, w2)] / unigram_counts[w1]\n",
    "\n",
    "# Function to calculate sentence probability using bigrams\n",
    "def sentence_probability(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    prob = unigram_probability(tokens[0])  # Start with unigram probability of the first word\n",
    "    for i in range(len(tokens) - 1):\n",
    "        prob *= bigram_probability(tokens[i], tokens[i+1])\n",
    "    return prob\n",
    "\n",
    "print(f\"{unigram_counts=}\")\n",
    "print(f\"{bigram_counts=}\")\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"I love NLP\"\n",
    "prob = sentence_probability(sentence)\n",
    "print(f\"P('I') = {unigram_probability('i')}\")\n",
    "print(f\"P('I love') = {bigram_probability('i', 'love')}\")\n",
    "print(f\"P('love NLP') = {bigram_probability('love', 'nlp')}\")\n",
    "print(f\"P('{sentence}') = {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log probability\n",
    "- Log probability, widely used in NLP, is the logarithm of a probability value\n",
    "  - If P is a probability, then log probability = ln(P)\n",
    "    - e.g. For sentence \"The quick brown fox\" in a large corpus: \n",
    "      - Probability p: 0.0000001 \n",
    "      - Log Probability ln(p): -16.11809565095832\n",
    "      - In Python, the function `log` is used to calculate the natural log function `ln`\n",
    "  - $\\displaystyle ‚àè_{i=1}^n P_i = ‚àë_{i=1}^n\\ln P_i$ allows us to convert products of probabilities to sums of log probabilities.\n",
    "- **Advantages:**\n",
    "  - Numerical stability (avoid underflow) \n",
    "  - Computational efficiency (additions instead of multiplications)\n",
    "  - Easier interpretation of very small probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluating Language Models**\n",
    "- **Extrinsic Evaluation:**\n",
    "  - Measures model performance `within an application`.\n",
    "  - Determines if improvements help the specific task.\n",
    "  - e.g: Comparing two language models in speech recognition.\n",
    "- **Intrinsic Evaluation:**\n",
    "  - Measures model quality `independently of an application`.\n",
    "  - Quick evaluation metric for potential improvements.\n",
    "\n",
    "\n",
    "### **Importance of Evaluation Metrics**\n",
    "- Running big NLP systems end-to-end can be costly.\n",
    "- **Intrinsic Evaluation:**\n",
    "  - Faster evaluation without running the entire application.\n",
    "  - Example: `Perplexity` as a standard intrinsic metric.\n",
    "- Used for n-gram models and sophisticated neural LMs.\n",
    "\n",
    "\n",
    "### **Dataset Partitioning  for Model Evaluation**\n",
    "- **Three Data Sets Needed:**\n",
    "  - **Training Set:** For learning model parameters.\n",
    "  - **Development Set (Dev Set):** For tuning and evaluating model changes.\n",
    "  - **Test Set:** For final unbiased evaluation of model performance.\n",
    "- **Importance of Each Set:**\n",
    "  - **Training Set:** Provides counts for probabilities in n-gram models.\n",
    "  - **Test Set:** Reflects the target language for accurate evaluation.\n",
    "  - **Dev Set:** Helps avoid overfitting to the test set.\n",
    "\n",
    "\n",
    "### **Choosing Training and Test Sets**\n",
    "- Test set should match the intended application of the model.\n",
    "- Example: For chemistry lecture speech recognition, use chemistry lectures as the test set.\n",
    "- **General Purpose Models:** Test set should be diverse and carefully selected.\n",
    "\n",
    "\n",
    "### **Comparing Model Performance**\n",
    "- **Comparing N-gram Models:**\n",
    "  - Train both models on the training set.\n",
    "  - Evaluate on the test set.\n",
    "- **Higher Probability:** Indicates a better fit to the test set.\n",
    "- Avoid training on the test set to prevent bias.\n",
    "\n",
    "\n",
    "### **Avoiding Bias in Evaluation**\n",
    "- **Training on the Test Set:**\n",
    "  - Leads to artificially high probabilities.\n",
    "  - Distorts evaluation metrics like perplexity.\n",
    "- **Best Practice:**\n",
    "  - Use the dev set for testing during model development.\n",
    "  - Use the test set sparingly, only when the model is finalized.\n",
    "\n",
    "\n",
    "### **Final Evaluation**\n",
    "- **Dev Set:** Used for tuning the model.\n",
    "- **Test Set:** Used for final evaluation.\n",
    "- Ensure test set size is sufficient for statistical significance.\n",
    "- Dev set should be similar to the test set for accurate evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Perplexity: A Measure of Language Model Quality**\n",
    "- **Definition:**\n",
    "  - Perplexity is a measurement of `how well a probability model predicts a sample`.\n",
    "  - It is used to evaluate the performance of language models in NLP.\n",
    "- **Key Idea:**\n",
    "  - `Lower perplexity indicates a better model`, as it suggests the model is less \"perplexed\" by the data.\n",
    "\n",
    "\n",
    "### **Mathematical Definition of Perplexity**\n",
    "- **Formula for Perplexity:**\n",
    "  - `Perplexity (PP)` of a model over a sequence $W$ of words $w_1, w_2, \\dots, w_N$:\n",
    "  - $\\displaystyle \\text{PP}(W) = P(W)^{-\\frac{1}{N}} = P(w_1, w_2, \\dots, w_N)^{-\\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1, w_2, \\dots, w_N)}} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i | w_{1:i-1})}$\n",
    "    - $=\\displaystyle\\sqrt[N]{‚àè_{i=1}^{N}\\frac{1}{P(w_i)}}$ (for unigram model)\n",
    "    - $=\\displaystyle\\sqrt[N]{‚àè_{i=1}^{N}\\frac{1}{P(w_i|w_{i-1})}}$ (for bigram model)\n",
    "    - $=\\displaystyle\\sqrt[N]{‚àè_{i=1}^{N}\\frac{1}{P(w_i|(w_{i-1},w_{i-2}))}}$ (for trigram model)\n",
    "- **Explanation:**\n",
    "  - $P(W)$: Probability of the entire sequence.\n",
    "  - $N$: Number of words in the sequence.\n",
    "  - The lower the perplexity, the higher the probability the model assigns to the sequence.\n",
    "\n",
    "\n",
    "### üçé**Perplexity Example**\n",
    "- **Given Sequence:** \"I love NLP\"\n",
    "- **Assume:** Bigram Model with probabilities:\n",
    "  - $P(I) = 0.1$\n",
    "  - $P(love | I) = 0.3$\n",
    "  - $P(NLP | love) = 0.5$\n",
    "- **Calculate:**\n",
    "  - $P(W) = 0.1 \\times 0.3 \\times 0.5 = 0.015$\n",
    "  - $N = 3$\n",
    "  - $\\text{Perplexity} = (0.015)^{-\\frac{1}{3}} \\approx 6.3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perplexity in Language Model Evaluation**\n",
    "- **Intrinsic Metric:**\n",
    "  - Perplexity is an intrinsic evaluation metric used to assess language model quality.\n",
    "- **Comparison:**\n",
    "  - Used to compare different language models on the same dataset.\n",
    "- **Interpretation:**\n",
    "  - A lower perplexity score indicates a better language model.\n",
    "\n",
    "### **Perplexity and Language Models**\n",
    "- **Perplexity** is influenced by both the text and the language model. \n",
    "  - Different n-gram models will have varying perplexities for the same text.\n",
    "- **Purpose**: Perplexity allows us to compare the effectiveness of different n-gram models.\n",
    "- üçé **Example**: \n",
    "  - Trained unigram, bigram, and trigram models on 38 million words from the Wall Street Journal (WSJ) with a 19,979 word vocabulary.\n",
    "  - **Test Set**: Perplexity calculated on a 1.5 million word WSJ test set.\n",
    "  - **Perplexity Results**:\n",
    "    - Unigram: 962\n",
    "    - Bigram: 170\n",
    "    - Trigram: 109\n",
    "  - **Interpretation**: Trigram models, which have more contextual information, are better predictors (lower perplexity) than unigrams and bigrams.\n",
    "\n",
    "### **Important Considerations in Perplexity Evaluation**\n",
    "- **Model Construction**: The n-gram model must be built without prior knowledge of the test set or its vocabulary to avoid artificially low perplexities.\n",
    "- **Comparability**: Only comparable if models use identical vocabularies.\n",
    "- **Intrinsic vs. Extrinsic Evaluation**:\n",
    "  - **Intrinsic**: Perplexity improvement doesn't always translate to better performance in tasks like speech recognition.\n",
    "  - **Extrinsic**: End-to-end evaluation on real tasks is recommended to confirm improvements.\n",
    "\n",
    "\n",
    "### **Perplexity as Weighted Average Branching Factor**\n",
    "- **Branching Factor**: The number of possible next words that can follow any given word in a language.\n",
    "- **Perplexity Interpretation**: Perplexity can be seen as the weighted average of this branching factor.\n",
    "  - For a language where all digits (0-9) are equally probable, the perplexity would equal 10.\n",
    "  - $\\displaystyle PP(W)=\\sqrt[N]{‚àè_{i=1}^{N}\\frac{1}{P(w_i)}}=\\sqrt[10]{‚àè_{i=1}^{10}\\frac{1}{1/10}} = \\sqrt[10]{‚àè_{i=1}^{10}10} = 10$\n",
    "- **Influence of Probability Distribution on Perplexity**\n",
    "  - **Unequal Distribution**: If one digit (e.g., 0) is much more frequent, the perplexity decreases\n",
    "    - as the next digit becomes more predictable.\n",
    "  - üçé e.g.: With a test set dominated by the digit 0, the weighted branching factor is lower than 10\n",
    "    - reflecting a decrease in perplexity due to predictability.\n",
    "\n",
    "\n",
    "### **Limitations of Perplexity**\n",
    "- **Dependence on Dataset:**\n",
    "  - Perplexity is sensitive to the domain and type of text in the dataset.\n",
    "- **Doesn‚Äôt Measure Understanding:**\n",
    "  - A model might have low perplexity but still lack true understanding of the language.\n",
    "- **Bias Towards Shorter Sequences:**\n",
    "  - Models may achieve low perplexity by favoring shorter sequences, which may not always be desirable.\n",
    "\n",
    "\n",
    "### **Practical Application of Perplexity**\n",
    "- **Use in Research:**\n",
    "  - Perplexity is widely used in NLP research to gauge model improvements.\n",
    "- **Example Application:**\n",
    "  - Evaluating different versions of an n-gram model on a test corpus.\n",
    "- **Tool for Development:**\n",
    "  - Helps in tuning hyperparameters and refining models during development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sampling Sentences from a Language Model**\n",
    "- **Definition**: \n",
    "  - Sampling from a language model involves generating sentences by choosing words based on their likelihood according to the model's distribution.\n",
    "  - More likely sentences (according to the model) are sampled more frequently, while less likely ones are sampled less frequently.\n",
    "- **Importance**: \n",
    "  - Sampling helps to visualize the kind of knowledge a language model holds by generating sentences it considers probable.\n",
    "\n",
    "\n",
    "### Historical Context and Unigram Sampling\n",
    "- **Historical Context**:\n",
    "  - The idea of sampling from language models was first suggested by Shannon (1948) and Miller and Selfridge (1950).\n",
    "- **Unigram Sampling**:\n",
    "  - Imagine the words of a language spread out across a probability space from 0 to 1.\n",
    "  - Each word occupies an interval proportional to its frequency.\n",
    "  - **Process**:\n",
    "    1. Choose a random number between 0 and 1.\n",
    "    2. Identify the word whose interval includes this number.\n",
    "    3. Continue generating words until the sentence-final token `</s>` is selected.\n",
    "- **Visualization**:\n",
    "  - A cumulative probability line where frequent words (e.g., \"the\", \"of\") occupy larger intervals, \n",
    "    - and rare words (e.g., \"polyphonic\") occupy smaller intervals.\n",
    "\n",
    "\n",
    "### Bigram Sampling\n",
    "- **Bigram Sampling Process**:\n",
    "  - Start by generating a bigram that begins with `<s>` based on its probability.\n",
    "  - The second word of the bigram becomes the first word of the next bigram.\n",
    "  - Repeat the process, generating each subsequent word based on the previous one.\n",
    "- **Key Concept**:\n",
    "  - Sampling bigrams provides a more contextual generation compared to unigrams, \n",
    "    - as each word is dependent on the previous one.\n",
    "- **Application**:\n",
    "  - By sampling bigrams, one can visualize how the model predicts word sequences and generates more natural sentences compared to unigrams.\n",
    "\n",
    "\n",
    "### **Sampling Strategies**\n",
    "- **Greedy Sampling**:\n",
    "  - Always select the word with the highest probability.\n",
    "  - **Pros**: Produces highly probable sentences.\n",
    "  - **Cons**: Can result in repetitive or deterministic text.\n",
    "- **Random Sampling**:\n",
    "  - Select words based on their probabilities, allowing less probable words to be chosen.\n",
    "  - **Pros**: Produces more diverse sentences.\n",
    "  - **Cons**: May result in less coherent text.\n",
    "\n",
    "\n",
    "### üçé **Practical Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Generated Text:\n",
      "the take outrageous , mind them question against of of and 't question is opposing or in the to of\n",
      "\n",
      "Bigram Generated Text:\n",
      "to suffer the question : whether 't is the slings and arrows of outrageous fortune , or not to be\n",
      "\n",
      "Trigram Generated Text:\n",
      "to be , that is the question : whether 't is nobler in the mind to suffer the slings and\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/qingshan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Ensure that the necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text to be used for n-gram models\n",
    "#  üõ†Ô∏è try a large corpus such as Shakespeare\n",
    "text = \"\"\"To be, or not to be, that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles\n",
    "And by opposing end them.\"\"\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Generate Unigrams, Bigrams, and Trigrams\n",
    "unigrams = tokens\n",
    "unigram_freq = Counter(unigrams)\n",
    "\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "bigram_freq = Counter(bigrams)\n",
    "\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "trigram_freq = Counter(trigrams)\n",
    "\n",
    "# Function to generate text using unigrams\n",
    "def generate_text_unigram(unigram_freq, num_words=20):\n",
    "    words = list(unigram_freq.keys())\n",
    "    generated_text = random.choices(words, weights=unigram_freq.values(), k=num_words)\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Function to generate text using bigrams\n",
    "def generate_text_bigram(starting_word, bigram_freq, num_words=20):\n",
    "    current_word = starting_word\n",
    "    generated_text = [current_word]\n",
    "    for _ in range(num_words - 1):\n",
    "        possible_bigrams = [bigram for bigram in bigram_freq if bigram[0] == current_word]\n",
    "        if possible_bigrams:\n",
    "            next_word = random.choices(\n",
    "                [bigram[1] for bigram in possible_bigrams],\n",
    "                weights=[bigram_freq[bigram] for bigram in possible_bigrams]\n",
    "            )[0]\n",
    "            generated_text.append(next_word)\n",
    "            current_word = next_word\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Function to generate text using trigrams\n",
    "def generate_text_trigram(starting_words, trigram_freq, num_words=20):\n",
    "    current_words = starting_words\n",
    "    generated_text = list(current_words)\n",
    "    for _ in range(num_words - 2):\n",
    "        possible_trigrams = [trigram for trigram in trigram_freq if trigram[:2] == current_words]\n",
    "        if possible_trigrams:\n",
    "            next_word = random.choices(\n",
    "                [trigram[2] for trigram in possible_trigrams],\n",
    "                weights=[trigram_freq[trigram] for trigram in possible_trigrams]\n",
    "            )[0]\n",
    "            generated_text.append(next_word)\n",
    "            current_words = (current_words[1], next_word)\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "# Generate text using unigrams\n",
    "generated_unigram_text = generate_text_unigram(unigram_freq)\n",
    "print(\"Unigram Generated Text:\")\n",
    "print(generated_unigram_text)\n",
    "\n",
    "# Generate text using bigrams\n",
    "starting_word = 'to'\n",
    "generated_bigram_text = generate_text_bigram(starting_word, bigram_freq)\n",
    "print(\"\\nBigram Generated Text:\")\n",
    "print(generated_bigram_text)\n",
    "\n",
    "# Generate text using trigrams\n",
    "starting_words = ('to', 'be')\n",
    "generated_trigram_text = generate_text_trigram(starting_words, trigram_freq)\n",
    "print(\"\\nTrigram Generated Text:\")\n",
    "print(generated_trigram_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generalization and Zeros**\n",
    "- **N-gram Model Dependency**:\n",
    "  - N-gram models rely heavily on the training corpus.\n",
    "  - Higher N-values in N-grams lead to better modeling of the training corpus.\n",
    "- **Implications**:\n",
    "  - Probabilities encode specific facts from the training corpus.\n",
    "  - As N increases, the model captures more contextual relationships.\n",
    "\n",
    "\n",
    "### **Visualization of N-gram Models**\n",
    "- **Sampling Method**:\n",
    "  - Visualizes the increasing power of higher-order N-grams.\n",
    "  - üçé Example: Sentences generated from unigram, bigram, trigram, and 4-gram models trained on Shakespeare.\n",
    "- **Key Observations**:\n",
    "  - **Unigram**: No coherent word relation or punctuation.\n",
    "  - **Bigram**: Some local word-to-word coherence.\n",
    "  - **Trigram/4-gram**: Sentences resemble Shakespeare's style but may overfit the training corpus.\n",
    "\n",
    "\n",
    "### **N-gram Sparsity Problem**\n",
    "- **Corpus Size and Sparsity**:\n",
    "  - Shakespeare's corpus: 884,647 words; 29,066 vocabulary.\n",
    "  - **Sparsity Issue**: V¬≤ = 844 million possible bigrams, V‚Å¥ = 7√ó10¬π‚Å∑ possible 4-grams.\n",
    "  - **Overfitting Example**: \"It cannot be but so\" from Shakespeare's \"King John.\"\n",
    "\n",
    "\n",
    "### **Generalization across Genres**\n",
    "- **Training on Different Corpora**:\n",
    "  - Compare N-gram models trained on Shakespeare vs. Wall Street Journal (WSJ).\n",
    "  - Both generate \"English-like sentences\" but with no overlap in content.\n",
    "- **Genre Dependence**:\n",
    "  - Training and test sets must match in genre to avoid poor model performance.\n",
    "\n",
    "\n",
    "### **Matching Genres and Dialects**\n",
    "- **Importance of Genre Matching**:\n",
    "  - Use a training corpus relevant to the task's genre (e.g., legal documents for legal models).\n",
    "- **Dialect and Variety**:\n",
    "  - Consider appropriate dialects, especially for social media or spoken transcripts.\n",
    "  - Example: African American English (AAE) and Nigerian Pidgin have unique features and n-gram patterns.\n",
    "\n",
    "\n",
    "### **Dealing with Zeros**\n",
    "- **Zero Probability N-grams**:\n",
    "  - Some word sequences may be missing from the training corpus, leading to \"zero probability n-grams.\"\n",
    "  - Example: \"denied the offer\" may be unseen, resulting in zero probability estimation.\n",
    "- **Consequences**:\n",
    "  - Underestimation of probabilities and difficulty in computing perplexity.\n",
    "\n",
    "\n",
    "### **Solutions to Zeros**\n",
    "- **Smoothing Algorithms**:\n",
    "  - Redistribute probability mass to unseen events, mitigating zero probabilities.\n",
    "- **Unknown Words**:\n",
    "  - For unseen words, use a pseudo-word `<UNK>` in open vocabulary systems.\n",
    "  - Convert out-of-vocabulary words to `<UNK>` during training to estimate their probabilities.\n",
    "\n",
    "\n",
    "### **Unknown Words and Perplexity**\n",
    "\n",
    "- **Dealing with Unknown Words**:\n",
    "  - Closed vocabulary systems avoid unknown words using subword tokenization.\n",
    "  - Open vocabulary systems use `<UNK>` to handle unknown words.\n",
    "- **Effect on Perplexity**:\n",
    "  - Perplexity scores depend on the choice of vocabulary and `<UNK>` probabilities.\n",
    "  - Comparisons of perplexity across models require the same vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Smoothing in Language Models**\n",
    "- **Definition:** `Smoothing`, or `discounting` techniques are used to handle the issue of zero probabilities in language models by assigning non-zero probabilities to unseen events\n",
    "  - by transferring a bit of probability mass from some more frequent events to the unseen events \n",
    "- **Importance:** Essential for improving the performance of n-gram models by addressing data sparsity.\n",
    "- **Methods:** \n",
    "  - Laplace (Add-One) Smoothing\n",
    "  - Add-k Smoothing\n",
    "  - Stupid Backoff\n",
    "  - Kneser-Ney Smoothing\n",
    "\n",
    "\n",
    "### **Laplace (Add-One) Smoothing**\n",
    "- Add 1 to all the n-gram counts before normalizing them into probabilities\n",
    "  - effective for small datasets.\n",
    "- **Formula**\n",
    "  - Smoothing unigram: $\\displaystyle P(w_n)  = \\dfrac{C(w_n) + 1}{Œ£_w(C(w) + 1)} = \\dfrac{C(w_n) + 1}{N + V}$\n",
    "  - Smoothing bigram: $\\displaystyle P(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + 1}{Œ£_w(C(w_{n-1}w) + 1)} = \\frac{C(w_{n-1}w_n) + 1}{C(w_{n-1}) + V}$\n",
    "    - $C(w_{n-1}w_n)$: Count of the bigram $w_{n-1}w_n$.\n",
    "    - $C(w_{n-1})$: Count of the unigram $w_{n-1}$.\n",
    "    - $V$: Size of the vocabulary.\n",
    "- **Example:**\n",
    "  - Suppose we have the bigram \"I am\" with a count of 2, \"I\" appears 3 times, and the vocabulary size $V = 5$.\n",
    "  - Calculation: $\\displaystyle P(\\text{\"am\"}|\\text{\"I\"}) = \\frac{2 + 1}{3 + 5} = \\frac{3}{8} = 0.375$.\n",
    "- **Python Code Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplace Smoothed Probability: 0.375\n"
     ]
    }
   ],
   "source": [
    "def laplace_smoothing(bigram_count, unigram_count, vocab_size):\n",
    "    return (bigram_count + 1) / (unigram_count + vocab_size)\n",
    "\n",
    "result = laplace_smoothing(2, 3, 5)\n",
    "print(f\"Laplace Smoothed Probability: {result:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Adjusted, or discounted count**  can be turned into a probability like an MLE count by normalizing by $N$:\n",
    "  - Unigram:\n",
    "    - $\\displaystyle C_{\\text{adj}}(w_n) = (C(w_n)+1)\\frac{N}{N+V}$\n",
    "    - It easier to compare directly with the MLE counts \n",
    "    - Relative discount: $\\displaystyle d_{C(w_n)}=\\dfrac{C_{\\text{adj}}(w_n)}{C(w_n)}$\n",
    "  - Bigram:\n",
    "    - $\\displaystyle C_{adj}(w_{n-1}w_n) = \\left(C(w_{n-1}w_n) + 1\\right)\\frac{C(w_{n-1})}{C(w_{n-1}) + V}$\n",
    "- Add-one smoothing makes a very big change to the counts due to huge word sparsity in n-grams (n>1) in big corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Add-k Smoothing**\n",
    "- Generalizes Laplace, allows tuning of the smoothing parameter.\n",
    "  - Add a fractional count $k (0<k<1)$ instead of 1\n",
    "- **Formula:**\n",
    "  - $\\displaystyle P(w_n | w_{n-1}) = \\frac{C(w_{n-1}w_n) + k}{C(w_{n-1}) + kV}$\n",
    "    - $k$: Smoothing parameter (a non-negative value).\n",
    "- **Example:**\n",
    "  - Using the same example with $k = 0.5$, the calculation becomes $\\displaystyle P(\\text{\"am\"}|\\text{\"I\"}) = \\frac{2 + 0.5}{3 + 0.5 \\times 5} = \\frac{2.5}{5.5} \\approx 0.455$.\n",
    "- **Python Code Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add-k Smoothed Probability: 0.455\n"
     ]
    }
   ],
   "source": [
    "def add_k_smoothing(bigram_count, unigram_count, vocab_size, k=0.5):\n",
    "    return (bigram_count + k) / (unigram_count + k * vocab_size)\n",
    "\n",
    "result = add_k_smoothing(2, 3, 5, 0.5)\n",
    "print(f\"Add-k Smoothed Probability: {result:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add-k smoothing, optimized on a devset, is helpful for some tasks like text classification.\n",
    "- However, it performs poorly in language modeling, \n",
    "  - leading to poor variance and inappropriate discounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Backoff and Interpolation in Language Models**\n",
    "- **Backoff** uses a lower-order n-gram if a higher-order n-gram has insufficient evidence.\n",
    "  - **Process:** $\\displaystyle P(w_n|w_{n-2}w_{n-1}) ‚§† P(w_n|w_{n-1}) ‚§† P(w_n)$\n",
    "    - Use trigram if available. \n",
    "    - Back off to bigram if trigram is absent.\n",
    "    - Finally, use unigram if bigram is also absent.\n",
    "  - **Key Point:** Backoff only occurs if the higher-order n-gram has zero counts.\n",
    "- **Interpolation** combines probability estimates from multiple n-gram orders.\n",
    "  - **Process:**\n",
    "    - Linearly combine unigram, bigram, and trigram probabilities.\n",
    "    - Use weighted average, with weights summing to 1.\n",
    "  - **Key Point:** Interpolation always considers all n-grams, not just those with zero counts.\n",
    "\n",
    "\n",
    "### **Simple Linear Interpolation**\n",
    "- Combines different order n-grams by linearly interpolating them\n",
    "- **Formula:**\n",
    "- $\\hat{P}(w_n |w_{n-2}w_{n-1}) = \\lambda_1 P(w_n) + \\lambda_2 P(w_n|w_{n-1}) + \\lambda_3 P(w_n|w_{n-2}w_{n-1})$\n",
    "  - Each $\\lambda$ represents a weight for unigram, bigram, and trigram probabilities.\n",
    "  - The weights $\\lambda$ are chosen to maximize the likelihood of a `held-out corpus` (an additional training corpus held out from the training data).\n",
    "\n",
    "\n",
    "### **Conditional Interpolation**\n",
    "- Conditions on the context by assigning heavier weights to more accurate n-grams\n",
    "- **Formula:**\n",
    "- $\\hat{P}(w_n |w_{n-2}w_{n-1}) = \\lambda_1(w_{n-2:n-1})P(w_n) + \\lambda_2(w_{n-2:n-1})P(w_n|w_{n-1}) + \\lambda_3(w_{n-2:n-1})P(w_n|w_{n-2}w_{n-1})$\n",
    "  - Weights $\\lambda$ vary depending on the context, making them more accurate.\n",
    "  - Helps in giving appropriate weight based on the confidence in the counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stupid Backoff\n",
    "- A simple and efficient non-discounted backoff algorithm, useful for large datasets, but lacks probabilistic interpretation.\n",
    "- **Explanation:**\n",
    "  - A heuristic used in place of more complex smoothing methods when scaling to large datasets.\n",
    "  - $\\displaystyle P_{\\text{backoff}}(w_n | w_{n-1}) = \n",
    "    \\begin{cases} \n",
    "    \\dfrac{C(w_{n-1}w_n)}{C(w_{n-1})} & \\text{if } C(w_{n-1}w_n) > 0 \\\\\n",
    "    \\alpha P(w_n) & \\text{otherwise}\n",
    "    \\end{cases}$\n",
    "  - $\\alpha$ is a discounting factor (typically $\\alpha = 0.4$).\n",
    "- **Python Code Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stupid Backoff Probability: 0.080\n"
     ]
    }
   ],
   "source": [
    "def stupid_backoff(bigram_count, unigram_count, unigram_prob, alpha=0.4):\n",
    "    if bigram_count > 0:\n",
    "        return bigram_count / unigram_count\n",
    "    else:\n",
    "        return alpha * unigram_prob\n",
    "\n",
    "# Example usage\n",
    "result = stupid_backoff(0, 3, 0.2, 0.4)\n",
    "print(f\"Stupid Backoff Probability: {result:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kneser-Ney Smoothing**\n",
    "- State-of-the-art, particularly for higher-order n-grams, considers the distribution of words across different contexts.\n",
    "- **Explanation:**\n",
    "  - One of the most sophisticated smoothing methods, particularly effective for bigram and trigram models.\n",
    "- **Formula:**\n",
    "  $\\displaystyle P_{\\text{KN}}(w_n | w_{n-1}) = \\max\\left(\\dfrac{C(w_{n-1}w_n) - d}{C(w_{n-1})}, 0\\right) + \\lambda(w_{n-1})P_{\\text{continuation}}(w_n)$\n",
    "  - $d$ is a discounting factor.\n",
    "  - $\\lambda(w_{n-1})$ is a normalizing constant.\n",
    "  - $P_{\\text{continuation}}(w_n)$ is the continuation probability, i.e., the likelihood of $w_n$ occurring in any context.\n",
    "- **Python Code Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kneser-Ney Smoothed Probability: 0.467\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def kneser_ney_smoothing(bigram_count, unigram_count, continuation_prob, discount=0.75):\n",
    "    adjusted_count = max(bigram_count - discount, 0)\n",
    "    return adjusted_count / unigram_count + (discount / unigram_count) * continuation_prob\n",
    "\n",
    "# Example continuation probability\n",
    "continuation_prob = defaultdict(float, {'am': 0.2, 'I': 0.1})\n",
    "\n",
    "# Example usage\n",
    "result = kneser_ney_smoothing(2, 3, continuation_prob['am'])\n",
    "print(f\"Kneser-Ney Smoothed Probability: {result:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Challenges and Solutions**\n",
    "   - **Data Sparsity:** Limited data can lead to zero probabilities for unseen word sequences.\n",
    "     - **Solution:** Techniques like smoothing, backoff models, or neural networks help in assigning non-zero probabilities to rare or unseen sequences.\n",
    "   - **Long-Range Dependencies:** Traditional models like N-grams struggle with context far back in the sequence.\n",
    "     - **Solution:** Advanced models like RNNs, LSTMs, and Transformers capture long-range dependencies more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Relation Between Perplexity and Entropy**\n",
    "- **Perplexity:** A normalized measure of how well an n-gram model predicts a test set.\n",
    "- **Entropy:** An information-theoretic concept that measures `the uncertainty or information content in a random variable`.\n",
    "  - Higher entropy = More unpredictability.\n",
    "  - Lower entropy = Less unpredictability.\n",
    "- **Relationship:** Perplexity arises from the concept of `cross-entropy`, which is closely tied to entropy.\n",
    "\n",
    "### Entropy Definition\n",
    "- **Entropy (H):** Measures the average amount of information produced by a stochastic process.\n",
    "- **Formula:**\n",
    "  - $H(X) = -\\sum_{x \\in \\chi} p(x) \\log_2 p(x)$\n",
    "    - $H(X)$: Entropy of random variable $X$.\n",
    "    - $p(x)$: Probability of each outcome $x$.\n",
    "    - Logarithm base 2 indicates information is measured in bits.  \n",
    "    - $\\log_2 p(x)$: Amount of information received when event $x$ occurs.\n",
    "    - $-p(x) \\log_2 p(x)$: Weighted contribution of each event to total entropy.\n",
    "  - Represents the `lower bound on the number of bits` needed to encode a piece of information in the optimal coding scheme.\n",
    "\n",
    "### üçé **Entropy in a Fair Coin Toss**\n",
    "- **Scenario:**\n",
    "  - A fair coin has two outcomes: Heads (H) and Tails (T).\n",
    "- **Calculation:**\n",
    "  - $H(X) = -\\left(\\frac{1}{2} \\log_2 \\frac{1}{2} + \\frac{1}{2} \\log_2 \\frac{1}{2}\\right) = 1 \\text{ bit}$\n",
    "- **Interpretation:**\n",
    "  - Each toss has an entropy of 1 bit, indicating maximum uncertainty.\n",
    "\n",
    "\n",
    "### üçé **Entropy in a Biased Coin**\n",
    "- **Scenario:**\n",
    "  - A biased coin with $P(H) = 0.8$ and $P(T) = 0.2$.\n",
    "- **Calculation:**\n",
    "  - $H(X) = -\\left(0.8 \\log_2 0.8 + 0.2 \\log_2 0.2\\right) \\approx 0.72 \\text{ bits}$\n",
    "- **Interpretation:**\n",
    "  - Lower entropy reflects less uncertainty compared to a fair coin.\n",
    "\n",
    "### Entropy of Sequences\n",
    "- **Sequence Entropy:** The entropy of a sequence of words $W = \\{w_1, w_2, \\dots, w_n\\}$ in language $L$:\n",
    "  - $\\displaystyle H(w_1, w_2, \\dots, w_n) = -\\sum_{w_{1:n} \\in L} p(w_{1:n}) \\log_2 p(w_{1:n})$\n",
    "- Treats $L$ as a stochastic process that produces this sequence of words $W$, then **L's Entropy Rate:** (per-word entropy)\n",
    "  - $\\displaystyle H(L) = \\lim_{n \\to \\infty} \\frac{1}{n} H(w_1, w_2, \\dots, w_n)$\n",
    "\n",
    "### Cross-Entropy\n",
    "- **Cross-Entropy:** Measures the difference between the true distribution $p$ and a model $m$.\n",
    "- **Formula:**\n",
    "  - $\\displaystyle H(p, m) = \\lim_{n \\to \\infty} -\\frac{1}{n} \\sum_{W \\in L} p(W) \\log_2 m(W)$\n",
    "- **Upper Bound:** Cross-entropy $H(p, m)$ is always greater than or equal to entropy $H(p)$.\n",
    "\n",
    "### Perplexity and Cross-Entropy\n",
    "- **Perplexity:** Related to cross-entropy, serving as an approximation.\n",
    "- **Approximation Formula:**\n",
    "  - $\\displaystyle H(W) = -\\frac{1}{N} \\log_2 P(w_1, w_2, \\dots, w_N)$\n",
    "- **Perplexity Formula:**\n",
    "  - $\\displaystyle \\text{Perplexity} = 2^{H(W)} =\\sqrt[N]{\\frac{1}{P(w_1, w_2, \\dots, w_N)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- Entropy quantifies the uncertainty in predicting a sequence.\n",
    "- Cross-entropy evaluates how well a model approximates the true distribution.\n",
    "- Perplexity is a practical measure derived from cross-entropy to evaluate language models.\n",
    "- **Large language models**, built on neural networks rather than n-grams, **solve two key problems of n-grams**:\n",
    "  - The exponential growth of parameters with increasing n-gram order\n",
    "  - The inability to generalize from training to test examples unless identical words are used. \n",
    "- Neural models project words into a `continuous space` where `words with similar contexts have similar representations`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
