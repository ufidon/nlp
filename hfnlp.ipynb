{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/hfnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/hfnlp.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "\n",
    "- üìù A Cram of the [HuggingFace NLP course](https://huggingface.co/learn/nlp-course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Huggingface core libraries\n",
    "!pip install tokenizers transformers datasets accelerate\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Behind the pipeline\n",
    "# Tokenizer ‚Üí Model ‚Üí Post Processing\n",
    "\n",
    "# a) Tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Tokenization\n",
    "raw_inputs = [\n",
    "    \"What did the grape say when it got stepped on?\",\n",
    "    \"Nothing, it just let out a little wine!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# b) Model\n",
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# Inference\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)\n",
    "\n",
    "# b.1) Another architecture\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint2 = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(checkpoint2)\n",
    "outputs2 = model(**inputs)\n",
    "print(outputs2.logits.shape)\n",
    "\n",
    "# c) Postprocessing\n",
    "print(outputs.logits)\n",
    "print(outputs2.logits)\n",
    "\n",
    "\n",
    "# c.1) logits ‚ßê probabilities\n",
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Models\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
