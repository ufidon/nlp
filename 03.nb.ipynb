{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/03.nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/03.nb.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Naive Bayes, Text Classification, and Sentiment\n",
    "\n",
    "ðŸ“ SALP chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” **What is Text Classification?**\n",
    "  - Text classification involves assigning a text to a predefined category from a finite set of classes.\n",
    "  - Examples include:\n",
    "    - **Language identification** (detecting the language of the text).\n",
    "    - **Authorship attribution** (determining the author of a document).\n",
    "    - **Sentiment analysis** (positive/negative)\n",
    "      - Used to determine a writer's emotional stance toward a product, topic, or service.\n",
    "      - e.g., movie review text being classified as positive or negative\n",
    "    - **Spam detection** (spam/not spam)\n",
    "    - **Topic/subject categorization** (sports, tech, etc.)\n",
    "  - **Linear classifiers** are commonly used for text classification tasks.\n",
    "\n",
    "\n",
    "### **Linear Classifiers Overview**\n",
    "- A **linear classifier** makes a classification decision based on a `linear combination of input features`.\n",
    "- The classifier creates a `decision boundary` in the `feature space` (text features like word frequencies).\n",
    "- Examples of linear classifiers include:\n",
    "  - **Naive Bayes**\n",
    "  - **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” **What is Naive Bayes?**\n",
    "- **Naive Bayes** is a family of probabilistic classifiers based on **Bayesâ€™ Theorem**.\n",
    "- It assumes **conditional independence** between features (words) given the class.\n",
    "- Often used in **text classification** tasks like **spam detection** or **sentiment analysis**.\n",
    "- **Bayes' Theorem**: $\\displaystyle P(C|X) = \\frac{P(X|C) P(C)}{P(X)}$\n",
    "  - **P(C|X)**: Probability of class $C$ given input data $X$.\n",
    "  - **P(X|C)**: Likelihood of data $X$ given class $C$.\n",
    "  - **P(C)**: Prior probability of class $C$.\n",
    "  - **P(X)**: Prior probability of data $X$.\n",
    "\n",
    "\n",
    "\n",
    "### **What is the Multinomial Naive Bayes?**\n",
    "- The **Multinomial Naive Bayes** model is particularly well-suited for text classification.\n",
    "- It is used when features represent **word counts** or **term frequencies**\n",
    "  - i.e., the number of occurrences of each word\n",
    "- Assumes that word occurrences follow a `multinomial distribution`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is a Multinomial Distribution?**\n",
    "- The **Multinomial Distribution** is an extension of the binomial distribution.\n",
    "- It models the probabilities of outcomes for multiple categories from a finite set, where each outcome has a certain probability.\n",
    "- In a multinomial experiment, there are:\n",
    "  1. **n trials** (fixed number of observations).\n",
    "  2. **k possible outcomes** (categories).\n",
    "  3. **p probabilities** associated with each outcome.\n",
    "- The sum of probabilities for all categories equals 1.\n",
    "  - ðŸŽ **Example 1**: Rolling a die multiple times where each face has an equal probability of showing up.\n",
    "\n",
    "- **Multinomial Distribution Formula**:\n",
    "  - $\\displaystyle P(X_1 = x_1, X_2 = x_2, ..., X_k = x_k) = \\frac{n!}{x_1! x_2! ... x_k!} p_1^{x_1} p_2^{x_2} ... p_k^{x_k}$\n",
    "  - Where:\n",
    "    - $n$: Total number of trials.\n",
    "    - $x_1, x_2, ..., x_k$: Number of occurrences for each category.\n",
    "    - $p_1, p_2, ..., p_k$: Probabilities of each category.\n",
    "\n",
    "- ðŸŽ **Example 2**:\n",
    "  - Suppose we roll a die 10 times, and the outcomes are as follows: \n",
    "    - Face 1: 2 times.\n",
    "    - Face 2: 3 times.\n",
    "    - Face 3: 1 time.\n",
    "    - Face 4: 1 time.\n",
    "    - Face 5: 2 times.\n",
    "    - Face 6: 1 time.\n",
    "  - Use the multinomial distribution to calculate the probability of this exact outcome:\n",
    "    - $P(2, 3, 1, 1, 2, 1) = (10!) / (2! Ã— 3! Ã— 1! Ã— 1! Ã— 2! Ã— 1!) Ã— (1/6)^2 Ã— (1/6)^3 Ã— (1/6)^1 Ã— (1/6)^1 Ã— (1/6)^2 Ã— (1/6)^1 = 0.002500571559213533$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Roll Experiment Results:\n",
      "{1: 176, 2: 176, 3: 187, 4: 146, 5: 146, 6: 169}\n",
      "\n",
      "Probability of specific outcome:\n",
      "0.00250057155921354\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multinomial\n",
    "\n",
    "# Example 1: Rolling a die multiple times\n",
    "def die_roll_experiment(num_rolls):\n",
    "    # Probabilities for each face of the die\n",
    "    p = [1/6] * 6\n",
    "    \n",
    "    # Simulate die rolls\n",
    "    rolls = np.random.choice(6, size=num_rolls, p=p) + 1\n",
    "    \n",
    "    # Count occurrences of each face\n",
    "    unique, counts = np.unique(rolls, return_counts=True)\n",
    "    \n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "# Example 2: Calculate probability of specific outcome\n",
    "def calculate_specific_outcome_probability():\n",
    "    n = 10  # Total number of rolls\n",
    "    x = [2, 3, 1, 1, 2, 1]  # Occurrences of each face\n",
    "    p = [1/6] * 6  # Probability of each face\n",
    "    \n",
    "    probability = multinomial.pmf(x, n, p)\n",
    "    return probability\n",
    "\n",
    "# Run Example 1\n",
    "num_rolls = 1000\n",
    "results = die_roll_experiment(num_rolls)\n",
    "print(\"Die Roll Experiment Results:\")\n",
    "print(results)\n",
    "\n",
    "# Run Example 2\n",
    "probability = calculate_specific_outcome_probability()\n",
    "print(\"\\nProbability of specific outcome:\")\n",
    "print(f\"{probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002500571559213533\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def multinomial_probability(trials, outcomes, probs):\n",
    "  total_outcomes = sum(outcomes)\n",
    "  if trials != total_outcomes:\n",
    "    raise ValueError(\"The sum of outcomes must equal the number of trials.\")\n",
    "\n",
    "  probability = math.factorial(trials)\n",
    "  for i,outcome in enumerate(outcomes):\n",
    "    probability /= math.factorial(outcome)\n",
    "    probability *= math.pow(probs[i], outcome)\n",
    "\n",
    "\n",
    "  return probability\n",
    "\n",
    "# Example usage:\n",
    "trials = 10\n",
    "outcomes = [2, 3, 1, 1, 2, 1]\n",
    "probs = [1/6]*6\n",
    "probability = multinomial_probability(trials, outcomes, probs)\n",
    "print(probability)  # Output: approximately 0.002500571559213533"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assumptions of Multinomial Naive Bayes**\n",
    "- **Conditional Independence**: \n",
    "  - Each word is conditionally independent of every other word, given the class.\n",
    "- **Bag-of-Words (BoWs) Assumption**: \n",
    "  - The position of words does not matter, only their frequency counts.\n",
    "- **Multinomial Distribution**: \n",
    "  - Words are drawn from a fixed vocabulary and follow a multinomial distribution.\n",
    "- ðŸŽ Create BaWs from a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 1, 'a': 1, 'sample': 1, 'document': 1, '.': 2, 'it': 1, 'contains': 1, 'some': 2, 'words': 2, ',': 1, 'and': 1, 'other': 1}\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def bag_of_words(document, stop_words=None):\n",
    "    # Tokenize the document into words\n",
    "    words = word_tokenize(document.lower())\n",
    "\n",
    "    # Remove stop words if provided\n",
    "    if stop_words is not None:\n",
    "        # stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Create a bag of words representation\n",
    "    bag = {}\n",
    "    for word in words:\n",
    "        bag[word] = bag.get(word, 0) + 1\n",
    "\n",
    "    return bag\n",
    "\n",
    "# Example usage\n",
    "document = \"This is a sample document. It contains some words, and some other words.\"\n",
    "bag = bag_of_words(document)\n",
    "print(bag)\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Naive Bayes Classifiers**\n",
    "- Naive Bayes is a **probabilistic model** for classification.\n",
    "- It selects the class $\\hat{c}$ that maximizes the **posterior probability** given a document $d$:\n",
    "  - $\\displaystyle\\hat{c} = \\arg\\max_{c \\in C} P(c|d)$\n",
    "  - This formula uses **Bayesian inference** to predict the `most likely class` for the input.\n",
    "- ðŸŽ An email $d$ being classified into two categories: \n",
    "  - $\\displaystyle\\hat{c} = \\arg\\max_{c \\in \\{S,H\\}} P(c|d)$\n",
    "  - where S = spam, H = ham, not spam\n",
    "\n",
    "\n",
    "### **Bayesâ€™ Theorem**\n",
    "- $P(c|d) = \\dfrac{P(d|c)P(c)}{P(d)}$\n",
    "  - $P(c|d)$: Probability of class $c$ given the document $d$.\n",
    "  - $P(d|c)$: Likelihood of observing the document $d$ if the class is $c$.\n",
    "  - $P(c)$: Prior probability of the class $c$.\n",
    "  - $P(d)$: Evidence or probability of the document $d$ under all classes.\n",
    "\n",
    "\n",
    "### **Simplifying Naive Bayes Formula**\n",
    "- The denominator $P(d)$ is constant for all classes and can be ignored for classification.\n",
    "  - $\\displaystyle\\hat{c} = \\arg\\max_{c \\in C} P(d|c)P(c)$\n",
    "- Naive Bayes is called a **generative model**:\n",
    "  - First, a class $c$ is sampled from the prior $P(c)$.\n",
    "  - Then the document is generated from the likelihood $P(d|c)$.\n",
    "\n",
    "\n",
    "### **Naive Bayes Assumptions**\n",
    "- With assumptions:\n",
    "  1. **Bag-of-Words**: \n",
    "     - The order of words doesnâ€™t matter.\n",
    "  2. **Conditional Independence**: \n",
    "     - Each wordâ€™s occurrence is independent given the class $c$.\n",
    "     - also called **naive Bayes assumption**.\n",
    "  \n",
    "- Then a document $d$ can be represented as a set of features $f_1, f_2, ..., f_n$:\n",
    "  - $P(f_1, f_2, ..., f_n | c) = P(f_1 | c) \\cdot P(f_2 | c) \\cdot ... \\cdot P(f_n | c)$\n",
    "- The final equation for the class chosen by a naive Bayes classifier is thus:\n",
    "  - $\\displaystyle\\hat{c} = C_{NB} = \\arg\\max_{c \\in C} P(c)âˆ_{f\\in F}P(f|c)$  \n",
    "\n",
    "\n",
    "### **Applying Naive Bayes to Text**\n",
    "- **Document as Features**:\n",
    "  - For text classification, each word in the document is treated as a feature:\n",
    "    - $c_{NB} = \\arg\\max_{c \\in C} P(c) \\prod_{i \\in \\text{positions}} P(w_i | c)$\n",
    "    - positions â† all word positions in test document\n",
    "  - The classifier computes the likelihood of the document given each class and picks the highest.\n",
    "- Use of Naive Bayes in **spam detection** where words like â€œfreeâ€ or â€œwinâ€ increase likelihood for spam.\n",
    "\n",
    "\n",
    "\n",
    "### **Practical Challenges of Naive Bayes**\n",
    "- **Computational Issues**:\n",
    "  - Calculating the product of probabilities for long documents can lead to **underflow**.\n",
    "  \n",
    "- **Solution**:\n",
    "  - Use `logarithms` to convert the product into a sum:\n",
    "    - $c_{NB} = \\arg\\max_{c \\in C} \\log P(c) + \\sum_{i \\in \\text{positions}} \\log P(w_i | c)$\n",
    "    - Naive Bayes is a **linear classifier** when expressed in log space\n",
    "- This transformation makes the computation efficient and avoids underflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Training the Naive Bayes Classifier**\n",
    "- **Key Idea**: Learn probabilities from data.\n",
    "- **Class Prior $P(c)$**:\n",
    "  - $\\displaystyle \\hat{P}(c) = \\dfrac{N_c}{N_{\\text{doc}}}$\n",
    "  - $N_c$: Number of documents in class $c$.\n",
    "  - $N_{\\text{doc}}$: Total number of documents.\n",
    "- **Word Likelihood $P(w_i|c)$**:\n",
    "  - $\\displaystyle \\hat{P}(w_i|c) = \\dfrac{\\text{count}(w_i, c)}{\\sum_{w \\in V} \\text{count}(w, c)}$\n",
    "  - $V$: Vocabulary (all words in all classes).\n",
    "\n",
    "\n",
    "\n",
    "### **The Problem with Zero Probabilities**\n",
    "- **Zero Likelihood Problem**:\n",
    "  - When a word does not appear in the training data for a class, its probability becomes zero.\n",
    "    - $\\displaystyle \\hat{P}(\\text{â€œword\\_not\\_exist\\_in\\_câ€}|c) = 0$\n",
    "  - Multiplied probabilities lead to zero for the entire class likelihood.\n",
    "- **Impact**: \n",
    "  - If any word's likelihood is zero, the whole class probability becomes zero.\n",
    "- **Solution**: \n",
    "  - Use **smoothing algorithms such as add-one (Laplace) smoothing**:\n",
    "  - $\\displaystyle \\hat{P}(w_i|c) = \\frac{\\text{count}(w_i, c) + 1}{\\sum_{w \\in V} (\\text{count}(w, c) + 1)}$\n",
    "\n",
    "\n",
    "### **Handling Unknown Words and Stop Words**\n",
    "- **Unknown Words**:\n",
    "  - Words in the test set but not in the training vocabulary are ignored.\n",
    "\n",
    "- **Stop Words**:\n",
    "  - High-frequency words like â€œtheâ€ or â€œaâ€ can be removed.\n",
    "  - Stop word removal does not always improve performance in text classification.\n",
    "\n",
    "\n",
    "\n",
    "### **Naive Bayes Algorithm - Training**\n",
    "- **Input:** the set of all training documents $D$, the set of all classes $C$\n",
    "- For each class $câˆˆ C$:\n",
    "  1. **Calculate P(c) terms**\n",
    "     - $N_{\\text{doc}}$ = number of documents in $D$\n",
    "     - $N_c$ = number of documents from $D$ in class $c$\n",
    "     - $\\text{logprior}[c] \\leftarrow \\log \\left( \\frac{N_c}{N_{\\text{doc}}} \\right)$\n",
    "     - $V \\leftarrow$ vocabulary of $D$\n",
    "     - $\\text{bigdoc}[c] \\leftarrow \\text{append}(d)$ for $d \\in D$ with class $c$\n",
    "\n",
    "  2. For each word $w \\in V$:\n",
    "     - **Calculate P(w|c) terms**\n",
    "       - $\\text{count}(w, c) \\leftarrow$ number of occurrences of $w$ in $\\text{bigdoc}[c]$\n",
    "       - $\\displaystyle \\text{loglikelihood}[w, c] \\leftarrow \\log \\left( \\frac{\\text{count}(w, c) + 1}{\\sum_{w' \\in V} (\\text{count}(w', c) + 1)} \\right)$\n",
    "- **Return**: Vocabulary $V$, log priors $P(c)$, and log likelihoods $P(w|c)$.\n",
    "\n",
    "\n",
    "### **Naive Bayes Algorithm - Testing**\n",
    "- **Input:** testdoc, logprior, loglikelihood, C, V\n",
    "- For each class $c \\in C$:\n",
    "  1. $\\text{sum}[c] \\leftarrow \\text{logprior}[c]$\n",
    "  2. For each position $i$ in $\\text{testdoc}$:\n",
    "     - $\\text{word} \\leftarrow \\text{testdoc}[i]$\n",
    "     - If $\\text{word} \\in V$:\n",
    "       - $\\text{sum}[c] \\leftarrow \\text{sum}[c] + \\text{loglikelihood}[\\text{word}, c]$\n",
    "- **Return:** $\\argmax_c \\, \\text{sum}[c]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ Example 3: Classifying emails using naive Bayes classifier\n",
    "#### 1. **Training Data**\n",
    "Let's take a small training dataset with the following emails:\n",
    "- Spam:\n",
    "  - Email 1: \"Buy cheap products now\"\n",
    "  - Email 2: \"Cheap deals available today\"\n",
    "  - Email 3: \"Get cheap tickets now\"\n",
    "- Ham:\n",
    "  - Email 1: \"Meeting is scheduled at 9\"\n",
    "  - Email 2: \"Your appointment is confirmed\"\n",
    "  - Email 3: \"Please confirm your attendance\"\n",
    "\n",
    "#### 2. **Vocabulary (V)**\n",
    "\n",
    "We first extract the vocabulary, i.e., all unique words across all emails.\n",
    "\n",
    "**Vocabulary (V):**\n",
    "```\n",
    "V = {\"buy\", \"cheap\", \"products\", \"now\", \"deals\", \"available\", \"today\", \n",
    "     \"get\", \"tickets\", \"meeting\", \"is\", \"scheduled\", \"at\", \"9\", \n",
    "     \"your\", \"appointment\", \"confirmed\", \"please\", \"confirm\", \"attendance\"}\n",
    "```\n",
    "**|V| (Size of vocabulary):** 20 words.\n",
    "\n",
    "#### 3. **Prior Probabilities: $P(c)$**\n",
    "\n",
    "To calculate the prior probability $P(c)$ of each class:\n",
    "\n",
    "- $N_{\\text{spam}} = 3$ (Spam has 3 emails)\n",
    "- $N_{\\text{ham}} = 3$ (Ham has 3 emails)\n",
    "- $N_{\\text{doc}} = 6$ (Total number of emails)\n",
    "\n",
    "We compute $P(\\text{spam})$ and $P(\\text{ham})$:\n",
    "\n",
    "$\\displaystyle P(\\text{spam}) = \\frac{N_{\\text{spam}}}{N_{\\text{doc}}} = \\frac{3}{6} = 0.5$\n",
    "\n",
    "$\\displaystyle P(\\text{ham}) = \\frac{N_{\\text{ham}}}{N_{\\text{doc}}} = \\frac{3}{6} = 0.5$\n",
    "\n",
    "So, the priors are:\n",
    "$P(\\text{spam}) = 0.5, \\quad P(\\text{ham}) = 0.5$\n",
    "\n",
    "#### 4. **Word Counts in Each Class**\n",
    "\n",
    "We now count the occurrences of each word in the **spam** and **ham** classes by concatenating all emails in each class.\n",
    "\n",
    "- **Spam concatenated:**\n",
    "  ```\n",
    "  \"buy cheap products now cheap deals available today get cheap tickets now\"\n",
    "  ```\n",
    "- **Ham concatenated:**\n",
    "  ```\n",
    "  \"meeting is scheduled at 9 your appointment is confirmed please confirm your attendance\"\n",
    "  ```\n",
    "\n",
    "The counts of words for each class are:\n",
    "\n",
    "| Word        | Spam (Count) | Ham (Count) |\n",
    "|-------------|--------------|-------------|\n",
    "| buy         | 1            | 0           |\n",
    "| cheap       | 3            | 0           |\n",
    "| products    | 1            | 0           |\n",
    "| now         | 2            | 0           |\n",
    "| deals       | 1            | 0           |\n",
    "| available   | 1            | 0           |\n",
    "| today       | 1            | 0           |\n",
    "| get         | 1            | 0           |\n",
    "| tickets     | 1            | 0           |\n",
    "| meeting     | 0            | 1           |\n",
    "| is          | 0            | 2           |\n",
    "| scheduled   | 0            | 1           |\n",
    "| at          | 0            | 1           |\n",
    "| 9           | 0            | 1           |\n",
    "| your        | 0            | 2           |\n",
    "| appointment | 0            | 1           |\n",
    "| confirmed   | 0            | 1           |\n",
    "| please      | 0            | 1           |\n",
    "| confirm     | 0            | 1           |\n",
    "| attendance  | 0            | 1           |\n",
    "\n",
    "#### 5. **Likelihood Calculation $P(w|c)$**\n",
    "\n",
    "Now, let's calculate the likelihood $P(w|c)$ using **Laplace (add-one) smoothing**.\n",
    "\n",
    "For a word $w$ and class $c$:\n",
    "$\\displaystyle P(w|c) = \\frac{\\text{count}(w, c) + 1}{\\sum_{w'} (\\text{count}(w', c) + 1)}$\n",
    "\n",
    "We need the total number of words in each class (plus the vocabulary size for smoothing):\n",
    "\n",
    "- **Total number of words in spam:** 12\n",
    "- **Total number of words in ham:** 13\n",
    "- **Vocabulary size:** $|V| = 20$\n",
    "\n",
    "Now, let's compute $P(w| \\text{spam})$ and $P(w| \\text{ham})$ for some example words:\n",
    "\n",
    "- **For word \"cheap\":**\n",
    "  $\\displaystyle P(\\text{\"cheap\"} | \\text{spam}) = \\frac{3 + 1}{12 + 20} = \\frac{4}{32} = 0.125$\n",
    "  \n",
    "  $\\displaystyle P(\\text{\"cheap\"} | \\text{ham}) = \\frac{0 + 1}{13 + 20} = \\frac{1}{33} = 0.0303$\n",
    "\n",
    "- **For word \"meeting\":**\n",
    "  $\\displaystyle P(\\text{\"meeting\"} | \\text{spam}) = \\frac{0 + 1}{12 + 20} = \\frac{1}{32} = 0.03125$\n",
    "  \n",
    "  $\\displaystyle P(\\text{\"meeting\"} | \\text{ham}) = \\frac{1 + 1}{13 + 20} = \\frac{2}{33} = 0.0606$\n",
    "\n",
    "#### 6. **Testing (Classifying a New Email)**\n",
    "\n",
    "Let's classify the new email: `\"Get cheap products\"`\n",
    "\n",
    "- **Tokenized email:** `[\"get\", \"cheap\", \"products\"]`\n",
    "\n",
    "We will compute the log-probabilities for both classes using the formula:\n",
    "\n",
    "$\\log P(c | \\text{email}) = \\log P(c) + \\sum_{w \\in \\text{email}} \\log P(w | c)$\n",
    "\n",
    "##### For Spam:\n",
    "\n",
    "$\\log P(\\text{spam}) = \\log(0.5) = -0.6931$\n",
    "\n",
    "$\\displaystyle \\log P(\\text{\"get\"} | \\text{spam}) = \\log\\left(\\frac{2}{32}\\right) = -2.7726$\n",
    "\n",
    "$\\log P(\\text{\"cheap\"} | \\text{spam}) = \\log(0.125) = -2.0794$\n",
    "\n",
    "$\\displaystyle \\log P(\\text{\"products\"} | \\text{spam}) = \\log\\left(\\frac{2}{32}\\right) = -2.7726$\n",
    "\n",
    "Total log-probability for spam:\n",
    "$\\log P(\\text{spam} | \\text{email}) = -0.6931 + (-2.7726) + (-2.0794) + (-2.7726) = -8.3178$\n",
    "\n",
    "##### For Ham:\n",
    "\n",
    "$\\log P(\\text{ham}) = \\log(0.5) = -0.6931$\n",
    "\n",
    "$\\displaystyle \\log P(\\text{\"get\"} | \\text{ham}) = \\log\\left(\\frac{1}{33}\\right) = -3.4965$\n",
    "\n",
    "$\\log P(\\text{\"cheap\"} | \\text{ham}) = \\log(0.0303) = -3.4965$\n",
    "\n",
    "$\\displaystyle \\log P(\\text{\"products\"} | \\text{ham}) = \\log\\left(\\frac{1}{33}\\right) = -3.4965$\n",
    "\n",
    "Total log-probability for ham:\n",
    "$\\log P(\\text{ham} | \\text{email}) = -0.6931 + (-3.4965) + (-3.4965) + (-3.4965) = -11.1827$\n",
    "\n",
    "#### 7. **Final Prediction**\n",
    "\n",
    "Since $\\log P(\\text{spam} | \\text{email}) = -8.3178$ is greater than $\\log P(\\text{ham} | \\text{email}) = -11.1827$, the classifier predicts **spam** for the email `\"Get cheap products\"`.\n",
    "\n",
    "#### 8. **Python implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The email 'Get cheap products' is classified as: spam\n",
      "The email 'Your meeting is confirmed' is classified as: ham\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Tokenize documents into words\n",
    "def tokenize(doc):\n",
    "    return doc.lower().split()\n",
    "\n",
    "# Training the Naive Bayes classifier\n",
    "def train_naive_bayes(data):\n",
    "    # Initialize variables\n",
    "    logprior = {}\n",
    "    loglikelihood = defaultdict(lambda: defaultdict(float))\n",
    "    class_word_count = defaultdict(lambda: defaultdict(int))\n",
    "    class_doc_count = defaultdict(int)\n",
    "    vocabulary = set()\n",
    "    Ndoc = len(data)\n",
    "    \n",
    "    # Concatenate documents by class\n",
    "    for label, doc in data:\n",
    "        class_doc_count[label] += 1\n",
    "        words = tokenize(doc)\n",
    "        vocabulary.update(words)\n",
    "        for word in words:\n",
    "            class_word_count[label][word] += 1\n",
    "    \n",
    "    # Calculate log P(c)\n",
    "    for label in class_doc_count:\n",
    "        logprior[label] = math.log(class_doc_count[label] / Ndoc)\n",
    "    \n",
    "    # Calculate log P(w|c) with Laplace smoothing\n",
    "    for label in class_doc_count:\n",
    "        total_word_count = sum(class_word_count[label].values())\n",
    "        for word in vocabulary:\n",
    "            word_count = class_word_count[label][word] + 1  # Add-one smoothing\n",
    "            loglikelihood[word][label] = math.log(word_count / (total_word_count + len(vocabulary)))\n",
    "    \n",
    "    return logprior, loglikelihood, vocabulary\n",
    "\n",
    "# Classify a new document\n",
    "def classify_naive_bayes(doc, logprior, loglikelihood, classes, vocabulary):\n",
    "    words = tokenize(doc)\n",
    "    scores = {label: logprior[label] for label in classes}\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary:\n",
    "            for label in classes:\n",
    "                scores[label] += loglikelihood[word][label]\n",
    "    \n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Sample training data\n",
    "training_data = [\n",
    "    (\"spam\", \"Buy cheap products now\"),\n",
    "    (\"ham\", \"Meeting is scheduled at 9\"),\n",
    "    (\"ham\", \"Your appointment is confirmed\"),\n",
    "    (\"spam\", \"Cheap deals available today\"),\n",
    "    (\"ham\", \"Please confirm your attendance\"),\n",
    "    (\"spam\", \"Get cheap tickets now\")\n",
    "]\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "logprior, loglikelihood, vocabulary = train_naive_bayes(training_data)\n",
    "\n",
    "# Sample classes\n",
    "classes = [\"spam\", \"ham\"]\n",
    "\n",
    "# Test the classifier with a new email\n",
    "test_email_1 = \"Get cheap products\"\n",
    "test_email_2 = \"Your meeting is confirmed\"\n",
    "\n",
    "predicted_class_1 = classify_naive_bayes(test_email_1, logprior, loglikelihood, classes, vocabulary)\n",
    "predicted_class_2 = classify_naive_bayes(test_email_2, logprior, loglikelihood, classes, vocabulary)\n",
    "\n",
    "# Output the predictions\n",
    "print(f\"The email '{test_email_1}' is classified as: {predicted_class_1}\")\n",
    "print(f\"The email '{test_email_2}' is classified as: {predicted_class_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Considerations in Sentiment Analysis**\n",
    "- Standard Naive Bayes text classification works well for sentiment analysis.\n",
    "- The performance can be greatly improved with some small optimizations such as\n",
    "  - Clip word counts at 1 per document (ignore frequency) because whether a word occurs matters more than its frequency\n",
    "    - This variant is called **Binary Multinomial Naive Bayes**\n",
    "  - Prepend **NOT** to every word after a negation word since **negation** can flip sentiment\n",
    "  - Add features to Naive Bayes for words in **sentiment lexicons**\n",
    "    - Sentiment Lexicons are pre-annotated lists of words with positive or negative sentiment\n",
    "\n",
    "\n",
    "### **Language Identification using Naive Bayes**\n",
    "- Words are not the best features in language identification.\n",
    "- **Character n-grams** (2, 3, or 4 characters) are more effective.\n",
    "- **Byte n-grams** treat text as raw bytes (ignoring Unicode)\n",
    "  - It can model statistics about the beginning or ending of words since spaces count as bytes\n",
    "- A widely used Naive Bayes system, [langid.py](https://github.com/saffsd/langid.py), is trained on multilingual data (Wikipedia, Twitter, religious texts).\n",
    "  - The system begins with all possible n-grams (1-4 length) and selects the 7000 most informative features.\n",
    "- ðŸŽ Example\n",
    "  ```bash\n",
    "  # install langid first\n",
    "  pip install langid\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "\n",
    "# Define some example texts in different languages\n",
    "texts = [\n",
    "    \"This is an English sentence.\",  # English\n",
    "    \"Esta es una oraciÃ³n en espaÃ±ol.\",  # Spanish\n",
    "    \"C'est une phrase en franÃ§ais.\",  # French\n",
    "    \"Das ist ein deutscher Satz.\",  # German\n",
    "    \"è¿™æ˜¯ä¸€ä¸ªä¸­æ–‡å¥å­ã€‚\",  # Chinese\n",
    "    \"ã“ã‚Œã¯æ—¥æœ¬èªžã®æ–‡ã§ã™ã€‚\"  # Japanese\n",
    "]\n",
    "\n",
    "# Iterate over each text and identify its language\n",
    "for text in texts:\n",
    "    lang, confidence = langid.classify(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Language: {lang}, Confidence: {confidence:.2f}\")\n",
    "    print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output languages above are represented by their two-letter ISO 639-1 codes:\n",
    "  - `en` â†’ English\n",
    "  - `es` â†’ Spanish\n",
    "  - `fr` â†’ French\n",
    "  - `de` â†’ German\n",
    "  - `zh` â†’ Chinese\n",
    "  - `ja` â†’ Japanese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Naive Bayes as a Language Model**\n",
    "\n",
    "- Naive Bayes classifiers can use various features: \n",
    "  - dictionaries, URLs, emails, phrases, etc.\n",
    "- When **only individual words** are used as features, Naive Bayes acts similarly to a **language model**.\n",
    "- Used as **Unigram Language Models**:\n",
    "  - Each class (e.g., positive, negative) has its own unigram language model.\n",
    "  - Probability of a sentence P(s|c) is the product of probabilities for each word $P(w_i|c)$ in that sentence:\n",
    "  - $P(s|c) = \\prod_{i \\in \\text{positions}} P(w_i|c)$\n",
    "- This makes Naive Bayes classifiers a powerful tool for tasks involving text, such as sentiment analysis or document classification.\n",
    "\n",
    "\n",
    "\n",
    "### **Naive Bayes Example for Sentence Classification**\n",
    "\n",
    "- Example: Sentiment classification with positive (+) and negative (-) classes.\n",
    "- Model Parameters:\n",
    "  \n",
    "| Word  | P(w\\|+) | P(w\\|-) |\n",
    "|-------|--------------|--------------|\n",
    "| I     | 0.1          | 0.2          |\n",
    "| love  | 0.1          | 0.001        |\n",
    "| this  | 0.01         | 0.01         |\n",
    "| fun   | 0.05         | 0.005        |\n",
    "| film  | 0.1          | 0.1          |\n",
    "\n",
    "- Sentence: **\"I love this fun film\"**\n",
    "  - $P(s|+) = 0.1 \\times 0.1 \\times 0.01 \\times 0.05 \\times 0.1 = 5 \\times 10^{-7}$\n",
    "  - $P(s|-) = 0.2 \\times 0.001 \\times 0.01 \\times 0.005 \\times 0.1 = 1.0 \\times 10^{-9}$  \n",
    "- Conclusion: The positive model assigns a **higher probability** to the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluating Naive Bayes Classifiers**\n",
    "- **Metrics**:\n",
    "  - **Precision**: The proportion of true positives among all predicted positives.\n",
    "  - **Recall**: The proportion of true positives among all actual positives.\n",
    "  - **F1-Score**: The `harmonic mean` of precision and recall.\n",
    "- **Evaluation Process**:\n",
    "  - Use **training, validation, and test sets**.\n",
    "  - Apply **cross-validation** to ensure the model generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Precision**:\n",
    "- `Precision` measures the proportion of `correctly predicted positive instances` among `all instances predicted as positive`.\n",
    "- $\\text{Precision} = \\dfrac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}$\n",
    "- **Example**: If 100 emails are classified as spam (predicted positives) and 80 of them are actually spam (true positives), then:\n",
    "  - $\\text{Precision} = \\dfrac{80}{100} = 0.8$\n",
    "\n",
    "### **Recall**:\n",
    "- `Recall` measures `the proportion of actual positives that were correctly predicted`.\n",
    "  - $\\text{Recall} = \\dfrac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}$\n",
    "- **Example**: If there are 90 actual spam emails and 80 were correctly identified, then:\n",
    "  - $\\text{Recall} = \\dfrac{80}{90} = 0.89$\n",
    "\n",
    "### **F1-Score**:\n",
    "- The F1-Score is the `harmonic mean of precision and recall`, providing a balance between the two.\n",
    "  - $\\text{F1-Score} = 2 \\times \\dfrac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ \n",
    "- **Example**: If precision is 0.8 and recall is 0.89, then:\n",
    "   - $\\text{F1-Score} = 2 \\times \\dfrac{0.8 \\times 0.89}{0.8 + 0.89} = 0.84$\n",
    "- **High F1-Score**: Indicates a good balance between Precision and Recall.\n",
    "- **Low F1-Score**: Indicates that the model struggles with either false positives or false negatives.\n",
    "\n",
    "\n",
    "#### **Harmonic Mean**:\n",
    "- $\\displaystyle\\text{HarmonicMean}(a_1, a_2, a_3, \\dots, a_n) = \\frac{n}{\\frac{1}{a_1} + \\frac{1}{a_2} + \\dots + \\frac{1}{a_n}}$\n",
    "- The `harmonic mean` is used because it is closer to the **minimum** of the values compared to the `arithmetic mean`.\n",
    "- It weighs lower values more heavily, providing a **conservative** estimate\n",
    "  - which is useful when combining precision and recall.\n",
    "- **Example**: The harmonic value for two values:\n",
    "  - $\\displaystyle\\text{HarmonicMean}(P, R) = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}}$\n",
    "\n",
    "\n",
    "### **F-measure as Harmonic Mean**\n",
    "- With parameter $\\alpha$, F-measure can be written as:\n",
    "  - $\\displaystyle F = \\frac{1}{\\alpha \\cdot \\frac{1}{P} + (1 - \\alpha) \\cdot \\frac{1}{R}}$\n",
    "  - **Î±**: Controls the balance between **Precision (P)** and **Recall (R)**.\n",
    "  - $\\displaystyle\\beta^2 = \\frac{1 - \\alpha}{\\alpha} $, making the F-measure dependent on the chosen tradeoff between P and R.\n",
    "- **Simplified F-measure ($F_\\beta$)**:\n",
    "  - $\\displaystyle F_\\beta = (1 + \\beta^2) \\cdot \\frac{P \\cdot R}{\\beta^2 \\cdot P + R}$\n",
    "    - **P** is Precision.\n",
    "    - **R** is Recall.\n",
    "    - **Î²** is the weight factor:\n",
    "      - **Î² > 1**: More weight to **Recall**, useful for minimizing false negatives.\n",
    "      - **Î² < 1**: More weight to **Precision**, useful for minimizing false positives.\n",
    "      - **Î² = 1**: Equal weight (this simplifies to the **F1-Score**).\n",
    "  - It gives **more weight to the lower value** (either Precision or Recall), leading to a more conservative and balanced evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Evaluation Process for Naive Bayes Classifiers**\n",
    "\n",
    "### **Training, Validation, and Test Sets**:\n",
    "- **Training Set**: Used to train the model by learning from labeled data.\n",
    "- **Validation Set**: Used for tuning hyperparameters and selecting the best model during the training process.\n",
    "- **Test Set**: Used to evaluate the final performance of the model on unseen data.\n",
    "\n",
    "### **Cross-Validation**:\n",
    "- **Definition**: Cross-validation involves dividing the data into k-folds, training the model on $k-1$ folds, and testing on the remaining fold. This is repeated $k$ times, with each fold serving as the test set once.\n",
    "- **Example**: In **5-fold cross-validation**, the data is split into 5 parts:\n",
    "  - Train on 4 folds, test on 1 fold, and repeat 5 times.\n",
    "  - Average the results from each iteration to get a robust estimate of the modelâ€™s performance.\n",
    "- **Advantages**:\n",
    "  - Helps to ensure that the model generalizes well to unseen data.\n",
    "  - Reduces the risk of overfitting by validating on multiple subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multiple-Class Classifier**\n",
    "- A classifier that predicts **more than two classes**.\n",
    "  - Examples: Positive, Neutral, Negative.\n",
    "- **Use Cases**:\n",
    "  - Sentiment Analysis (Positive, Neutral, Negative)\n",
    "  - Language Detection (English, French, Spanish)\n",
    "\n",
    "\n",
    "### **Classification Methods for Multi-Class Problems**\n",
    "- **1. One-vs-Rest (OvR)**:\n",
    "- Treats each class as a separate binary classification problem.\n",
    "  - Train **n** binary classifiers (one for each class).\n",
    "  - For a class $c_i$, treat all other classes as \"negative\".\n",
    "- **2. One-vs-One (OvO)**:\n",
    "- Create a classifier for **every pair** of classes.\n",
    "  - For $n$ classes, build $\\dfrac{n(n-1)}{2}$ binary classifiers.\n",
    "  - Classify by voting among the pairwise classifiers.\n",
    "- **3. Softmax Regression (Multinomial Logistic Regression)**:\n",
    "- Models the probability of each class directly.\n",
    "  - Outputs probabilities for each class and selects the class with the highest probability.\n",
    "\n",
    "\n",
    "### **Confusion Table for Multi-Class Classification**\n",
    "- A confusion matrix for multi-class classification has **rows** representing the **true classes** and **columns** representing the **predicted classes**.\n",
    "\n",
    "|              | Predicted: Class 1(A) | Predicted: Class 2(B) | Predicted: Class 3(C) | **Total Actual** |\n",
    "|--------------|-------------------|-------------------|-------------------|------------------|\n",
    "| **Actual: Class 1(A)** | 50                | 10                | 5                 | **65**           |\n",
    "| **Actual: Class 2(B)** | 8                 | 45                | 12                | **65**           |\n",
    "| **Actual: Class 3(C)** | 5                 | 15                | 50                | **70**           |\n",
    "| **Total Predicted** | **63**            | **70**            | **67**            | **200**          |\n",
    "\n",
    "- **Metrics Derived from the Confusion Table**:\n",
    "- **True Positives (TP)**: Correct predictions for each class.\n",
    "  - Class 1: 50\n",
    "  - Class 2: 45\n",
    "  - Class 3: 50\n",
    "\n",
    "- **False Positives (FP)**: Incorrect predictions for each class (predicted as that class but actually from another class).\n",
    "  - Class 1: 8 + 5 = 13\n",
    "  - Class 2: 10 + 15 = 25\n",
    "  - Class 3: 5 + 12 = 17\n",
    "\n",
    "- **False Negatives (FN)**: Incorrectly predicted as another class (actually from that class but predicted as another class).\n",
    "  - Class 1: 10 + 5 = 15\n",
    "  - Class 2: 8 + 12 = 20\n",
    "  - Class 3: 5 + 15 = 20\n",
    "- **Multi-Class Accuracy**:\n",
    "  - $\\text{Accuracy} = \\dfrac{\\text{Sum of True Positives}}{\\text{Total Number of Instances}}$\n",
    "\n",
    "\n",
    "\n",
    "### **Evaluating Multi-Class Classifiers**\n",
    "- For each class $c_i$, treat it as the \"positive\" class and all other classes as \"negative\".\n",
    "- **Precision for $c_i$**:\n",
    "  - $\\displaystyle\\text{Precision for } c_i = \\frac{\\text{True Positives for } c_i}{\\text{True Positives for } c_i + \\text{False Positives for } c_i}$\n",
    "  \n",
    "  - $\\displaystyle\\text{Precision}_A = \\frac{TP_A}{TP_A + FP_A} = \\frac{50}{50 + 13} = 0.7937$\n",
    "  \n",
    "  - $\\displaystyle\\text{Precision}_B = \\frac{TP_B}{TP_B + FP_B} = \\frac{45}{45 + 25} = 0.6429$\n",
    "  \n",
    "  - $\\displaystyle\\text{Precision}_C = \\frac{TP_C}{TP_C + FP_C} = \\frac{50}{50 + 17} = 0.7463$\n",
    "- **Recall for $c_i$**:\n",
    "  - $\\displaystyle\\text{Recall for } c_i = \\frac{\\text{True Positives for } c_i}{\\text{True Positives for } c_i + \\text{False Negatives for } c_i}$\n",
    "  \n",
    "  - $\\displaystyle\\text{Recall}_A = \\frac{TP_A}{TP_A + FN_A} = \\frac{50}{50 + 15} = 0.7692$\n",
    "  \n",
    "  - $\\displaystyle\\text{Recall}_B = \\frac{TP_B}{TP_B + FN_B} = \\frac{45}{45 + 20} = 0.6923$\n",
    "  \n",
    "  - $\\displaystyle\\text{Recall}_C = \\frac{TP_C}{TP_C + FN_C} = \\frac{50}{50 + 20} = 0.7143$\n",
    "- **F1-Score for $c_i$**:\n",
    "  - $\\displaystyle\\text{F1-Score}_A = \\frac{2 \\times \\text{Precision}_A \\times \\text{Recall}_A}{\\text{Precision}_A + \\text{Recall}_A} = \\frac{2 \\times 0.7937 \\times 0.7692}{0.7937 + 0.7692} = 0.7813$\n",
    "  \n",
    "  - $\\displaystyle\\text{F1-Score}_B = \\frac{2 \\times \\text{Precision}_B \\times \\text{Recall}_B}{\\text{Precision}_B + \\text{Recall}_B} = 0.6667$\n",
    "  \n",
    "  - $\\displaystyle\\text{F1-Score}_C = \\frac{2 \\times \\text{Precision}_C \\times \\text{Recall}_C}{\\text{Precision}_C + \\text{Recall}_C} = 0.7299$\n",
    "\n",
    "#### **Macro-Averaged Metrics**:\n",
    "- **Macro-averaged Precision**:\n",
    "  - $\\text{Macro Precision} = \\dfrac{\\sum_{i=1}^{n} \\text{Precision}_i}{n}=0.7276$\n",
    "\n",
    "- **Macro-averaged Recall**:\n",
    "  - $\\text{Macro Recall} = \\dfrac{\\sum_{i=1}^{n} \\text{Recall}_i}{n}=0.7143$\n",
    "- **Macro-average F1-Score**:\n",
    "  - $\\text{Macro F1-Score} = \\dfrac{\\sum_{i=1}^{n} \\text{F1-Score}_i}{n}=0.7299$\n",
    "\n",
    "#### **Micro-Averaging**:\n",
    "- Aggregates **True Positives**, **False Positives**, and **False Negatives** across all classes for an overall precision and recall.\n",
    "  - Considers each instance equally across all classes, regardless of the class distribution.\n",
    "- **Micro-Averaged Precision** = $\\dfrac{\\sum_{i=1}^{n} \\text{True Positives}_i}{\\sum_{i=1}^{n} (\\text{True Positives}_i + \\text{False Positives}_i)}$\n",
    "\n",
    "- **Micro-Averaged Recall** = $\\dfrac{\\sum_{i=1}^{n} \\text{True Positives}_i}{\\sum_{i=1}^{n} (\\text{True Positives}_i + \\text{False Negatives}_i)}$\n",
    "\n",
    "- **Micro-Averaged F1-Score** = $\\dfrac{2 \\times \\text{Micro Precision} \\times \\text{Micro Recall}}{\\text{Micro Precision} + \\text{Micro Recall}}$\n",
    "\n",
    "- Since **Micro Precision** and **Micro Recall** are equal when aggregating across all classes, the **Micro F1-Score** is calculated similarly to the F1-Score for binary classification:\n",
    "  - $\\text{Micro F1-Score} = \\text{Micro Precision} = \\text{Micro Recall}$ \n",
    "  \n",
    "  - $\\displaystyle =\\frac{TP_{\\text{total}}}{TP_{\\text{total}} + FP_{\\text{total}}} = \\frac{145}{145 + 55} = 0.7250$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Classifier Performance\n",
    "- **Problem**: How do we know if classifier A is better than classifier B?\n",
    "  - Example: Comparing the F1 score of a logistic regression classifier vs. a naive Bayes classifier.\n",
    "  - We observe the **effect size** Î´(x):\n",
    "    $\\delta(x) = M(A, x) - M(B, x)$\n",
    "    - Where $M(A, x)$ and $M(B, x)$ are the performance `metrics` (e.g., accuracy, F1) for classifiers A and B on test set $x$.\n",
    "    - Larger $Î´(x)$ indicates A might be better than B.\n",
    "\n",
    "\n",
    "### **Statistical Hypothesis Testing**\n",
    "- **Objective**: Is the observed performance difference statistically significant?\n",
    "  - **Null hypothesis (Hâ‚€)**: A is not better than B $(Î´(x) â‰¤ 0)$.\n",
    "  - **Alternative hypothesis (Hâ‚)**: A is better than B $(Î´(x) > 0)$.\n",
    "- **Question**: How likely is the observed $Î´(x)$ to occur by chance?\n",
    "  - The probability is formalized as the **p-value**:\n",
    "    $P(\\delta(X) \\geq \\delta(x) | Hâ‚€ \\text{ is true})$\n",
    "  - If the p-value is small enough (e.g., < 0.05), reject Hâ‚€ and conclude A is better than B.\n",
    "\n",
    "\n",
    "### **Effect Size and p-Value**\n",
    "- **Effect Size**: The difference in performance between A and B\n",
    "  - e.g., $Î´(x) = 0.2$ for accuracy.\n",
    "- **p-Value**: Measures how likely it is to observe a $Î´(x)$ as large as the one seen, assuming Hâ‚€ is true.\n",
    "  - **Small p-value (< 0.05)**: Unlikely to observe such a large effect under Hâ‚€ â†’ reject Hâ‚€.\n",
    "  - **Large p-value**: The observed difference could occur by chance â†’ fail to reject Hâ‚€.\n",
    "\n",
    "\n",
    "### **Non-Parametric Tests in NLP**\n",
    "- **Why Non-Parametric Tests?**\n",
    "  - Parametric tests assume distributions like normality, which may not hold in NLP.\n",
    "  - Non-parametric tests work by sampling data directly.\n",
    "- **Popular Tests** described in [Computer-Intensive Methods for Testing Hypotheses: An Introduction](https://www.wiley.com/en-us/Computer-Intensive+Methods+for+Testing+Hypotheses%3A+An+Introduction-p-9780471611363):\n",
    "  - **Approximate Randomization**\n",
    "  - **Bootstrap Test** (paired version, common in NLP)\n",
    "\n",
    "\n",
    "### **The Paired Bootstrap Test**\n",
    "- **Goal**: Create many virtual test sets ($b$ of them) from an observed test set to simulate different testing conditions.\n",
    "- [**Bootstrap Method**](./codes/03/btt.py):\n",
    "  1. Start with a test set $x$ of size $n$.\n",
    "  2. Randomly sample with replacement from $x$ to create virtual test sets $x(i)$.\n",
    "  3. Compute the difference $Î´(x(i))$ between A and B on each test set.\n",
    "  4. Count how often $Î´(x(i))$ exceeds `the observed 0` by $Î´(x)$ or more.\n",
    "     - $\\displaystyle p\\text{-value}(x) = \\frac{1}{b} \\sum_{i=1}^{b} ð•‹(\\delta(x(i)) - \\delta(x) \\geq 0)$\n",
    "     - Where $b$ is the number of bootstrap samples, and $ð•‹(x)$ is an indicator function that is 1 if $x$ is true and 0 otherwise.\n",
    "\n",
    "\n",
    "### **Paired Bootstrap Test**\n",
    "- Counts how often $Î´(x(i))$ exceeds `the expected value` of $Î´(x)$ by $Î´(x)$ or more:\n",
    "  - $\\displaystyle p\\text{-value}(x) = \\frac{1}{b} \\sum_{i=1}^{b} ð•‹(\\delta(x(i)) - \\delta(x) \\geq \\delta(x)) = \\frac{1}{b} \\sum_{i=1}^{b} ð•‹(\\delta(x(i)) \\geq 2\\delta(x))$\n",
    "- **Interpretation**: If very few bootstrap test sets have a difference as large as the observed $Î´(x)$, the p-value will be small, indicating A is likely better than B.\n",
    "\n",
    "\n",
    "### ðŸŽ **Bootstrap Example**\n",
    "- Assume test set with 10 documents.\n",
    "  - Logistic regression A accuracy: 0.70\n",
    "  - Naive Bayes B accuracy: 0.50\n",
    "  - âˆ´ Observed $Î´(x) = 0.20$\n",
    "- **Bootstrap Sampling**:\n",
    "  - Generate 10,000 virtual test sets by sampling with replacement.\n",
    "  - Compute $Î´(x(i))$ for each set.\n",
    "  - Count the number of times $Î´(x(i))$ exceeds $2Î´(x) = 0.40$.\n",
    "- **Result**: If only 47 out of 10,000 test sets exceed $2Î´(x)$, p-value = 0.0047 â†’ reject Hâ‚€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoiding Harms in Classification  \n",
    "- Classification algorithms, including naive Bayes, can perpetuate societal harms such as\n",
    "- `Representational harms`:  caused by a system that demeans a social group, for example by perpetuating negative stereotypes about them.\n",
    "- Other harms: like silencing marginalized groups.\n",
    "\n",
    "\n",
    "### **Representational Harms**\n",
    "- Harms caused by systems that demean a social group, perpetuating negative stereotypes.\n",
    "- **Example Study**: \n",
    "  - [Kiritchenko & Mohammad (2018)](https://aclanthology.org/S18-2005/): Sentiment analysis systems often assign more negative emotions to sentences with `African American names` than `European American names`.\n",
    "  - **Implications**: These biases reflect and perpetuate stereotypes.\n",
    "  \n",
    "\n",
    "### **Bias in Toxicity Detection**\n",
    "- **Goal**: Detect hate speech, harassment, or toxic language.\n",
    "- **Problem**: \n",
    "  - Toxicity classifiers may mistakenly flag non-toxic sentences that mention groups like women, blind people, or gay people.\n",
    "  - These errors can lead to the silencing of discourse.\n",
    "- **Examples Study**\n",
    "  - [Park et al. (2018)](https://doi.org/10.3115/v1/W14-2105): Classifiers flagging terms like â€œwomenâ€ as toxic.\n",
    "  - [Hutchinson et al. (2020)](https://doi.org/10.18653/v1/2020.acl-main.487): Misclassification of references to disabled people.\n",
    "  - [Sap et al. (2019)](https://doi.org/10.18653/v1/P19-1163): Bias against African-American Vernacular English.\n",
    "\n",
    "\n",
    "### **Causes of Model Bias**\n",
    "- **Training Data**: Models replicate and amplify biases present in data.\n",
    "- **Labels**: Biases in human labeling can affect model outputs.\n",
    "- **Resources**: Lexicons, pretrained embeddings, or model components can introduce bias.\n",
    "- **Model Architecture**: What the model is trained to optimize matters.\n",
    "\n",
    "\n",
    "## Mitigating Bias in Classification Models\n",
    "- **Current Research**: Ongoing work in addressing bias through data curation and evaluation.\n",
    "- **No General Solutions**: Each model needs to be critically evaluated.\n",
    "  \n",
    "### **Documenting Models: The Model Card**\n",
    "- **What is a Model Card?**: A tool to document model development and evaluation.\n",
    "  - It helps promote transparency and accountability\n",
    "- **Contents of a Model Card**:\n",
    "  - Training algorithms and parameters.\n",
    "  - Training data sources and preprocessing.\n",
    "  - Evaluation data sources and motivation.\n",
    "  - Intended use and user groups.\n",
    "  - Model performance across demographic groups.\n",
    "- **Example Model Card**\n",
    "  - **Training Data**: Describe where the data comes from and any biases.\n",
    "  - **Evaluation**: Provide details on model performance across different demographic groups.\n",
    "  - **Intended Use**: Specify what the model should and should not be used for."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
