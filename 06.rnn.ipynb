{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/06.rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/06.rnn.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# RNNs and LSTMs\n",
    "\n",
    "üìù SALP chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network(RNN)\n",
    "- **What is an RNN?**\n",
    "  - A neural network designed for sequential data.\n",
    "  - Ability to retain information across time steps.\n",
    "- **Application Areas:**\n",
    "  - Time Series Prediction, Natural Language Processing, Speech Recognition.\n",
    "\n",
    "---\n",
    "\n",
    "### **RNN Structures**\n",
    "![rnn loop](./images/rnn/loop.png)\n",
    "\n",
    "- RNN units consist of inputs, hidden layers, and outputs.\n",
    "- The `Looping mechanism (recurrent connection)` in the hidden layer feeds the output of one step as input to the next step.\n",
    "  - Offers a new way to represent `the prior context`, hundreds of words in the past\n",
    "    - Removed the fixed-length limit (a sliding window) on this prior context as in feedforward networks\n",
    "\n",
    "---\n",
    "\n",
    "### **Working Principle of RNNs**\n",
    "- RNN unfolded in time steps\n",
    "\n",
    "![temporal unfold](./images/rnn/unfold.png)\n",
    "\n",
    "- Takes a sequence of inputs $x_t$ at time step $t$\n",
    "- Hidden states $h_t$ capture information $W_{hh}^{(t-1)}$ from previous steps $h_{t-1}$.\n",
    "  - Retains information from previous steps (memory) to infer the next output.\n",
    "  - $h_t = f(W_{xh}^{(t)}x_t + W_{hh}^{(t-1)}h_{t-1} + b_h^{(t)})$\n",
    "  - $W_{xh}, W_{hh}, W_{hy}$: Weight matrices\n",
    "  - Activation functions $f$ and $g$ (typically $\\tanh$ or $ReLU$ for hidden states, and softmax for output)\n",
    "- Outputs $y_t$ are calculated at time step $t$:\n",
    "  - $y_t = g(W_{hy}^{(t)}h_t + b_y^{(t)})$\n",
    "  - Predictions at each time step depend on current input and hidden state.\n",
    "\n",
    "---\n",
    "\n",
    "### **Training RNNs**\n",
    "- Backpropagation Through Time (BPTT)\n",
    "- Loss calculated at each step, summed across the sequence.\n",
    "- Updates weights using gradient descent.\n",
    "  - $\\nabla L = \\sum_{t=1}^{T} \\nabla L_t$\n",
    "- **Challenges:**\n",
    "  - Vanishing/Exploding gradients.\n",
    "  - Long-term dependency issues.\n",
    "\n",
    "---\n",
    "\n",
    "### **Stacked RNNs**\n",
    "![stacked rnn](./images/rnn/stack.png)\n",
    "\n",
    "- Multiple RNN layers stacked on top of each other.\n",
    "- Output of one RNN layer serves as input to the next.\n",
    "- **Advantages:**\n",
    "  - Increases the capacity of the network.\n",
    "  - Handles more complex patterns and long-term dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bidirectional RNNs**\n",
    "![bidir rnn](./images/rnn/bidrnn.png)\n",
    "\n",
    "- RNNs with two hidden states: one moving forward, another moving backward.\n",
    "- Processes information from both past and future.\n",
    "- Useful for tasks where context from both directions is important (e.g., NLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üçé **Create RNNs**\n",
    "- **simple RNN**\n",
    "\n",
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Define a simple RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, input_shape=(timesteps, features)))\n",
    "model.add(Dense(output_units))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()\n",
    "```\n",
    "---\n",
    "\n",
    "- **Stacked RNN**:\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, return_sequences=True, input_shape=(timesteps, features)))\n",
    "model.add(SimpleRNN(50))\n",
    "model.add(Dense(output_units))\n",
    "```\n",
    "- **Bidirectional RNN**:\n",
    "```python\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(50), input_shape=(timesteps, features)))\n",
    "model.add(Dense(output_units))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs as Language Models\n",
    "\n",
    "**Language Models Overview**:\n",
    "- **Goal**: Predict the next word in a sequence given its preceding context.\n",
    "  - Example: Given the context *‚ÄúThanks for all the‚Äù*, how likely is the next word *‚Äúfish‚Äù*?\n",
    "  - $P(\\text{fish} | \\text{Thanks for all the})$\n",
    "  \n",
    "**Conditional Probability for Entire Sequences**:\n",
    "- Language models assign probabilities to entire sequences using the chain rule:\n",
    "  - $\\displaystyle P(w_{1:n}) = \\prod_{i=1}^{n} P(w_i | w_{<i})$\n",
    "- Each word‚Äôs probability depends on the context of the preceding words.\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparing Language Models**\n",
    "- **N-Gram Models**:\n",
    "  - **Fixed Context Size**: Probability depends on $n - 1$ previous words.\n",
    "  - Context limited to **short history**.\n",
    "\n",
    "![ffnn vs rnn](./images/rnn/fnnvrnn.png)\n",
    "\n",
    "- **a) Feedforward Models**:\n",
    "  - Context depends on a **window size**.\n",
    "  - **Fixed-length context** that is independent of the sequence length.\n",
    "\n",
    "- **b) RNN Language Models**:\n",
    "  - **Dynamic Context**: Uses all preceding words via hidden state.\n",
    "  - **No Fixed Context Limit**: The hidden state $h_{t-1}$ summarizes the entire past.\n",
    "  - Overcomes **limited context** of n-gram models and **fixed context** of feedforward models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Inference in an RNN Language Model\n",
    "- **Input Sequence $X$:**\n",
    "  - The input sequence $X = [x_1, x_2, \\dots, x_t, \\dots, x_N]$ consists of words represented as **one-hot vectors** of size $|V| \\times 1$, where $|V|$ is the vocabulary size.\n",
    "  - Assume the embedding dimension $d_e$ and hidden dimension $d_h$ are the same for simplicity, denoted as model dimension $d$.\n",
    "- **Output prediction $y$:** \n",
    "   - A vector representing a probability distribution over the vocabulary.\n",
    "- **Step-by-Step Process**:\n",
    "\n",
    "1. **Word Embedding**:\n",
    "   - Retrieve the word embedding $e_t$ for the current word $x_t$ using the embedding matrix $E$:\n",
    "     - $e_t = E x_t$\n",
    "     where:\n",
    "     - $E \\in \\mathbb{R}^{d \\times |V|}$ is the embedding matrix.\n",
    "     - $x_t \\in \\mathbb{R}^{|V| \\times 1}$ is the one-hot encoded word.\n",
    "     - $e_t \\in \\mathbb{R}^{d \\times 1}$ is the embedding vector.\n",
    "\n",
    "2. **Hidden State Update**:\n",
    "   - Compute the new hidden state $h_t$ using the previous hidden state $h_{t-1}$ and the current embedding $e_t$:\n",
    "     - $h_t = g(U h_{t-1} + W e_t)$\n",
    "     where:\n",
    "     - $U \\in \\mathbb{R}^{d \\times d}$ is the recurrent weight matrix.\n",
    "     - $W \\in \\mathbb{R}^{d \\times d}$ is the input weight matrix.\n",
    "     - $g$ is the activation function (commonly $\\tanh$).\n",
    "\n",
    "3. **Output Layer**:\n",
    "   - Compute the raw scores for each word in the vocabulary using the hidden state $h_t$:\n",
    "     - $\\hat{y}_t = V h_t$\n",
    "     where:\n",
    "     - $V \\in \\mathbb{R}^{|V| \\times d}$ is the output weight matrix.\n",
    "     - $\\hat{y}_t \\in \\mathbb{R}^{|V| \\times 1}$ contains the unnormalized scores over the vocabulary.\n",
    "\n",
    "4. **Softmax Layer**:\n",
    "   - Apply the **softmax** function to convert the scores into a probability distribution over the vocabulary:\n",
    "     - $\\hat{y}_t = \\text{softmax}(V h_t)$\n",
    "     - The probability that the next word is $w_{t+1} = k$ is given by:\n",
    "       - $P(w_{t+1} = k | w_1, \\dots, w_t) = \\hat{y}_t[k]$\n",
    "---\n",
    "\n",
    "### **Sequence Probability and Chain Rule**\n",
    "\n",
    "- The probability of an entire sequence $P(w_1, w_2, \\dots, w_n)=P(w_{1:n})$ is the product of the probabilities of each word in the sequence:\n",
    "  - $P(w_{1:n}) = \\prod_{i=1}^{n} P(w_i | w_{1:i-1}) = \\prod_{i=1}^{n} \\hat{y}_i[w_i]$\n",
    "  - $\\hat{y}_i[w_i]$ is the probability assigned to the true word $w_i$ at time step $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an RNN Language Model\n",
    "\n",
    "**Self-supervision**:\n",
    "- RNN language models are trained by **self-supervision**. \n",
    "  - At each time step $t$, the model predicts the next word in the sequence based on the preceding context.\n",
    "  - The sequence of words itself provides the **supervision** without needing labeled data.\n",
    "\n",
    "![training rnn as lm](./images/rnn/training.png)\n",
    "\n",
    "**Training Process**:\n",
    "- The model‚Äôs weights are adjusted using **Gradient Descent** to minimize the **average cross-entropy loss** across the sequence.\n",
    "  \n",
    "**Loss Function**:\n",
    "- We use **Cross-Entropy Loss (CE)** to measure how well the model‚Äôs predicted probability distribution $\\hat{y}_t$ matches the true next word‚Äôs distribution $y_t$.\n",
    "  - $\\displaystyle L_{CE} = - \\sum_{w \\in V} y_t[w] \\log \\hat{y}_t[w]$\n",
    "  - $V$ is the vocabulary,\n",
    "  - ‚àµ $y_t[w]$ is 1 for the true next word $w_{t+1}$ and 0 otherwise.\n",
    "    - represented as a one-hot vector corresponding to the vocabulary\n",
    "- ‚à¥ $L_{CE}(\\hat{y}_t, y_t) = - \\log \\hat{y}_t[w_{t+1}]$\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Process Steps**\n",
    "1. At time step $t$, input the correct word $w_t$ and the hidden state $h_{t-1}$ (which encodes the preceding context $w_1, w_2, \\dots, w_{t-1}$).\n",
    "2. Compute the predicted probability distribution $\\hat{y}_t$ for the next word.\n",
    "3. Calculate the **cross-entropy loss** $L_{CE}(\\hat{y}_t, y_t)$ based on the `true next word` $w_{t+1}$.\n",
    "4. Use **Gradient Descent** to adjust the weights in the RNN to minimize the total cross-entropy loss over the entire training sequence.\n",
    "5. Move to the next word in the sequence and repeat, always feeding the true word into the RNN.\n",
    "\n",
    "- Giving the model the `true history sequence` to predict the next word (rather than feeding the model its best prediction from the previous time step) is called **teacher forcing**.\n",
    "\n",
    "### üçé RNN as a language model\n",
    "- predict the next word giving previous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# Sample text data (sentences) for illustration\n",
    "sentences = [\n",
    "    \"I love deep learning\",\n",
    "    \"I love machine learning\",\n",
    "    \"deep learning is fun\",\n",
    "    \"machine learning is powerful\"\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 1000  # Adjust vocabulary size\n",
    "timesteps = 4      # Number of previous words to consider (sequence length)\n",
    "output_units = vocab_size  # Output units should match vocabulary size (for word prediction)\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Prepare input-output pairs for word prediction\n",
    "input_sequences = []\n",
    "output_words = []\n",
    "\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):\n",
    "        input_seq = seq[:i]\n",
    "        output_word = seq[i]\n",
    "        input_sequences.append(input_seq)\n",
    "        output_words.append(output_word)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=timesteps, padding='pre')\n",
    "\n",
    "# Convert output words to categorical format (one-hot encoding)\n",
    "output_words = tf.keras.utils.to_categorical(output_words, num_classes=vocab_size)\n",
    "\n",
    "# Reshape input to 3D for RNN [samples, timesteps, features]\n",
    "input_sequences = np.expand_dims(input_sequences, axis=-1)\n",
    "\n",
    "# Define a simple RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(50, input_shape=(timesteps, 1)))  # input_shape=(timesteps, features)\n",
    "model.add(Dense(output_units, activation='softmax'))  # Softmax for word prediction\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Show model summary and architecture\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "# Train the model (example)\n",
    "# Assuming you have a larger dataset, adjust the number of epochs, batch size, etc.\n",
    "model.fit(input_sequences, output_words, epochs=50, batch_size=16)\n",
    "\n",
    "# Example of predicting the next word given previous words\n",
    "def predict_next_word(model, tokenizer, input_text, max_length):\n",
    "    input_seq = tokenizer.texts_to_sequences([input_text])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_length, padding='pre')\n",
    "    input_seq = np.expand_dims(input_seq, axis=-1)\n",
    "    predicted = np.argmax(model.predict(input_seq), axis=-1)\n",
    "    predicted_word = tokenizer.index_word[predicted[0]]\n",
    "    return predicted_word\n",
    "\n",
    "# Predict the next word\n",
    "input_text = \"I love\"\n",
    "next_word = predict_next_word(model, tokenizer, input_text, timesteps)\n",
    "print(f\"Next word after '{input_text}': {next_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Tying\n",
    "- In RNN language models, both the input embedding matrix $E$ and the output matrix $V$ (for softmax) map words to vector spaces\n",
    "  - $V$ is essentially the transpose of $E$.\n",
    "  - $E$ is of shape $[d \\times |V|]$ and $V$ of shape $[|V| √ó d]$.\n",
    "\n",
    "- **Weight Tying**: Instead of learning two separate matrices $E$ and $V$, we **share** the same weights for both:\n",
    "  - Use $E$ for input embedding and $E^\\top$ for output prediction.\n",
    "  - This **halves** the number of parameters for embedding and softmax layers.\n",
    "\n",
    "- ‚à¥ weight-tied equations for an RNN language model\n",
    "  - $e_t = E x_t$\n",
    "  - $h_t = g(U h_{t-1} + W e_t)$\n",
    "  - $\\hat{y}_t = \\text{softmax}(E^\\top h_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNs in NLP Tasks\n",
    "**RNNs** are widely used in various NLP applications:\n",
    "\n",
    "- **Sequence Labeling**: e.g., part-of-speech(POS) tagging\n",
    "- **Sequence Classification**: e.g., Sentiment analysis\n",
    "- **Text Generation**: e.g., Machine translation, summarization\n",
    "\n",
    "---\n",
    "\n",
    "### **Sequence Labeling**\n",
    "\n",
    "- Assign a label to each token in a sequence\n",
    "  - e.g., POS tagging, named entity recognition.\n",
    "- Inputs: Pretrained **word embeddings** for each token.\n",
    "- Outputs: **Tag probabilities** for each token from a softmax layer.\n",
    "\n",
    "![pos](./images/rnn/pos.png)\n",
    "\n",
    "- **RNN Forward Inference**:\n",
    "  - Pass input tokens one-by-one through the RNN.\n",
    "  - Generate tag probabilities at each step using a softmax layer.\n",
    "  - Select the most likely tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sample data: words and their corresponding labels (e.g., POS tags)\n",
    "data = [\n",
    "    (['The', 'dog', 'barked'], ['DET', 'NOUN', 'VERB']),\n",
    "    (['A', 'cat', 'meowed'], ['DET', 'NOUN', 'VERB']),\n",
    "    (['The', 'bird', 'sings'], ['DET', 'NOUN', 'VERB'])\n",
    "]\n",
    "\n",
    "# Mapping words and labels to integers\n",
    "word_to_idx = {'<PAD>': 0}\n",
    "label_to_idx = {}\n",
    "idx = 1\n",
    "\n",
    "for sentence, labels in data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    for label in labels:\n",
    "        if label not in label_to_idx:\n",
    "            label_to_idx[label] = len(label_to_idx)\n",
    "\n",
    "idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "\n",
    "# Dataset class for sequence labeling\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data, word_to_idx, label_to_idx, max_len=5):\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = label_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, labels = self.data[idx]\n",
    "        sentence_idx = [self.word_to_idx[word] for word in sentence]\n",
    "        label_idx = [self.label_to_idx[label] for label in labels]\n",
    "\n",
    "        # Padding sequences to max length\n",
    "        sentence_idx += [self.word_to_idx['<PAD>']] * (self.max_len - len(sentence_idx))\n",
    "        label_idx += [self.label_to_idx['DET']] * (self.max_len - len(label_idx))  # Assume 'DET' as default padding tag\n",
    "\n",
    "        return torch.tensor(sentence_idx), torch.tensor(label_idx)\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "num_labels = len(label_to_idx)\n",
    "max_len = 5\n",
    "batch_size = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SequenceDataset(data, word_to_idx, label_to_idx, max_len=max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# RNN-based model for sequence labeling\n",
    "class RNNSequenceLabelingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels):\n",
    "        super(RNNSequenceLabelingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Convert words to embeddings\n",
    "        rnn_out, _ = self.rnn(x)  # Pass embeddings through RNN\n",
    "        logits = self.fc(rnn_out)  # Pass RNN output through the fully connected layer\n",
    "        return logits\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = RNNSequenceLabelingModel(len(word_to_idx), embedding_dim, hidden_dim, num_labels)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for sentences, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(sentences)\n",
    "\n",
    "        # Reshape logits and labels to calculate loss\n",
    "        logits = logits.view(-1, num_labels)  # Flatten logits to (batch_size * max_len, num_labels)\n",
    "        labels = labels.view(-1)  # Flatten labels to (batch_size * max_len)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "# Inference on a new sentence\n",
    "def predict(model, sentence, word_to_idx, idx_to_label):\n",
    "    model.eval()\n",
    "    sentence_idx = [word_to_idx.get(word, 0) for word in sentence]\n",
    "    sentence_idx += [word_to_idx['<PAD>']] * (max_len - len(sentence_idx))  # Pad sequence\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(sentence_idx).unsqueeze(0)  # Add batch dimension\n",
    "        logits = model(inputs)\n",
    "        predictions = torch.argmax(logits, dim=2).squeeze(0)  # Get label predictions\n",
    "\n",
    "    return [idx_to_label[pred.item()] for pred in predictions]\n",
    "\n",
    "# Example inference\n",
    "sentence = ['A', 'bird', 'sings']\n",
    "predicted_labels = predict(model, sentence, word_to_idx, idx_to_label)\n",
    "print(f'Input: {sentence}')\n",
    "print(f'Predicted Labels: {predicted_labels}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sequence Classification**\n",
    "- **Task**: Assign a label to an entire sequence.\n",
    "  - e.g., Sentiment analysis, document classification.\n",
    "- **Process**:\n",
    "  - Pass the sequence through an RNN, word-by-word.\n",
    "  - Use **final hidden state** $h_n$ as a compressed representation of the entire sequence.\n",
    "\n",
    "![text classification](./images/rnn/classification.png)\n",
    "\n",
    "**Training for Sequence Classification**:\n",
    "- Use a **feedforward network** with a softmax layer for classification.\n",
    "- **End-to-End Training**:\n",
    "  - Loss from the classification task is **backpropagated** through the entire network.\n",
    "  - Cross-entropy loss drives training.\n",
    "\n",
    "**Alternative Approach**:\n",
    "- Instead of using the final hidden state $h_n$, use a **pooling function** (mean or max) over all hidden states $h_1, h_2, ..., h_n$.\n",
    "  - $\\displaystyle\\bar{h} = \\frac{1}{n}\\sum_{i=1}^n h_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Sample data: sentences and their corresponding labels (e.g., sentiment: 0 for negative, 1 for positive)\n",
    "data = [\n",
    "    (\"The movie was fantastic\", 1),\n",
    "    (\"I did not like the film\", 0),\n",
    "    (\"It was an amazing experience\", 1),\n",
    "    (\"The plot was very boring\", 0),\n",
    "    (\"The acting was great\", 1),\n",
    "    (\"I would not recommend this movie\", 0),\n",
    "]\n",
    "\n",
    "# Mapping words to indices\n",
    "word_to_idx = {'<PAD>': 0}\n",
    "idx = 1\n",
    "\n",
    "for sentence, _ in data:\n",
    "    for word in sentence.split():\n",
    "        if word.lower() not in word_to_idx:\n",
    "            word_to_idx[word.lower()] = idx\n",
    "            idx += 1\n",
    "\n",
    "# Dataset class for sentence classification\n",
    "class SentenceDataset(Dataset):\n",
    "    def __init__(self, data, word_to_idx, max_len=10):\n",
    "        self.data = data\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.data[idx]\n",
    "        sentence_idx = [self.word_to_idx[word.lower()] for word in sentence.split()]\n",
    "\n",
    "        # Padding sequences to max length\n",
    "        sentence_idx += [self.word_to_idx['<PAD>']] * (self.max_len - len(sentence_idx))\n",
    "        sentence_idx = sentence_idx[:self.max_len]  # Truncate to max length if longer\n",
    "\n",
    "        return torch.tensor(sentence_idx), torch.tensor(label)\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 2  # Binary classification (positive, negative)\n",
    "max_len = 10\n",
    "batch_size = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SentenceDataset(data, word_to_idx, max_len=max_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# RNN-based model for sentence classification\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Convert words to embeddings\n",
    "        rnn_out, _ = self.rnn(x)  # Pass embeddings through RNN\n",
    "        hidden_state = rnn_out[:, -1, :]  # Take the hidden state of the last time step\n",
    "        logits = self.fc(hidden_state)  # Pass the last hidden state through the fully connected layer\n",
    "        return logits\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = RNNClassifier(len(word_to_idx), embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for sentences, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(sentences)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accuracy\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Inference on a new sentence\n",
    "def predict(model, sentence, word_to_idx):\n",
    "    model.eval()\n",
    "    sentence_idx = [word_to_idx.get(word.lower(), 0) for word in sentence.split()]\n",
    "    sentence_idx += [word_to_idx['<PAD>']] * (max_len - len(sentence_idx))  # Pad sequence\n",
    "    sentence_idx = sentence_idx[:max_len]  # Truncate to max length if longer\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = torch.tensor(sentence_idx).unsqueeze(0)  # Add batch dimension\n",
    "        logits = model(inputs)\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    return predicted_class\n",
    "\n",
    "# Example inference\n",
    "test_sentence = \"The film was boring\"\n",
    "predicted_label = predict(model, test_sentence, word_to_idx)\n",
    "label_map = {0: \"Negative\", 1: \"Positive\"}\n",
    "\n",
    "print(f'Sentence: \"{test_sentence}\"')\n",
    "print(f'Predicted Sentiment: {label_map[predicted_label]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Generation**\n",
    "- RNN-based models can generate text word-by-word conditioned on some other text.\n",
    "  - Applications: Machine translation, summarization, dialogue generation.\n",
    "  \n",
    "- **Autoregressive Generation**:  generates words by repeatedly sampling the next word conditioned on its previous choices.\n",
    "  1. Start with a seed (e.g., `<s>`) or a richer task-appropriate context\n",
    "     - e.g., questions in QA, documents to be summarized, etc.\n",
    "  2. Generate the first word by sampling from the softmax output.\n",
    "  3. Feed the generated word back into the RNN for the next time step.\n",
    "  4. Continue until the end-of-sequence marker is generated or a fixed length is reached.\n",
    "\n",
    "![autogregressive generation](./images/rnn/gen.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the text data\n",
    "text_data = \"\"\"\n",
    "Your text data goes here. This is an example of text that will be used to train the RNN. \n",
    "You can replace this with any large text dataset to improve results.\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "# Tokenization and creating sequences\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, text):\n",
    "        self.text = text.lower().split()\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.build_vocab()\n",
    "\n",
    "    def build_vocab(self):\n",
    "        words = set(self.text)\n",
    "        for i, word in enumerate(words):\n",
    "            self.word2idx[word] = i\n",
    "            self.idx2word[i] = word\n",
    "\n",
    "    def text_to_sequences(self):\n",
    "        return [self.word2idx[word] for word in self.text]\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "text_preprocessor = TextPreprocessor(text_data)\n",
    "sequences = text_preprocessor.text_to_sequences()\n",
    "vocab_size = text_preprocessor.get_vocab_size()\n",
    "\n",
    "# Create input-output pairs\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length]\n",
    "        target = data[i + seq_length]\n",
    "        sequences.append(seq)\n",
    "        targets.append(target)\n",
    "    return torch.tensor(sequences), torch.tensor(targets)\n",
    "\n",
    "seq_length = 5  # Number of words to consider as input\n",
    "X, y = create_sequences(sequences, seq_length)\n",
    "\n",
    "# Step 2: Define the RNN Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        output = self.fc(output[:, -1, :])  # Take the last output state\n",
    "        return output\n",
    "\n",
    "# Define model parameters\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "\n",
    "# Create the model\n",
    "model = RNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "def train_model(model, X, y, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch [{epoch}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Train the model for 100 epochs\n",
    "train_model(model, X, y, epochs=100)\n",
    "\n",
    "# Step 4: Text Generation\n",
    "def generate_text(seed_text, next_words, model, seq_length, vocab_size):\n",
    "    model.eval()\n",
    "    for _ in range(next_words):\n",
    "        # Convert seed text to sequences\n",
    "        seed_sequence = [text_preprocessor.word2idx[word] for word in seed_text.split()[-seq_length:]]\n",
    "        seed_sequence = torch.tensor(seed_sequence).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(seed_sequence)\n",
    "            predicted_idx = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        predicted_word = text_preprocessor.idx2word[predicted_idx]\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n",
    "\n",
    "# Generate text using seed text\n",
    "seed_text = \"your text text\"\n",
    "generated_text = generate_text(seed_text, next_words=20, model=model, seq_length=seq_length, vocab_size=vocab_size)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problems with RNNs**\n",
    "- **Distant Dependencies:**\n",
    "  - Hidden states update for current information.\n",
    "  - Hard to retain and recall distant information accurately.\n",
    "  - üçé Given *The flights the airline was canceling were full.*\n",
    "    - \"Airline\" is close to \"was\" (singular), but \"flights\" (plural) is distant, making prediction difficult.\n",
    "- **Vanishing Gradient**\n",
    "  - RNNs struggle to carry forward critical information over long sequences.\n",
    "  - RNNs update weights through sequences, resulting in repeated multiplication of gradients.\n",
    "  - Gradients shrink, leading to the `vanishing gradient problem`.\n",
    "\n",
    "---\n",
    "\n",
    "## Long Short-Term Memory (LSTM) Networks\n",
    "\n",
    "- **Solution to RNN Issues**:\n",
    "  - LSTMs manage context more effectively.\n",
    "  - Learn to remember important information and forget irrelevant data.\n",
    "- **Architecture**:\n",
    "  - Adds an `explicit context layer`.\n",
    "  - Uses gates to control the flow of information.\n",
    "    - Each consists of a feedforward layer, \n",
    "      - followed by a sigmoid activation function (control how much to open a gate)\n",
    "      - followed by a pointwise multiplication ‚äô with the layer being gated\n",
    "- Standard for modern systems requiring sequence processing, like NLP.\n",
    "\n",
    "![A single LSTM unit displayed as a computation graph](./images/rnn/lstmunit.png)\n",
    "\n",
    "### **LSTM Gate Functionalities**\n",
    "- **Forget Gate**: Discards unnecessary information.\n",
    "  - $f_t = \\sigma(U_f h_{t-1} + W_f x_t)$\n",
    "  - $k_t = c_{t-1} ‚äô f_t$\n",
    "- Compute the actual information from the previous hidden state and current inputs\n",
    "  - $g_t = \\tanh(U_g h_{t-1} + W_g x_t)$\n",
    "- **Add Gate (Input Gate)**: Adds the selected information to the context.\n",
    "  - $i_t = \\sigma(U_i h_{t-1} + W_i x_t)$\n",
    "  - $j_t = g_t ‚äô i_t$\n",
    "  - $c_t = j_t + k_t$\n",
    "- **Output Gate**: Determines what information is used for the current output.\n",
    "  - $o_t = \\sigma(U_o h_{t-1} + W_o x_t)$\n",
    "  - $h_t = o_t ‚äô \\tanh(c_t)$\n",
    "\n",
    "### üçé Text generation using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "# 1. Load and preprocess text data\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    return text\n",
    "\n",
    "# Sample text file\n",
    "text = \"\"\"The flights the airline was canceling were full. The sky was blue and clear. \n",
    "          The world of artificial intelligence is advancing rapidly.\"\"\"\n",
    "\n",
    "# 2. Tokenizing text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Converting text to sequences\n",
    "input_sequences = []\n",
    "for line in text.split('.'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# 3. Padding sequences to ensure uniform length\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Split input and label\n",
    "X = input_sequences[:, :-1]  # Input sequence\n",
    "y = input_sequences[:, -1]   # Label (next word)\n",
    "y = to_categorical(y, num_classes=total_words)  # Convert to one-hot encoding\n",
    "\n",
    "# 4. Building the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))  # Embedding layer\n",
    "model.add(LSTM(150, return_sequences=True))  # LSTM layer\n",
    "model.add(LSTM(100))  # Another LSTM layer\n",
    "model.add(Dense(total_words, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(X, y, epochs=100, verbose=1)\n",
    "\n",
    "# 6. Text generation function\n",
    "def generate_text(seed_text, next_words, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "# 7. Generate new text\n",
    "seed_text = \"The airline\"\n",
    "next_words = 10\n",
    "print(generate_text(seed_text, next_words, max_sequence_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Model with RNNs\n",
    "- **Sequence Labeling:** Input and output sequences have the same length (e.g., POS tagging).  \n",
    "- **Sequence-to-Sequence (Encoder-Decoder):** Input and output sequences differ in length and structure (e.g., machine translation).\n",
    "  - **Encoder RNN $e(.)$:** Converts input sequence $x_{1:n}$ into a contextualized representation $h^e_{1:n}$. \n",
    "    - $h^e_{1:n} = e(x_{1:n})$\n",
    "  - **Context Vector $c(.)$:** ,a function of $h^e_{1:n}$, captures the essence of the input sequence.\n",
    "    - $h^d_{1:m} = c(h^e_{1:n})$  \n",
    "  - **Decoder RNN $d(.)$:** Generates output sequence $y_{1:m}$ using the context vector $h^d_{1:m}$.\n",
    "    - $y_{1:m} = d(h^d_{1:m})$\n",
    "\n",
    "![Four architectures for NLP tasks](./images/rnn/fourarch.png)\n",
    "\n",
    "### **RNN Language Model**\n",
    "- Models the probability $p(y)$ of a sequence $y$.  \n",
    "  - $p(y) = p(y_1)p(y_2|y_1)p(y_3|y_1, y_2)...p(y_m|y_1,...,y_{m-1})$  \n",
    "- **Autoregressive Generation:** Use hidden states $h_t$ to predict next token $\\hat{y}_t$.\n",
    "  - $h_t = g(h_{t-1}, x_t)$\n",
    "  - $\\hat{y}_t = \\text{softmax} (h_t)$\n",
    "- Transition to Encoder-Decoder Model \n",
    "  - **Modification:** Add a sentence separator (e.g., `<s>`) token to the input sequence $x$.  \n",
    "  - **Sequence Translation:**   source text $x$ `<s>` target text $y$ ‚Üí target text $y$\n",
    "    - $p(y|x) = p(y_1|x)p(y_2|y_1, x)p(y_3|y_1, y_2, x)...$\n",
    "\n",
    "![Encoder-decoder for translation](./images/rnn/edtran.png)\n",
    " \n",
    "- **Process:** Encoder processes the English sentence, context vector is passed to the decoder, and Spanish sentence is generated step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### **Encoder-Decoder Architecture with RNNs**\n",
    "- **Encoder:** Generates hidden states for each token in the input.  \n",
    "- **Decoder:** Autoregressively generates output using context $c$ and previously predicted tokens.  \n",
    "- **Stacked Architectures:** Often use stacked biLSTMs for more complex representations.\n",
    "\n",
    "![Formal enc-dec translator](./images/rnn/formed.png)\n",
    "---\n",
    "\n",
    "- **Decoder Process**\n",
    "  - **Decoder Initialization:** \n",
    "    - $h_0^d = c = h_n^e$  \n",
    "  - **Decoder State Update:**  \n",
    "    - The context $c$ is used as input at each decoding timestep\n",
    "      - to keep long distance dependency\n",
    "    - $h_t^d = g(\\hat{y}_{t-1}, h_{t-1}^d, c)$  \n",
    "  - **Output:**  \n",
    "    - $\\hat{y}_t = \\text{softmax}(h_t^d)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Encoder-Decoder Model\n",
    "![Training RNN encoder-decoder of machine translation](./images/rnn/trantrain.png)\n",
    "\n",
    "-Encoder-decoder architectures are trained `end-to-end`.\n",
    "- **Training Data:** Pairs of source and target sequences\n",
    "  - e.g., machine translation datasets with aligned sentence pairs.  \n",
    "- **Input to Encoder:** Source text with a separator token.  \n",
    "- **Decoder Training:** Autoregressively predicts the next token starting from the separator.  \n",
    "- **Inference vs. Training:**  \n",
    "  - **Inference:** Decoder uses its own outputs to generate the next token\n",
    "    - may cause drift.  \n",
    "  - **Training:** The decoder is forced to use the `correct target token` from the training data instead of its own previous prediction.\n",
    "    - **Benefit:** Faster training and reduces error accumulation.\n",
    "- **Loss Computation**\n",
    "  - **Softmax Output:** The decoder produces probabilities for each possible word.  \n",
    "  - **Token-Level Loss:** Compare predicted vs. correct token.  \n",
    "  - **Sentence-Level Loss:** Average loss over all tokens in the sentence.\n",
    "- **Backpropagation Process:**  \n",
    "  1. Compute token-level loss.  \n",
    "  2. Backpropagate through the decoder.  \n",
    "  3. Backpropagate through the encoder.  \n",
    "- **Parameter Update:** Use gradient descent to optimize both encoder and decoder parameters, minimizing overall loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "**Bottleneck Problem**\n",
    "  - In a simple encoder-decoder model, the decoder relies solely on the `final hidden state` $h_n^e$ of the encoder, creating a `bottleneck`.\n",
    "  - Information at the beginning of the sentence may not be equally well represented in the context vector\n",
    "\n",
    "![encoder-decoder bottleneck](./images/rnn/bottleneck.png)\n",
    "\n",
    "- **AAttention Solution** solves this bottleneck by allowing the decoder to access `all encoder hidden states`, not just the final one.\n",
    "  - Instead of a static context vector, attention creates a `dynamically generated context` vector $c$ that takes into account relevant parts of the input at each decoding step.\n",
    "    - $c = f(h_1^e, h_2^e, ‚ãØ, h_n^e)$\n",
    "\n",
    "---\n",
    "\n",
    "### **Attention Mechanism**\n",
    "- The context vector $c_i$ is computed for each decoding step, based on a weighted sum of the encoder's hidden states.\n",
    "- Weights reflect how relevant each encoder state is to the current decoding step $h_i^d$.\n",
    "  - $h_i^d = g(\\hat{y}_{i-1}, h^d_{i-1}, c_i)$\n",
    "- The relevance score between each decoder hidden state $h^d_{i‚àí1}$ and encoder hidden state $h^e_j$ can be calculated via **dot-product attention**:\n",
    "  - $\\text{score}(h^d_{i‚àí1} , h^e_j ) = h^d_{i‚àí1} ¬∑ h^e_j$\n",
    "- Apply a **softmax** to these scores to normalize the relevance weights $Œ±_{ij}$, \n",
    "  - $Œ±_{ij} = \\text{softmax}(\\text{score}(h^d_{i‚àí1} , h^e_j ))$\n",
    "    - $\\displaystyle =\\dfrac{e^{Œ±_{ij}}}{‚àë_k e^{Œ±_{ik}}}$\n",
    "  - which are used to compute a fixed-length $c_i$ as a weighted sum of all encoder hidden states for the current decoder state:\n",
    "    - $\\displaystyle c_i=‚àë_j Œ±_{ij}h_j^e$\n",
    "\n",
    "![encoder-decoder network with attention](./images/rnn/withattension.png)\n",
    "---\n",
    "\n",
    "### **Sophisticated Attention**\n",
    "- More complex scoring functions can be used, like the **bilinear attention model**, where the score is parameterized by trainable weights $W_s$:\n",
    "  - $Œ±_{ij} = \\text{score}(h^d_{i‚àí1} , h^e_j) = h^d_{i‚àí1} W_s h^e_j$\n",
    "- This allows for different dimensionalities between encoder and decoder states, unlike dot-product attention.\n",
    "- The concept of attention is key to more advanced architectures, such as **self-attention** in transformers, which we‚Äôll explore further.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
