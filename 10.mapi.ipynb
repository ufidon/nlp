{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/10.mapi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/10.mapi.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "#  Model Alignment, Prompting, and In-Context Learning  \n",
    "üìù SALP chapter 12\n",
    "\n",
    "- **Prompting LLMs with Natural Language**: \n",
    "  - Large language models (LLMs) can perform tasks like translation, summarization, and sentiment analysis through `natural language prompts`, which act as `instructions` for the model.\n",
    "\n",
    "- **In-Context Learning**: \n",
    "  - Prompts serve as a learning signal, allowing LLMs to learn new tasks through `example-based instructions` `without altering their internal parameters`.\n",
    "\n",
    "- **Limitations of Pretraining**: \n",
    "  - LLMs are often ineffective at following instructions precisely due to insufficient pretraining; \n",
    "  - `instruction tuning`, which finetunes models on specific instructions and responses, helps improve task accuracy.\n",
    "\n",
    "- **Safety and Harm Prevention**: \n",
    "  - Pretrained LLMs can generate harmful or unsafe outputs; \n",
    "  - addressing this involves training for safety during instruction tuning to prevent harmful language or misinformation.\n",
    "\n",
    "- **Model Alignment**: \n",
    "  - To better align LLM behavior with human values, `preference alignment techniques` like RLHF (Reinforcement Learning from Human Feedback) are used alongside instruction tuning, \n",
    "  - enhancing model responses to align with user goals and safety standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "- **Purpose of a Prompt**: A prompt is a text input that guides a language model (LLM) to perform specific tasks by providing context for generating relevant outputs.\n",
    "\n",
    "  - Creating effective prompts for various tasks is known as `prompt engineering`, crucial for guiding LLMs to perform well.\n",
    "\n",
    "- **Sentiment Classification Example**: \n",
    "  - For sentiment analysis, a prompt like ‚ÄúIn short, our stay was‚Ä¶‚Äù can lead an LLM to generate contextually fitting completions, such as negative or positive summaries.\n",
    "- üçé **Example**: \n",
    "  - Prompt: ‚ÄúDid not like the service that I was provided... In short, our stay was‚Äù\n",
    "  - Output: ‚Äú‚Ä¶not a pleasant one. The staff at the front desk were not welcoming or friendly.‚Äù\n",
    "\n",
    "- **Task Versatility with Contextual Prompts**: LLMs can perform multiple tasks (e.g., summarization, translation, truthfulness check) by simply adjusting the prompt context.\n",
    "  - With the right prompts, a single LLM can adapt to different tasks, requiring minimal additional information to produce relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Prompt Templates for Efficiency**: Using templates for tasks like summarization, translation, and sentiment analysis reduces the need to create new prompts for each task.\n",
    "  - Templates include an `input placeholder followed by task-specific text`, guiding LLMs to generate targeted outputs.\n",
    "\n",
    "| Task                  | Prompt Template                               |\n",
    "|-----------------------|-----------------------------------------------|\n",
    "| **Summarization**     | `{input} ; tldr;`                            |\n",
    "| **Translation**       | `{input} ; translate to French:`             |\n",
    "| **Sentiment**         | `{input}; Overall, it was`                   |\n",
    "| **Fine-Grained Sentiment** | `{input}; What aspects were important in this review?` |\n",
    "\n",
    "  - Templates are `populated with actual input` text, creating `filled prompts` suitable for the LLM to process.\n",
    "  - Positioning task `instructions at the end` of the prompt helps direct LLMs to produce desired outputs.\n",
    "  - Unclear prompts can lead to unexpected outputs, as seen in cases where prompts don't specify clear constraints.\n",
    "- **Structured Answer Prompts**: Some prompts `specify answer choices` (e.g., positive or negative sentiment), making outputs more predictable and structured.\n",
    "  - Prompts can `enforce constraints` like response length, role-playing (persona responses), or structured outputs in formats like JSON.\n",
    "- **Instruction Tuning**: More complex prompts, such as `chain-of-thought` reasoning, prompt\n",
    "the system to `break down complex tasks`.\n",
    "- **Prompt Workflow**: Tasks are processed by \n",
    "  - selecting or designing a template with a `free parameter` for the input text \n",
    "  - filling it with input, \n",
    "  - generating output via autoregressive decoding, \n",
    "  - and using or refining the result for specific needs.\n",
    "\n",
    "- üçé**Example**:\n",
    "  - Prompt Template for Sentiment: `{input};` Overall, it was\n",
    "  - Filled Prompt: ‚Äú`Did not like the service‚Ä¶` Overall, it was‚Äù\n",
    "  - Expected Output: ‚Äú‚Ä¶not enjoyable‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from Demonstrations: Few-Shot Prompting\n",
    "- `Few-shot prompting` involves adding labeled examples (demonstrations) to prompts\n",
    "  - contrasting with zero-shot prompting which has no examples.\n",
    "- `A small number of demonstrations` generally improves model performance, \n",
    "  - with the first example yielding the most benefit \n",
    "  - and diminishing returns from additional examples.\n",
    "- Including too many demonstrations can reduce generalization, \n",
    "  - as models might overfit to specific example details instead of learning task patterns.\n",
    "- Demonstrations should ideally be `similar` to the current input, \n",
    "  - dynamically retrieved based on similarity, such as using embeddings.\n",
    "- Frameworks like [DSPy](https://drchrislevy.github.io/posts/dspy/dspy.html) can `programmatically optimize prompt performance` by identifying the best set of demonstrations to include based on task performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Context Learning and Induction Heads\n",
    "- Prompting, even without demonstrations, can enhance a model's ability to predict upcoming tokens, a process termed `in-context learning`.\n",
    "- `In-context learning` refers to a model‚Äôs capacity to improve task performance or predictive accuracy based `solely on information within a prompt context`.\n",
    "- `Induction heads`, specialized circuits in transformer attention mechanisms, are hypothesized to drive in-context learning by `recognizing and predicting repeated patterns`.\n",
    "- Induction heads function by identifying patterns like \"AB...A\" and predicting \"B\" to complete the sequence with a `prefix matching` component, leveraging a `copying mechanism` for prediction accuracy.\n",
    "  \n",
    "  ![preÔ¨Åx matching mechanism](./images/mlm/copy.png)\n",
    "\n",
    "- [Olsson et al. (2022)](https://arxiv.org/abs/2209.11895) suggest a `generalized fuzzy matching rule`: A*B*...A‚ÜíB*, \n",
    "  - where A*‚âàA and B*‚âàB (b ‚âà means semantically similar)\n",
    "  - allowing semantic similarity in repeated sequences, may underpin in-context learning.\n",
    "- Ablation studies by [Crosbie and Shutova (2022)](https://arxiv.org/abs/2407.07011) show that removing induction heads significantly reduces in-context learning performance, suggesting their critical role in prompt-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-training and Model Alignment\n",
    "- Limitations exist due to LLMs‚Äô main training goal of `predicting the next word`, which may lead to errors in following specific instructions.\n",
    "  - üçé [Examples](https://arxiv.org/abs/2203.02155) show LLMs may misunderstand prompts, like failing to translate or explain in child-friendly terms, instead generating unrelated text continuations.\n",
    "\n",
    "  ```python\n",
    "  Prompt: Explain the moon landing to a six year old in a few sentences.\n",
    "  Output: Explain the theory of gravity to a 6 year old.\n",
    "\n",
    "  Prompt: Translate to French: The small dog\n",
    "  Output: The small dog crossed the road.\n",
    "  ```\n",
    "\n",
    "- LLMs can produce harmful content, including misinformation, hate speech, and [stereotypes](https://doi.org/10.18653/v1/2023.acl-long.84), often even from [neutral prompts](https://doi.org/10.18653/v1/2020.findings-emnlp.301).\n",
    "- The `primary issue is the misalignment` between the LLMs‚Äô word-prediction objective and the human need for safe, helpful responses.\n",
    "- Two additional training approaches‚Äî`instruction tuning` and `preference alignment` (such as Reinforcement Learning from Human Feedback, RLHF)‚Äîhelp `align LLMs with human needs`.\n",
    "- `Base model` refers to an LLM before these alignment processes, which are applied `post-training` to improve safety and utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Alignment: Instruction Tuning\n",
    "- `Instruction tuning` enhances an LLM‚Äôs ability to follow diverse instructions across tasks.\n",
    "  - It fine-tunes a pretrained LLM on `task-specific instruction-response` pairs, improving general instruction-following.\n",
    "  - Treated as `supervised learning` using the same language modeling objective as pretraining, it trains the model to predict the next token using labeled instructions as training data, each instruction has a correct answer or response.\n",
    "- It differs from `domain adaptation`, where the LLM is fine-tuned on new domain data with all parameters updated.\n",
    "\n",
    "  ![Instruction tuning compared to the other kinds of finetuning.](./images/mapi/inst.png)\n",
    "\n",
    "- `Parameter-efficient fine-tuning`, such as LoRA, involves adding small new parameters specific to a domain while freezing the main model parameters.\n",
    "- `Task-based fine-tuning` adapts the model to particular tasks (like classification) with specialized classification heads\n",
    "  - The parameters of the pretrained model may be frozen or might be slightly updated.\n",
    "- Instruction tuning continues training on instruction data, but without adding new parameters or task-specific components.\n",
    "  - It is cost-effective, requiring far fewer resources than initial LLM pretraining, with training typically limited to several epochs over instruction datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions as Training Data\n",
    "- Instruction tuning combines natural language descriptions with labeled `task demonstrations` to update models via supervised fine-tuning, including specific task prompts and `complex instructions like length or persona constraints`.\n",
    "  \n",
    "- Large instruction-tuning datasets cover diverse tasks and languages. Examples include \n",
    "  - [Aya](https://paperswithcode.com/paper/aya-dataset-an-open-access-collection-for) (503M instructions in 114 languages for 12 tasks), \n",
    "  - [SuperNatural Instructions](https://arxiv.org/abs/2204.07705) (12M examples for 1600 tasks), \n",
    "  - [Flan](https://arxiv.org/abs/2301.13688) (15M examples for 1836 tasks), and \n",
    "  - [OPT-IML](https://arxiv.org/abs/2212.12017) (18M examples for 2000 tasks).\n",
    "\n",
    "- Four main methods for creating instruction-tuning data:\n",
    "  1. **Manual Writing**: Contributors generate instruction/response pairs, such as the 204K instances in Aya, written by volunteers to improve multilingual LLMs.\n",
    "  2. **Reuse of Curated Data**: Existing NLP datasets (e.g., [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)) are repurposed using templates to produce instruction/input-output pairs.\n",
    "  3. **Annotation Guidelines**: Detailed guidelines from crowd-sourced annotation are adapted as prompts for creating task examples.\n",
    "  4. **Language Model Assistance**: LLMs help generate safer responses and diverse paraphrases for instruction-tuning data, particularly in filtering and rephrasing harmful prompts, as shown by [Bianchi et al. (2024)](https://arxiv.org/abs/2309.07875).\n",
    "\n",
    "- Instruction tuning aims to enhance model safety and task versatility, as shown by adding safe responses to harmful questions to reduce potentially harmful model outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Instruction-Tuned Models\n",
    "- Instruction tuning aims to teach models to follow `general instructions` rather than perform specific tasks, requiring evaluation on tasks that are `novel` to the model.\n",
    "\n",
    "- The `leave-one-out method` is typically used for evaluation: models are instruction-tuned on many tasks and then tested on a withheld task to assess performance on new instructions.\n",
    "\n",
    "- Due to `task overlap` in large datasets (e.g., SuperNatural Instructions with 1,600 tasks), tasks are `grouped into clusters` based on similarity to avoid testing on tasks too similar to those in the training data.\n",
    "  - e.g. SuperNatural Instructions has 76 task clusters\n",
    "  - For evaluation, all datasets within a specific task cluster (e.g., sentiment analysis) are withheld during training and used for testing, ensuring that the model is genuinely tested on a new task type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting\n",
    "- Chain-of-thought prompting is a technique used to enhance language model performance on `challenging reasoning tasks by encouraging step-by-step reasoning`, similar to how humans solve complex problems.\n",
    "  - This technique involves adding reasoning steps to few-shot prompts, guiding the model to break down problems into smaller, logical steps, which helps in reaching the correct answer.\n",
    "\n",
    "- Studies have shown that including these reasoning steps improves accuracy on difficult tasks, particularly in domains like math, where complex reasoning is required\n",
    "  - e.g., [the GSM8k math problem dataset](https://paperswithcode.com/dataset/gsm8k).\n",
    "  - It also demonstrates that language models are more likely to arrive at the right answer when trained to replicate a [structured reasoning process](https://openai.com/index/learning-to-reason-with-llms/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Prompt Optimization\n",
    "- **Prompt optimization** aims to improve task performance by `iteratively searching for` and refining prompts based on their effectiveness for a given task.\n",
    "\n",
    "- **Core components** of this optimization process include \n",
    "  - `an initial prompt`, \n",
    "  - `a scoring metric` to evaluate prompt performance, \n",
    "  - and an `expansion method` to generate prompt variations.\n",
    "\n",
    "- **Beam search** is often used to manage the search space efficiently, focusing on high-performing prompt variations and balancing exploration depth with search constraints.\n",
    "- **Stopping criteria** for these searches typically combine a set number of iterations with early stopping if no improvements are found, similar to early stopping in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Scoring\n",
    "- **Candidate scoring** assesses the effectiveness of potential prompts by identifying promising options and eliminating ineffective ones, which is essential for efficient prompt optimization.\n",
    "\n",
    "- **Scoring methods** vary by task type: \n",
    "  - classification tasks use accuracy (0/1 loss), \n",
    "  - generative tasks use similarity metrics like [BERTScore](https://huggingface.co/spaces/evaluate-metric/bertscore), [BLEU (Bilingual Evaluation Understudy)](https://en.wikipedia.org/wiki/BLEU), or [ROUGE (Recall-Oriented Understudy for Gisting Evaluation)](https://en.wikipedia.org/wiki/ROUGE_(metric)), depending on how closely outputs match labeled data.\n",
    "\n",
    "- **Efficiency** is achieved by evaluating candidate prompts on a small sample of training data, minimizing the computational demands of scoring prompts against a full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Expansion\n",
    "- **Prompt expansion** aims to improve task performance by generating variants of an original prompt, often through `paraphrasing` or `truncating` prompts to create diverse prompt versions.\n",
    "\n",
    "- **Uninformed search** methods, like [Zhou et al. (2023)](https://arxiv.org/abs/2211.01910), generate prompt variants randomly without regard for quality, relying on later scoring to prioritize the best prompts.\n",
    "\n",
    "- **Informed search**, as used by [Prasad et al. (2023)](https://arxiv.org/abs/2203.07281), guides prompt expansion by critiquing failed examples: a model evaluates the prompt on training data, identifies errors, generates a critique, and uses this feedback to create better prompts.\n",
    "  - It improves prompt quality iteratively, applying critiques as feedback to refine prompts based on model performance on specific examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Prompted Language Models\n",
    "- Language models are evaluated through various metrics, such as perplexity, accuracy on NLP tasks, and benchmarks for efficiency, toxicity, and fairness.\n",
    "\n",
    "- In a prompting setup, model accuracy on multiple-choice questions can be assessed using datasets like [MMLU (Massive Multitask Language Understanding)](https://paperswithcode.com/dataset/mmlu), which contains knowledge and reasoning questions across 57 domains, such as medicine and computer science."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
