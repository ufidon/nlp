{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/10.mapi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/10.mapi.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "#  Model Alignment, Prompting, and In-Context Learning  \n",
    "üìù SALP chapter 12\n",
    "\n",
    "- **Prompting LLMs with Natural Language**: \n",
    "  - Large language models (LLMs) can perform tasks like translation, summarization, and sentiment analysis through `natural language prompts`, which act as `instructions` for the model.\n",
    "\n",
    "- **In-Context Learning**: \n",
    "  - Prompts serve as a learning signal, allowing LLMs to learn new tasks through `example-based instructions` `without altering their internal parameters`.\n",
    "\n",
    "- **Limitations of Pretraining**: \n",
    "  - LLMs are often ineffective at following instructions precisely due to insufficient pretraining; \n",
    "  - `instruction tuning`, which finetunes models on specific instructions and responses, helps improve task accuracy.\n",
    "\n",
    "- **Safety and Harm Prevention**: \n",
    "  - Pretrained LLMs can generate harmful or unsafe outputs; \n",
    "  - addressing this involves training for safety during instruction tuning to prevent harmful language or misinformation.\n",
    "\n",
    "- **Model Alignment**: \n",
    "  - To better align LLM behavior with human values, `preference alignment techniques` like RLHF (Reinforcement Learning from Human Feedback) are used alongside instruction tuning, \n",
    "  - enhancing model responses to align with user goals and safety standards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "- **Purpose of a Prompt**: A prompt is a text input that guides a language model (LLM) to perform specific tasks by providing context for generating relevant outputs.\n",
    "\n",
    "  - Creating effective prompts for various tasks is known as `prompt engineering`, crucial for guiding LLMs to perform well.\n",
    "\n",
    "- **Sentiment Classification Example**: \n",
    "  - For sentiment analysis, a prompt like ‚ÄúIn short, our stay was‚Ä¶‚Äù can lead an LLM to generate contextually fitting completions, such as negative or positive summaries.\n",
    "- üçé **Example**: \n",
    "  - Prompt: ‚ÄúDid not like the service that I was provided... In short, our stay was‚Äù\n",
    "  - Output: ‚Äú‚Ä¶not a pleasant one. The staff at the front desk were not welcoming or friendly.‚Äù\n",
    "\n",
    "- **Task Versatility with Contextual Prompts**: LLMs can perform multiple tasks (e.g., summarization, translation, truthfulness check) by simply adjusting the prompt context.\n",
    "  - With the right prompts, a single LLM can adapt to different tasks, requiring minimal additional information to produce relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Prompt Templates for Efficiency**: Using templates for tasks like summarization, translation, and sentiment analysis reduces the need to create new prompts for each task.\n",
    "  - Templates include an `input placeholder followed by task-specific text`, guiding LLMs to generate targeted outputs.\n",
    "\n",
    "  | Task                  | Prompt Template                               |\n",
    "  |-----------------------|-----------------------------------------------|\n",
    "  | **Summarization**     | `{input} ; tldr;`                            |\n",
    "  | **Translation**       | `{input} ; translate to French:`             |\n",
    "  | **Sentiment**         | `{input}; Overall, it was`                   |\n",
    "  | **Fine-Grained Sentiment** | `{input}; What aspects were important in this review?` |\n",
    "\n",
    "  - Templates are `populated with actual input` text, creating `filled prompts` suitable for the LLM to process.\n",
    "  - Positioning task `instructions at the end` of the prompt helps direct LLMs to produce desired outputs.\n",
    "  - Unclear prompts can lead to unexpected outputs, as seen in cases where prompts don't specify clear constraints.\n",
    "- **Structured Answer Prompts**: Some prompts `specify answer choices` (e.g., positive or negative sentiment), making outputs more predictable and structured.\n",
    "  - Prompts can `enforce constraints` like response length, role-playing (persona responses), or structured outputs in formats like JSON.\n",
    "- **Instruction Tuning**: More complex prompts, such as `chain-of-thought` reasoning, prompt\n",
    "the system to `break down complex tasks`.\n",
    "- **Prompt Workflow**: Tasks are processed by \n",
    "  - selecting or designing a template with a `free parameter` for the input text \n",
    "  - filling it with input, \n",
    "  - generating output via autoregressive decoding, \n",
    "  - and using or refining the result for specific needs.\n",
    "\n",
    "- üçé**Example**:\n",
    "  - Prompt Template for Sentiment: `{input};` Overall, it was\n",
    "  - Filled Prompt: ‚Äú`Did not like the service‚Ä¶` Overall, it was‚Äù\n",
    "  - Expected Output: ‚Äú‚Ä¶not enjoyable‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning from Demonstrations: Few-Shot Prompting\n",
    "- `Few-shot prompting` involves adding labeled examples (demonstrations) to prompts\n",
    "  - contrasting with zero-shot prompting which has no examples.\n",
    "- `A small number of demonstrations` generally improves model performance, \n",
    "  - with the first example yielding the most benefit \n",
    "  - and diminishing returns from additional examples.\n",
    "- Including too many demonstrations can reduce generalization, \n",
    "  - as models might overfit to specific example details instead of learning task patterns.\n",
    "- Demonstrations should ideally be `similar` to the current input, \n",
    "  - dynamically retrieved based on similarity, such as using embeddings.\n",
    "- Frameworks like [DSPy](https://drchrislevy.github.io/posts/dspy/dspy.html) can `programmatically optimize prompt performance` by identifying the best set of demonstrations to include based on task performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Context Learning and Induction Heads\n",
    "- Prompting, even without demonstrations, can enhance a model's ability to predict upcoming tokens, a process termed `in-context learning`.\n",
    "- `In-context learning` refers to a model‚Äôs capacity to improve task performance or predictive accuracy based `solely on information within a prompt context`.\n",
    "- `Induction heads`, specialized circuits in transformer attention mechanisms, are hypothesized to drive in-context learning by `recognizing and predicting repeated patterns`.\n",
    "- Induction heads function by identifying patterns like \"AB...A\" and predicting \"B\" to complete the sequence with a `prefix matching` component, leveraging a `copying mechanism` for prediction accuracy.\n",
    "  \n",
    "  ![preÔ¨Åx matching mechanism](./images/mlm/copy.png)\n",
    "\n",
    "- [Olsson et al. (2022)](https://arxiv.org/abs/2209.11895) suggest a `generalized fuzzy matching rule`: A*B*...A‚ÜíB*, \n",
    "  - where A*‚âàA and B*‚âàB (b ‚âà means semantically similar)\n",
    "  - allowing semantic similarity in repeated sequences, may underpin in-context learning.\n",
    "- Ablation studies by [Crosbie and Shutova (2022)](https://arxiv.org/abs/2407.07011) show that removing induction heads significantly reduces in-context learning performance, suggesting their critical role in prompt-based learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
