{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/cbds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/cbds.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "**Chatbots & Dialogue Systems**\n",
    "\n",
    "- üìù SALP chapter 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- `Conversation` is a foundational aspect of language, learned early and used widely, driving the design of `interactive programs` such as \n",
    "  - `frame-based` dialogue systems for `structured tasks` \n",
    "  - `chatbots` for more flexible, `unstructured` conversations\n",
    "  - many modern systems blend both, as seen in tools like Siri and ChatGPT.\n",
    "\n",
    "- Early chatbot ELIZA, designed to simulate a therapist, used `simple pattern-matching and regular expressions`, creating responses that seemed personalized and led users to feel emotionally connected.\n",
    "  - Its responses were crafted based on `specific keywords`, allowing the system to adapt replies to certain words for more engaging interactions, sometimes using `general responses` when no keywords were matched.\n",
    "  - It demonstrated that users could become emotionally involved, often treating the system as a human, which led to behaviors typical of human interactions, like sharing personal issues.\n",
    "\n",
    "- `Emotional attachment` to chatbots raises privacy concerns; \n",
    "  - users tend to disclose private information more freely, increasing risks, especially when the chatbot seems more human-like.\n",
    "  - Privacy and emotional impact require careful consideration when designing and deploying chatbots,\n",
    "    - as they may inadvertently influence users' emotional well-being and cognitive states.\n",
    "- Chatbot usage, especially in `sensitive areas`, may need oversight, such as Institutional Review Board (IRB) approval, to ensure `ethical interaction` with human participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Properties of Human Conversation\n",
    "- üçé A short dialog between Alice and Mr. Rabbit \n",
    "  ```python\n",
    "  Alice: \"Mr. Rabbit, why are you always in such a rush? Do you ever just... stop to smell the roses?\"\n",
    "  Rabbit: \"Smell the roses? I barely have time to smell the *carrots*! Honestly, I‚Äôm late for everything!\"\n",
    "  Alice: \"Well, at least you‚Äôre never early enough to make a hare-brained decision!\" \n",
    "  Rabbit: üôÇ \"Very funny, Alice. But with these ears, I‚Äôve heard them all!\" \n",
    "  ```\n",
    "- `Conversation Dynamics`\n",
    "   - Human conversation is a complex, joint activity requiring mutual understanding.\n",
    "   - Conversations are structured in `turns`, \n",
    "     - where each speaker contributes sequentially; \n",
    "     - `turn-taking` is essential for natural interaction, as speakers need to know when to start and stop talking.\n",
    "   - Dialogue systems must detect when a user has finished speaking to respond accurately, \n",
    "     - a task known as `endpoint detection`.\n",
    "\n",
    "- `Speech Acts`\n",
    "   - Each utterance in a conversation serves as a specific `speech act`, \n",
    "     - such as answering, requesting, or acknowledging.\n",
    "   - Speech acts are categorized into \n",
    "     - `Constatives` (statements), `Directives` (requests), `Commissives` (promises), and `Acknowledgments` (thanks), \n",
    "     - each reflecting the `speaker's intent`.\n",
    "   - Recognizing speech acts helps dialogue systems interpret user intentions and respond appropriately, \n",
    "     - such as answering questions or confirming details.\n",
    "`\n",
    "- `Grounding`\n",
    "   - *Grounding* is the process of confirming mutual understanding, \n",
    "     - often through `repeating or affirming` statements.\n",
    "     - Examples include saying \"OK\" or repeating key details, \n",
    "       - to establish mutual understanding and build common ground.\n",
    "   - Dialogue systems need grounding mechanisms to ensure they understand and maintain natural conversational flow.\n",
    "\n",
    "- `Dialogue Structure and Subdialogues`\n",
    "   - Conversations often follow structured patterns called `adjacency pairs`, \n",
    "     - like a `question and answer` or a `proposal and acceptance/rejection`.\n",
    "   - `Subdialogues` or side sequences, such as clarifications, \n",
    "     - can temporarily shift focus and require the system to manage interruptions and resume the main conversation.\n",
    "   - `Presequences`, or preliminary questions, set the stage for requests, \n",
    "     - like asking if the system can make reservations before making one.\n",
    "\n",
    "- `Initiative`\n",
    "   - *Initiative* in conversation can be held by one participant or shared; \n",
    "   - `mixed initiative` allows both parties to ask and answer questions, \n",
    "     - typical in human-human dialogue.\n",
    "   - Dialogue systems with full mixed initiative are challenging to build; \n",
    "     - many rely on either `system-initiative` (system-led) or `user-initiative` (user-led) approaches.\n",
    "\n",
    "- `Inference and Implicature`\n",
    "   - *Inference* and *implicature* enable systems to `deduce unstated information` from context, \n",
    "     - such as deducing travel dates based on a meeting time.\n",
    "   - Systems need to `interpret relevance and draw conclusions` beyond literal statements, \n",
    "     - a process crucial for `understanding implicit information` in human conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame-Based Dialogue Systems\n",
    "- `Task-based dialogue` systems assist users with `specific tasks`, such as travel reservations, \n",
    "  - by using `frames‚Äîknowledge structures` with slots for capturing task details, forming a `domain ontology`.\n",
    "- ![Architecture of a dialogue-state system for task-oriented dialogue](./images/chat/diag.png)\n",
    "- The dialogue-state architecture, a common frame-based structure, includes `six components`, with four key components covered here and speech recognition/synthesis introduced later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frames and Slot Filling\n",
    "- Task-based dialogue systems use `frames with slots` to gather necessary details for tasks like booking a hotel or setting an alarm.\n",
    "  - A frame-based system's goal is to `fill these slots` based on user input, \n",
    "    - using `questions` to clarify slot details.\n",
    "    - Simple systems use `pre-written questions`, while advanced systems `generate questions` dynamically.\n",
    "  - Slot fillers are constrained to specific semantic types, such as city or date.\n",
    "  \n",
    "| **Slot**            | **Type** | **Example Question**                      |\n",
    "|---------------------|----------|--------------------------------------------|\n",
    "| ORIGIN CITY         | city     | \"From what city are you leaving?\"          |\n",
    "| DESTINATION CITY    | city     | \"Where are you going?\"                     |\n",
    "| DEPARTURE TIME      | time     | \"When would you like to leave?\"            |\n",
    "| DEPARTURE DATE      | date     | \"What day would you like to leave?\"        |\n",
    "| ARRIVAL TIME        | time     | \"When do you want to arrive?\"              |\n",
    "| ARRIVAL DATE        | date     | \"What day would you like to arrive?\"       |  \n",
    "\n",
    "  - `Multiple frames` may be required for `different domains`, \n",
    "    - and the system must identify which frame and slot to use for each input.\n",
    "- `Three key tasks in slot filling` are \n",
    "  - domain classification, \n",
    "  - intent determination, and \n",
    "  - filling slots based on user input.\n",
    "  - üçé ‚ÄúShow me morning flights from Boston to San Francisco on Tuesday‚Äù \n",
    "    - fills slots for origin, destination, and time within the air-travel domain.\n",
    "- `Handwritten rules or machine learning methods` are used for slot-filling, \n",
    "  - with `regular expressions` for simpler tasks like setting an alarm.\n",
    "  - Most modern systems rely on supervised machine learning, \n",
    "    - using labeled examples for domain, intent, and slot-filling.\n",
    "- `BIO tagging` is often employed, where each word is tagged as `beginning (B), inside (I), or outside (O)` a slot label.\n",
    "  - Slot-filling architecture uses a language model encoder, feedforward layer, and softmax output to assign BIO tags.\n",
    "  - ![Slot-filling architecture](./images/chat/fillslot.png)\n",
    "- Synonyms or codes (e.g., ‚ÄúSan Francisco‚Äù to ‚ÄúSFO‚Äù) are normalized using dictionaries, \n",
    "  - with a mix of rules and machine learning used in practical applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Task-Based Dialogue\n",
    "- Task-based systems are evaluated based on \n",
    "  - `task success rate`: correctly completing tasks like booking flights,\n",
    "  - `slot error rate`: percentage of slots correctly filled.\n",
    "\n",
    "- üçé Given sentence `Make an appointment with Chris at 10:30 in Gates 104`, and extracted slot structure:\n",
    "\n",
    "| **Slot** | **Filler**    |\n",
    "|----------|---------------|\n",
    "| PERSON   | Chris         |\n",
    "| TIME     | 11:30 a.m.    |\n",
    "| ROOM     | Gates 104     |\n",
    "\n",
    "  - has a slot error rate of 1/3, since the TIME is wrong.\n",
    "\n",
    "- Additional metrics include \n",
    "  - precision, recall, F-score, \n",
    "  - efficiency costs, such as dialogue length in seconds or turns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Acts and Dialogue State\n",
    "- More complex task-based dialogue systems use `dialogue acts` and `dialogue states` \n",
    "  - to handle confirmations, clarifications, and nuanced interactions with users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialogue Acts\n",
    "- Dialogue acts, which extend speech acts, are used to `structure interactions` \n",
    "  - by defining specific functions like `confirming, requesting, or providing information`, \n",
    "  - tailored for particular dialogue tasks.\n",
    "  - üçé [Dialogue acts used by the HIS restaurant recommendation system](https://hal.science/hal-00598186/document)\n",
    "\n",
    "| **Tag**                  | **Sys** | **User** | **Description**                                               |\n",
    "|--------------------------|---------|----------|---------------------------------------------------------------|\n",
    "| HELLO(a = x, b = y, ...) | ‚úî       | ‚úî        | Open a dialogue and give info a = x, b = y, ...               |\n",
    "| INFORM(a = x, b = y, ...) | ‚úî       | ‚úî        | Give info a = x, b = y, ...                                   |\n",
    "| REQUEST(a, b = x, ...)   |  ‚úî      | ‚úî        | Request value for a given b = x, ...                          |\n",
    "| REQALTS(a = x, ...)      |  ‚ùå      |  ‚úî        | Request alternative with a = x, ...                           |\n",
    "| CONFIRM(a = x, b = y, ...) | ‚úî       | ‚úî        | Explicitly confirm a = x, b = y, ...                          |\n",
    "| CONFREQ(a = x, ..., d)   | ‚úî       |   ‚ùå       | Implicitly confirm a = x, ... and request value of d          |\n",
    "| SELECT(a = x, a = y)     | ‚úî       |  ‚ùå        | Implicitly confirm a = x, ... and request value of d          |\n",
    "| AFFIRM(a = x, b = y, ...) | ‚úî       | ‚úî        | Affirm and give further info a = x, b = y, ...                |\n",
    "| NEGATE(a = x)            |  ‚ùå       | ‚úî        | Negate and give corrected value a = x                         |\n",
    "| DENY(a = x)              | ‚ùå       | ‚úî        | Deny that a = x                                               |\n",
    "| BYE()                    | ‚úî       | ‚úî        | Close a dialogue                                              |\n",
    "\n",
    "- A sample tagset for [the HIS System](https://hal.science/hal-00598186/document) includes \n",
    "  - acts like INFORM, CONFIRM, and REQUEST to handle user needs,\n",
    "  - as shown in a HIS system example where users confirm preferences, inquire about specifics, and close dialogues.\n",
    "\n",
    "| **Utterance**                                        | **Dialogue Act**                                       |\n",
    "|------------------------------------------------------|--------------------------------------------------------|\n",
    "| U: Hi, I am looking for somewhere to eat.            | hello(task = find, type = restaurant)                   |\n",
    "| S: You are looking for a restaurant. What type of food do you like? | confreq(type = restaurant, food)         |\n",
    "| U: I‚Äôd like an Italian near the museum.              | inform(food = Italian, near = museum)                   |\n",
    "| S: Roma is a nice Italian restaurant near the museum. | inform(name = \"Roma\", type = restaurant, food = Italian, near = museum) |\n",
    "| U: Is it reasonably priced?                          | confirm(pricerange = moderate)                          |\n",
    "| S: Yes, Roma is in the moderate price range.         | affirm(name = \"Roma\", pricerange = moderate)            |\n",
    "| U: What is the phone number?                         | request(phone)                                          |\n",
    "| S: The number of Roma is 385456.                     | inform(name = \"Roma\", phone = \"385456\")                 |\n",
    "| U: Ok, thank you goodbye.                            | bye()                                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialogue State Tracking\n",
    "- The dialogue-state tracker determines the `current state of the frame` and `the most recent user dialogue act`, \n",
    "  - summarizing all user constraints.\n",
    "- Dialogue act detection involves `classifying the user's input sentence` using an encoder and an act classifier, \n",
    "  - with prior dialogue acts improving classification.\n",
    "- Dialogue-act detection and slot-filling tasks are often performed together, \n",
    "  - as dialogue acts constrain slot values.\n",
    "- The state tracker uses slot-filling output or a classifying model to track changes in slot values after each sentence.\n",
    "- Detecting correction acts is essential, as users may rephrase or correct utterances, \n",
    "  - which are harder to recognize due to speech adjustments like `hyperarticulation`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialogue Policy: Which act to generate\n",
    "- Early frame-based systems followed a simple `dialogue policy`: \n",
    "  - ask questions until all slots are filled, \n",
    "  - then query the database and report back.\n",
    "- A more advanced dialogue policy helps systems decide when to respond, \n",
    "  - ask for clarification, or take other actions, \n",
    "  - guiding the generation of dialogue acts.\n",
    "- Systems often misrecognize words or meaning, \n",
    "  - so they use explicit or implicit confirmation acts to ensure shared understanding with the user.\n",
    "  - Explicit confirmation acts allow users to easily correct misrecognitions, but they can be time-consuming and awkward,\n",
    "    - whereas implicit confirmation is more efficient.\n",
    "- Systems use `ASR (Automatic Speech Recognition) confidence levels` to decide \n",
    "  - when to confirm explicitly, implicitly, or reject based on transcription accuracy, \n",
    "  - with different thresholds for each action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural language generation: Sentence Realization\n",
    "- **Sentence realization** is the process of `generating a user response` after a dialogue act and slots are chosen by the content planner.\n",
    "- The system uses **delexicalization** to generalize training examples, \n",
    "  - replacing specific slot values with generic tokens for flexibility in generating sentences.\n",
    "- üçé [A restaurant recommendation system](https://www.isca-archive.org/interspeech_2017/nayak17_interspeech.html): \n",
    "  - The content planner selects a dialogue act and attributes (e.g., restaurant name, neighborhood, cuisine), \n",
    "  - and the sentence realizer generates different possible sentences based on these inputs.\n",
    "\n",
    "| Delexicalized sentences |\n",
    "|---------|\n",
    "| recommend(restaurant name= Au Midi, neighborhood = midtown, cuisine = french) |\n",
    "| 1. restaurant name is in neighborhood and serves cuisine food. |\n",
    "| 2. There is a cuisine restaurant in neighborhood called restaurant name. |\n",
    "\n",
    "- An **encoder-decoder model** is used to map frames (slots and fillers) to delexicalized sentences, \n",
    "  - which are later relexicalized with specific values.\n",
    "  - ![An encoder decoder sentence realizer mapping slots/fillers to English](./images/chat/delex.png)\n",
    "- The **encoder-decoder model** is trained on labeled dialogue corpora like MultiWOZ to improve sentence realization, \n",
    "  - enabling the system to generate varied responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbots\n",
    "- Chatbots evolved from early systems like ELIZA to neural models like ChatGPT, integrating NLP tasks.\n",
    "- Recent neural chatbots focus on functional applications like question answering and machine translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training chatbots\n",
    "- Chatbots are trained on `large language model data`, \n",
    "  - including web sources like Common Crawl, Wikipedia, and books.\n",
    "  - Additional `dialogue datasets`, like [Topical-Chat](https://github.com/alexa/Topical-Chat) and [EMPATHETIC DIALOGUES](https://paperswithcode.com/dataset/empatheticdialogues), \n",
    "    - are often used to train chatbots with real conversations.\n",
    "  - `Social media` data from platforms like Twitter, Reddit, and Weibo is also used, \n",
    "    - with posts treated as conversation starters and comments as replies.\n",
    "  - Datasets from the web are `filtered for toxicity` using toxicity classifiers before being used for training.\n",
    "- Chatbot models are typically trained using a `causal language model (LM) architecture (decoder-only)`, \n",
    "  - predicting each word based on previous words in a conversation.\n",
    "  - ![Training a causal (decoder-only) language model for a chatbot.](./images/chat/casual.png)\n",
    "- An alternative approach is to use an `encoder-decoder architecture`, \n",
    "  - where the encoder processes the entire conversation and the decoder generates the next turn.\n",
    "  - ![an encoder-decoder language model for a chatbot](./images/chat/ed.png)\n",
    "- Despite pretraining on dialogue data, \n",
    "  - further `fine-tuning` is often required to customize the chatbot for specific tasks.\n",
    "  - Fine-tuning stages are essential for improving the chatbot's responses and aligning with desired conversational behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning for Quality and Safety\n",
    "- Dialogue systems are fine-tuned using `labeled data` to improve the quality and safety of responses, \n",
    "  - ensuring sensible and interesting dialogue while `avoiding harmful suggestions`.\n",
    "- Fine-tuning involves training the system with high-quality, safe dialogues, \n",
    "  - often using a `multi-task learning approach` for tasks like answering questions and following instructions.\n",
    "- Additional discriminative data is used to downweight low-quality or harmful responses, \n",
    "  - with `human-labeled ratings` for safety and quality assigned to each system turn.\n",
    "- A language model can classify the quality and safety of responses by \n",
    "  - generating a label (e.g., `SENSIBLE, INTERESTING, UNSAFE`) \n",
    "  - in a two-phase process: `generative and discriminative`.\n",
    "- At inference time, the system generates responses and assigns `safety/quality labels` to filter out unsafe options, \n",
    "  - returning the `highest-ranking safe response` to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to perform retrieval as part of responding\n",
    "- Modern chatbots, like Sparrow, integrate `retrieval-based` components \n",
    "  - where a fake dialogue participant (e.g., Search Query) is used to query search engines for information.\n",
    "- Chatbot prompts can include special participants (`Search Query and Search Results`) \n",
    "  - to guide the system in generating search queries and handling fact-based questions.\n",
    "- Systems can be fine-tuned to trigger search queries by using labeled data, \n",
    "  - where labelers perform fact checks and create appropriate search queries for incorrect responses.\n",
    "  - The chatbot then uses search results as context to refine its responses, similar to retrieval-based question-answering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Chatbots\n",
    "- Chatbots are evaluated by \n",
    "  - `participants`: who chat with the bot\n",
    "  - or `observers`: who read transcripts.\n",
    "- Evaluations use [Likert scales](https://en.wikipedia.org/wiki/Likert_scale) to rate qualities like engagingness, fluency, and humanness.\n",
    "  - Observer evaluations focus on turn coherence or overall conversation quality.\n",
    "  - The acute-eval metric compares two systems on metrics like engagingness and knowledgability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue System Design\n",
    "- Dialogue systems, especially task-oriented ones, are closely linked to `Human-Computer Interaction (HCI)` and follow `user-centered` design.\n",
    "- Designers study users and tasks by \n",
    "  - interviewing users, \n",
    "  - examining similar systems, \n",
    "  - and analyzing human-human dialogues.\n",
    "- [Wizard-of-Oz](https://en.wikipedia.org/wiki/Wizard_of_Oz_experiment) systems allow simulation of dialogue interactions with a human `wizard` controlling responses, \n",
    "  - enabling early testing of system architecture and interface.\n",
    "- Wizard systems reveal design issues but may not fully replicate real system limitations,\n",
    "  - providing an initial understanding of dialogue dynamics.\n",
    "- Iterative user testing refines design to align with user behavior, \n",
    "  - while `value-sensitive design principles` consider the potential impact on the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical Issues in Dialogue System Design\n",
    "- `Ethical concerns` in AI and dialogue systems date back to early discussions, \n",
    "  - as seen in Mary Shelley‚Äôs *Frankenstein*, \n",
    "  - about the risks of creating agents without considering human impact.\n",
    "- `Safety` is crucial, especially for dialogue systems used in medical or emergency situations;\n",
    "  - incorrect advice can endanger users.\n",
    "- Dialogue systems may perpetuate harmful stereotypes, as seen with `Microsoft‚Äôs Tay`, \n",
    "  - which `learned offensive behavior` from biased user input.\n",
    "- Training data often contain `toxic language`, which AI systems can replicate and amplify,\n",
    "  - leading to representational harms for certain groups.\n",
    "- `Privacy` is a major issue, as dialogue systems frequently overhear personal information and users may disclose more to human-like systems.\n",
    "- `Gender bias` is prevalent, with many chatbots assigned female names, \n",
    "  - reinforcing subservient stereotypes and inadequately addressing harassment.\n",
    "- Addressing abuse and toxicity in dialogue systems is vital, \n",
    "  - with methods including toxicity detection and value-sensitive design practices.\n",
    "- Researchers prioritize participant consent and ethical reviews, \n",
    "  - often consulting `Institutional Review Boards (IRBs)` to protect human subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üèÉ Practice from HuggingFace NLP\n",
    "  - [Summarization](https://huggingface.co/learn/nlp-course/en/chapter7/5?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Preparing a multilingual corpus\n",
    "# https://huggingface.co/datasets/defunct-datasets/amazon_reviews_multi\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "!curl -L -o ./archive.zip https://www.kaggle.com/api/v1/datasets/download/mexwell/amazon-reviews-multi\n",
    "!unzip archive.zip\n",
    "\n",
    "data_files={\n",
    "    'train':'train.csv',\n",
    "    'validation':'validation.csv',\n",
    "    'test':'test.csv'}\n",
    "\n",
    "dataset = load_dataset('csv',data_files=data_files)\n",
    "\n",
    "partitioned_datasets = DatasetDict()\n",
    "\n",
    "# Get unique languages in the train, validation, and test sets\n",
    "languages = set(dataset['train'].unique('language'))\n",
    "languages.update(dataset['validation'].unique('language'))\n",
    "languages.update(dataset['test'].unique('language'))\n",
    "\n",
    "# Partition the dataset based on each unique language\n",
    "for lang in languages:\n",
    "    partitioned_datasets[lang] = DatasetDict({\n",
    "        \"train\": dataset[\"train\"].filter(lambda x: x[\"language\"] == lang),\n",
    "        \"validation\": dataset[\"validation\"].filter(lambda x: x[\"language\"] == lang),\n",
    "        \"test\": dataset[\"test\"].filter(lambda x: x[\"language\"] == lang)\n",
    "    })\n",
    "\n",
    "print(partitioned_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_dataset = partitioned_datasets['zh'] # Chinese\n",
    "chinese_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_dataset = partitioned_datasets['en'] # English\n",
    "english_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 show samples\n",
    "def show_samples(dataset, num_samples=3, seed=42):\n",
    "    sample = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "    for example in sample:\n",
    "        print(f\"\\n'>> Title: {example['review_title']}'\")\n",
    "        print(f\"'>> Review: {example['review_body']}'\")\n",
    "\n",
    "\n",
    "show_samples(english_dataset)\n",
    "show_samples(chinese_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 find an interesting domain of products\n",
    "english_dataset.set_format(\"pandas\")\n",
    "english_df = english_dataset[\"train\"][:]\n",
    "# Show counts for top 20 products\n",
    "english_df[\"product_category\"].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_dataset.set_format(\"pandas\")\n",
    "chinese_df = chinese_dataset[\"train\"][:]\n",
    "# Show counts for top 20 products\n",
    "chinese_df[\"product_category\"].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 choose books\n",
    "def filter_books(example):\n",
    "    return (\n",
    "        example[\"product_category\"] == \"book\"\n",
    "        or example[\"product_category\"] == \"digital_ebook_purchase\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch the format from \"pandas\" back to \"arrow\":\n",
    "english_dataset.reset_format()\n",
    "chinese_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_books = chinese_dataset.filter(filter_books)\n",
    "english_books = english_dataset.filter(filter_books)\n",
    "# the reviews are not strictly about books :(\n",
    "show_samples(chinese_books)\n",
    "show_samples(english_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 combine them into a bilingual dataset\n",
    "from datasets import concatenate_datasets, DatasetDict\n",
    "\n",
    "books_dataset = DatasetDict()\n",
    "\n",
    "for split in english_books.keys():\n",
    "    books_dataset[split] = concatenate_datasets(\n",
    "        [english_books[split], chinese_books[split]]\n",
    "    )\n",
    "    books_dataset[split] = books_dataset[split].shuffle(seed=42)\n",
    "\n",
    "# Peek at a few examples\n",
    "show_samples(books_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Models for text summarization\n",
    "# [mT5](https://huggingface.co/google/mt5-base) \n",
    "# [mBART-50](https://huggingface.co/facebook/mbart-large-50)\n",
    "from transformers import AutoTokenizer, MT5TokenizerFast\n",
    "\n",
    "model_checkpoint = \"google/mt5-small\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer = MT5TokenizerFast.from_pretrained(model_checkpoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram is especially useful for multilingual corpora\n",
    "# since it allows SentencePiece to be agnostic about accents, \n",
    "# punctuation, and the fact that many languages, \n",
    "# like Chinese, do not have whitespace characters.\n",
    "zinputs = tokenizer(\"ËøôÊú¨„ÄäË•øÊ∏∏ËÆ∞„ÄãÂ•ΩÊúâË∂£!\")\n",
    "zinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(zinputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einputs = tokenizer(\"I loved reading the Hunger Games!\")\n",
    "einputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(einputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocessing the data\n",
    "# `text_target` argument that allows you to \n",
    "# tokenize the labels in parallel to the inputs.\n",
    "\n",
    "max_input_length = 512\n",
    "max_target_length = 30\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"review_body\"],\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    # Ensure 'review_title' elements are strings\n",
    "    labels = tokenizer(\n",
    "        [str(title) for title in examples[\"review_title\"]], \n",
    "        max_length=max_target_length, \n",
    "        truncation=True,\n",
    "        padding=\"max_length\" # Pad to max_target_length\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = books_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Metrics for text summarization\n",
    "generated_summary = \"I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games\"\n",
    "\n",
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate all the metrics at once:\n",
    "scores = rouge_score.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Creating a strong baseline\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def three_sentence_summary(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])\n",
    "\n",
    "\n",
    "print(three_sentence_summary(books_dataset[\"train\"][1][\"review_body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the ROUGE scores for the baseline:\n",
    "def evaluate_baseline(dataset, metric):\n",
    "    summaries = [three_sentence_summary(text) for text in dataset[\"review_body\"]]\n",
    "    return metric.compute(predictions=summaries, references=dataset[\"review_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "score = evaluate_baseline(books_dataset[\"validation\"], rouge_score)\n",
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "rouge_dict = dict((rn, round(score[rn] * 100, 2)) for rn in rouge_names)\n",
    "rouge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Fine-tuning mT5 with the Trainer API\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "import torch\n",
    "\n",
    "batch_size = 8\n",
    "num_train_epochs = 8\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5.6e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    logging_steps=logging_steps,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    # Decode generated summaries into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # Decode reference summaries into text\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    # Compute ROUGE scores\n",
    "    result = rouge_score.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    # Extract the median scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste data_collator on a small batch of examples\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "    books_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "features = [tokenized_datasets[\"train\"][i] for i in range(2)]\n",
    "data_collator(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer with the standard arguments:\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Fine-tuning mT5 with ü§ó Accelerate\n",
    "# 7.1 Preparing everything for training\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a fresh version of the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the data collator and use this to define our dataloaders:\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 10\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Training loop\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            # Replace -100 in the labels as we can't decode them\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(\n",
    "                generated_tokens, skip_special_tokens=True\n",
    "            )\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(\n",
    "                decoded_preds, decoded_labels\n",
    "            )\n",
    "\n",
    "            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    result = rouge_score.compute()\n",
    "    # Extract the median ROUGE scores\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    print(f\"Epoch {epoch}:\", result)\n",
    "\n",
    "    # Save and upload\n",
    "    output_dir = './'\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(commit_message=f\"Training in progress epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Using your fine-tuned model\n",
    "from transformers import pipeline\n",
    "\n",
    "hub_model_id = \"./\"\n",
    "summarizer = pipeline(\"summarization\", model=hub_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(idx):\n",
    "    review = books_dataset[\"test\"][idx][\"review_body\"]\n",
    "    title = books_dataset[\"test\"][idx][\"review_title\"]\n",
    "    summary = summarizer(books_dataset[\"test\"][idx][\"review_body\"])[0][\"summary_text\"]\n",
    "    print(f\"'>>> Review: {review}'\")\n",
    "    print(f\"\\n'>>> Title: {title}'\")\n",
    "    print(f\"\\n'>>> Summary: {summary}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üèÉ Practice from HuggingFace NLP\n",
    "  - [Training a causal language model from scratch](https://huggingface.co/learn/nlp-course/en/chapter7/5?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. install required libraries\n",
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Gathering the data\n",
    "# 1.1 filter the code samples using the libraries as keywords\n",
    "def any_keyword_in_string(string, keywords):\n",
    "    for keyword in keywords:\n",
    "        if keyword in string:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 1.2 test the filter on two samples\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "example_1 = \"import numpy as np\"\n",
    "example_2 = \"import pandas as pd\"\n",
    "\n",
    "print(\n",
    "    any_keyword_in_string(example_1, filters), any_keyword_in_string(example_2, filters)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 stream the dataset and filter the elements we want:\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def filter_streaming_dataset(dataset, filters):\n",
    "    filtered_dict = defaultdict(list)\n",
    "    total = 0\n",
    "    for sample in tqdm(iter(dataset)):\n",
    "        total += 1\n",
    "        if any_keyword_in_string(sample[\"content\"], filters):\n",
    "            for k, v in sample.items():\n",
    "                filtered_dict[k].append(v)\n",
    "    print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
    "    return Dataset.from_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 apply the stream filter to the streaming dataset:\n",
    "# This cell will take a very long time to execute, so you should skip it and go to\n",
    "# the next one!\n",
    "from datasets import load_dataset\n",
    "\n",
    "split = \"train\"  # \"valid\"\n",
    "filters = [\"pandas\", \"sklearn\", \"matplotlib\", \"seaborn\"]\n",
    "\n",
    "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n",
    "filtered_data = filter_streaming_dataset(data, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Get the filtered dataset on the Hub instead\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
    "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "      # Pretraining the language model will take a while\n",
    "      # choose samples to test a fluent flow first\n",
    "      \"train\": ds_train.shuffle().select(range(50000)),\n",
    "      \"valid\": ds_valid.shuffle().select(range(500))\n",
    "      # later, use this\n",
    "      # \"train\": ds_train, \n",
    "      # \"valid\": ds_valid     \n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste a sample\n",
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 cut long context into chunks\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "context_length = 128\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"],\n",
    "    truncation=True,\n",
    "    max_length=context_length,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 apply chunking on the dataset\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initializing a new model\n",
    "# 3.1 load the pretrained configuration\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# freshly initialize a GPT-2 model\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 set up a data collator that will take care of creating the batches\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# DataCollatorForLanguageModeling supports  \n",
    "# both masked language modeling (MLM) \n",
    "# and causal language modeling (CLM).\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste the model on a sample\n",
    "out = data_collator([tokenized_datasets[\"train\"][i] for i in range(5)])\n",
    "for key in out:\n",
    "    print(f\"{key} shape: {out[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 configure the training arguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  fire up the Trainer\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Code generation with a pipeline\n",
    "# see how well the trained model actually works\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=\"./\", device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the mode on several tasks\n",
    "# 4.1 simple task of creating a scatter plot\n",
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create scatter plot with x, y\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# create scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 create a DataFrame from two arrays:\n",
    "txt = \"\"\"\\\n",
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "\n",
    "# create dataframe from x and y\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "df.insert(0,'x', x)\n",
    "for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 have the model help us use the groupby operation:\n",
    "txt = \"\"\"\\\n",
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with profession, income and name\n",
    "df = pd.DataFrame({'profession': x, 'income':y, 'name': z})\n",
    "\n",
    "# calculate the mean income per profession\n",
    "profession = df.groupby(['profession']).mean()\n",
    "\n",
    "# compute the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 see if we can also use it for scikit-learn and set up a Random Forest model:\n",
    "txt = \"\"\"\n",
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random forest regressor from scikit-learn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# fit random forest model with 300 estimators on X, y:\n",
    "rf = RandomForestRegressor(n_estimators=300, random_state=random_state, max_depth=3)\n",
    "rf.fit(X, y)\n",
    "rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training with ü§ó Accelerate\n",
    "# 5.1 Tokens can have a whitespace prefix\n",
    "# \n",
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"plt\",\n",
    "    \"pd\",\n",
    "    \"sk\",\n",
    "    \"fit\",\n",
    "    \"predict\",\n",
    "    \" plt\",\n",
    "    \" pd\",\n",
    "    \" sk\",\n",
    "    \" fit\",\n",
    "    \" predict\",\n",
    "    \"testtest\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    print(f\"Keyword {keyword} has id(s) {ids}\")\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 write a custom loss function\n",
    "# align the logits and inputs\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False)\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 load data for training\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_dataset[\"train\"], batch_size=32, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_dataset[\"valid\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 group the parameters\n",
    "# to determine which ones will get an additional weight decay\n",
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 evaluate the model regularly \n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "\n",
    "        losses.append(accelerator.gather(outputs.loss))\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 redefine our model to make sure we train from scratch again:\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(fp16=True)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the baseline before training\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 5_000\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 100 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"samples\": step * samples_per_step,\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                print(commit_message=f\"Training in progress step {step}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
