{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/srs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/srs.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Automatic Speech Recognition and Text-to-Speech\n",
    "\n",
    "üìù SALP chapter 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- An early know `automatic speech recognition (ASR)` example is [Radio Rex](https://en.wikipedia.org/wiki/Virtual_assistant) appeared in the 1920s, \n",
    "  - a toy responding to specific sounds like the `vowel [eh]` in \"Rex.\"  \n",
    "- Despite limitations in diverse environments, **modern ASR** `converts speech waveforms into text` and is widely used in\n",
    "  - smart appliances, personal assistants, call routing, transcription, and assisting individuals with disabilities.  \n",
    "- [Wolfgang von Kempelen‚Äôs late 18th-century speech synthesizer](https://en.wikipedia.org/wiki/Wolfgang_von_Kempelen%27s_speaking_machine) marked the first `text-to-speech (TTS)` system using mechanical components.  \n",
    "  - **Modern TTS** maps `text to audio waveforms`, aiding communication for visually impaired users and individuals with neurological disorders.  \n",
    "- ASR and TTS share core algorithmic principles:\n",
    "  - encoder-decoder models, \n",
    "  - Connectionist Temporal Classification (CTC) loss functions, \n",
    "  - word error rate evaluation, \n",
    "  - and acoustic feature extraction.  \n",
    "\n",
    "üî≠ Explore \n",
    "- [ASR models](https://www.gladia.io/blog/best-open-source-speech-to-text-models)\n",
    "- [TTS models](https://www.bentoml.com/blog/exploring-the-world-of-open-source-text-to-speech-models)\n",
    "\n",
    "üèÉ Play\n",
    "- [Whisper: a general-purpose speech recognition model](https://github.com/openai/whisper)\n",
    "- [OpenVoice: versatile instant voice cloning](https://github.com/myshell-ai/OpenVoice)\n",
    "- [UltraVox: a fast multimodal LLM for real-time voice](https://github.com/fixie-ai/ultravox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Automatic Speech Recognition Task\n",
    "- **Dimensions of Variation in ASR:**\n",
    "  - Vocabulary size:\n",
    "    - Small vocabularies (e.g., yes/no, digits) are highly accurate.\n",
    "    - Large vocabularies (up to 60,000 words) in open-ended tasks are more difficult.\n",
    "  - Speaker and context:\n",
    "    - Human-to-machine speech (dictation/dialogue systems) is easier.\n",
    "    - Read speech (e.g., audiobooks) is relatively easy.\n",
    "    - Conversational speech between humans is the hardest due to faster, less clear speech.\n",
    "  - Channel and noise:\n",
    "    - Quiet environments with head-mounted microphones are ideal.\n",
    "    - Noisy settings (e.g., streets, open car windows) complicate recognition.\n",
    "  - Accent and speaker characteristics:\n",
    "    - Recognition is better for accents/dialects similar to the training data.\n",
    "    - Regional/ethnic dialects and children's speech are more challenging.\n",
    "\n",
    "- **Key ASR Corpora:**\n",
    "  - [LibriSpeech](https://huggingface.co/datasets/facebook/multilingual_librispeech):\n",
    "    - Open-source, read-speech dataset with 1,000+ hours of audiobooks.\n",
    "    - Divided into \"clean\" (high quality) and \"other\" (lower quality) portions.\n",
    "  - [Switchboard Corpus](https://huggingface.co/datasets/cgpotts/swda):\n",
    "    - Prompted telephone conversations between strangers (~240 hours).\n",
    "    - Extensive linguistic labeling (e.g., dialogue acts, prosody).\n",
    "  - [CALLHOME Corpus](https://huggingface.co/datasets/talkbank/callhome):\n",
    "    - Unscripted 30-minute telephone conversations (friends/family).\n",
    "    - Focus on natural, casual speech.\n",
    "  - [Santa Barbara Corpus](https://www.linguistics.ucsb.edu/research/santa-barbara-corpus):\n",
    "    - Everyday spoken interactions across the US (e.g., conversations, town halls).\n",
    "    - Anonymized transcripts.\n",
    "  - [CORAAL](https://huggingface.co/datasets/Padomin/coraal-asr):\n",
    "    - Sociolinguistic interviews with African American speakers.\n",
    "    - Focus on African American Language (AAL).\n",
    "  - [CHiME Challenge](https://www.chimechallenge.org/):\n",
    "    - Datasets for robust ASR tasks in noisy, real environments (e.g., dinner parties).\n",
    "  - [AISHELL-1 Corpus](https://paperswithcode.com/dataset/aishell-1):\n",
    "    - 170 hours of Mandarin read speech from various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice [Hugging Face Audio course](https://huggingface.co/learn/audio-course)\n",
    "- üìñ [Unit 2. A gentle introduction to audio applications](https://huggingface.co/learn/audio-course/chapter2/introduction)\n",
    "  - üìù 1. Investigate audio applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets librosa soundfile\n",
    "\n",
    "# 1. Audio classification with a pipeline\n",
    "# 1.1 Load the dataset minds14\n",
    "# It contains recordings of people asking an e-banking system questions \n",
    "#   in several languages and dialects\n",
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# 1.2 Create a classifier\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"anton-l/xtreme_s_xlsr_300m_minds14\",\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste an example\n",
    "example = minds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying\n",
    "classifier(example[\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual label for this example is:\n",
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "id2label(example[\"intent_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatic speech recognition with a pipeline\n",
    "# using the same MINDS-14 dataset as before\n",
    "\n",
    "# transcribe an audio recording using the automatic-speech-recognition pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try an example\n",
    "example = minds[0]\n",
    "asr(example[\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare it to the actual transcription\n",
    "example[\"english_transcription\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Try it on Chinese\n",
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"zh-CN\", split=\"train\")\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste an example\n",
    "example = minds[0]\n",
    "example[\"transcription\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a pre-trained ASR model for Chinese language on the ü§ó Hub\n",
    "from transformers import pipeline\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", model=\"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\", device='cuda')\n",
    "asr(example[\"audio\"][\"array\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Audio generation with a pipeline\n",
    "# upgrade transformers\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Generating speech\n",
    "from transformers import pipeline\n",
    "\n",
    "# https://huggingface.co/suno/bark-small\n",
    "pipe = pipeline(\"text-to-speech\", model=\"suno/bark-small\", device='cuda')\n",
    "\n",
    "# try a text\n",
    "text = \"Ladybugs have had important roles in culture and religion, being associated with luck, love, fertility and prophecy. \"\n",
    "output = pipe(text)\n",
    "\n",
    "# play the speech\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(output[\"audio\"], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different language\n",
    "zh_text = \"ÊúàËêΩ‰πåÂïºÈúúÊª°Â§©ÔºåÊ±üÊû´Ê∏îÁÅ´ÂØπÊÑÅÁú†„ÄÇ ÂßëËãèÂüéÂ§ñÂØíÂ±±ÂØ∫ÔºåÂ§úÂçäÈíüÂ£∞Âà∞ÂÆ¢Ëàπ„ÄÇ\"\n",
    "output = pipe(zh_text)\n",
    "Audio(output[\"audio\"], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate audio with non-verbal communications and singing.\n",
    "song = \"‚ô™ In the jungle, the mighty jungle, the ladybug was seen. ‚ô™ \"\n",
    "output = pipe(song)\n",
    "Audio(output[\"audio\"], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Generating music\n",
    "# https://huggingface.co/facebook/musicgen-small\n",
    "music_pipe = pipeline(\"text-to-audio\", model=\"facebook/musicgen-small\", device='cuda')\n",
    "\n",
    "prompt = \"90s rock song with electric guitar and heavy drums\"\n",
    "\n",
    "# generate the music\n",
    "forward_params = {\"max_new_tokens\": 512}\n",
    "\n",
    "output = music_pipe(prompt, forward_params=forward_params)\n",
    "Audio(output[\"audio\"][0], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for ASR: Log Mel Spectrum\n",
    "- ASR converts waveforms into [log mel spectrum](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53) feature vectors.\n",
    "  - representing information from small time windows of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and Quantization\n",
    "- Speech recognizers process air pressure changes caused by vocalizations, visualized as waveforms showing air pressure over time. \n",
    "  - ![A waveform of an instance of the vowel iy](./images/stts/wave.png)\n",
    "    - A waveform of an instance of the vowel `i` in `Ààbe…™bi`\n",
    "- Analog sound waves are digitized through \n",
    "  - **sampling**: measuring wave amplitude at intervals,\n",
    "  - **quantization**: converting measurements into integers.  \n",
    "- **Nyquist frequency** defines the maximum measurable frequency as half the sampling rate; \n",
    "  - 8 kHz is sufficient for telephone speech, \n",
    "  - while 16 kHz is used for microphone speech.  \n",
    "- Different sampling rates (e.g., 8 kHz vs. 16 kHz) cannot be mixed in ASR training/testing, \n",
    "  - requiring downsampling for consistency.  \n",
    "- Quantization stores amplitudes as integers (e.g., 8-bit or 16-bit), \n",
    "  - with log compression (e.g., ¬µ-law) optimizing for human auditory sensitivity to smaller intensities.  \n",
    "- Audio files vary by sample rate, sample size, number of channels (mono/stereo), and storage type (linear vs. compressed).  \n",
    "- Common file formats include \n",
    "  - [.wav](https://ccrma.stanford.edu/courses/422-winter-2014/projects/WaveFormat/), a subset of Microsoft [Resource Interchange File Format (RIFF)](https://en.wikipedia.org/wiki/Resource_Interchange_File_Format-based), \n",
    "  - Apple‚Äôs [Audio Interchange File Format (AIFF)](https://en.wikipedia.org/wiki/Audio_Interchange_File_Format), \n",
    "  - and [raw headerless formats](https://en.wikipedia.org/wiki/Raw_audio_format).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "- `Spectral features` are extracted from small windows of speech, \n",
    "  - treating the signal as stationary within each window, \n",
    "  - despite speech being non-stationary overall.  \n",
    "- A **frame** represents the speech within each window, determined by three parameters:\n",
    "  - window size, or fame size with width in ms, \n",
    "  - frame stride, offset, or shift between successive windows, \n",
    "  - and window shape.\n",
    "  - ![Windowing, showing a 25 ms rectangular window with a 10ms stride](./images/stts/win.png)  \n",
    "- `Windowing` is performed by multiplying the signal $s(n)$ at time $n$ by the window function $w(n)$ at $n$, producing a windowed waveform $y[n]$.\n",
    "  - $y[n]=w[n]s[n]$\n",
    "- Common window shapes include \n",
    "  - **rectangular**: simple but causes boundary issues in Fourier analysis\n",
    "  - **Hamming**: smoothes boundary discontinuities for better feature extraction.\n",
    "- ![Windowing a sine wave with the rectangular or Hamming windows](./images/stts/winshape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Fourier Transform\n",
    "- The **Discrete Fourier Transform (DFT)** is used to extract spectral information from a windowed discrete-time signal $x[n]$. \n",
    "  - The output $X[k]$ represents the magnitude and phase of $N$ discrete frequency components, enabling visualization of the signal spectrum.\n",
    "- ![dft](./images/stts/dft.png)\n",
    "  - Left: A 25 ms Hamming-windowed portion of a signal from the vowel `i` in `Ààbe…™bi`\n",
    "  - Right: its spectrum computed by a DFT.\n",
    "\n",
    "- The **Fast Fourier Transform (FFT)** is an efficient algorithm for computing the DFT, optimized for $N$ values that are powers of 2.\n",
    "  - $X[k] = \\sum_{n=0}^{N-1} x[n]e^{-j\\frac{2\\pi kn}{N}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Filter Bank and Log\n",
    "- The **FFT results** provide the energy at each frequency band, \n",
    "  - but human hearing is biased toward low frequencies, \n",
    "    - aiding recognition of critical low-frequency features (e.g., vowels or nasals) over high-frequency features (e.g., fricatives). \n",
    "  - Incorporating this bias enhances speech recognition.\n",
    "\n",
    "- The **mel scale**, an auditory frequency scale, models human perception of pitch. \n",
    "  - It spaces sounds perceptually equidistant in pitch,\n",
    "  - with the mel frequency $m$ calculated from the raw frequency $f$ as:  \n",
    "    - $\\text{mel}(f) = 1127 \\ln(1 + \\dfrac{f}{700})$\n",
    "\n",
    "- A **mel filter bank**, composed of logarithmically spaced triangular filters, \n",
    "  - collects energy with fine resolution at low frequencies and coarse resolution at high frequencies. \n",
    "  - This approach creates a **mel spectrum**, representing the perceptual energy distribution.\n",
    "- ![The mel filter bank](./images/stts/melbank.png)\n",
    "\n",
    "- Applying a **logarithmic transformation** to the mel spectrum values mirrors the human logarithmic response to signal levels. \n",
    "  - This reduces sensitivity to variations in input power, such as changes in the speaker's distance from the microphone, stabilizing feature estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice [Hugging Face Audio course](https://huggingface.co/learn/audio-course)\n",
    "- [Unit 1. Working with audio data](https://huggingface.co/learn/audio-course/chapter1/introduction)\n",
    "  - üìù 1. Visualize audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. install the librosa used to plot the waveform for an audio signal\n",
    "!pip install librosa\n",
    "\n",
    "# 2. load audio data\n",
    "import librosa\n",
    "\n",
    "array, sampling_rate = librosa.load(librosa.ex(\"trumpet\"))\n",
    "\n",
    "# 3. plot audio waveform\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. plot frequency spectrum, i.e. frequency domain representation\n",
    "# power spectrum, which measures energy rather than amplitude; \n",
    "#   this is simply a spectrum with the amplitude values squared.\n",
    "import numpy as np\n",
    "\n",
    "dft_input = array[:4096]\n",
    "\n",
    "# calculate the DFT\n",
    "window = np.hanning(len(dft_input))\n",
    "windowed_input = dft_input * window\n",
    "dft = np.fft.rfft(windowed_input)\n",
    "\n",
    "# get the amplitude spectrum in decibels\n",
    "amplitude = np.abs(dft)\n",
    "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "# get the frequency bins\n",
    "frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "plt.plot(frequency, amplitude_db)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Amplitude (dB)\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot audio spetrogram\n",
    "# A spectrogram plots the frequency content of an audio signal as it changes over time. \n",
    "# It allows you to see time, frequency, and amplitude all on one graph. \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "D = librosa.stft(array)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot Mel spectrogram\n",
    "#  it shows the frequency content of an audio signal over time, \n",
    "# but on a different frequency axis.\n",
    "S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sampling_rate, fmax=8000)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 2. Load and explore an audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install the Datasets library\n",
    "!pip install datasets[audio]\n",
    "\n",
    "# 2. load and explore and audio dataset called MINDS-14\n",
    "# https://huggingface.co/datasets/PolyAI/minds14\n",
    "# It contains recordings of people asking an e-banking system questions \n",
    "# in several languages and dialects.\n",
    "from datasets import load_dataset\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste an example\n",
    "example = minds[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intent_class is a classification category of the audio recording.\n",
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "id2label(example[\"intent_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. remove unused features\n",
    "columns_to_remove = [\"lang_id\", \"english_transcription\"]\n",
    "minds = minds.remove_columns(columns_to_remove)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. listen to a few examples\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_audio():\n",
    "    example = minds.shuffle()[0]\n",
    "    audio = example[\"audio\"]\n",
    "    return (\n",
    "        audio[\"sampling_rate\"],\n",
    "        audio[\"array\"],\n",
    "    ), id2label(example[\"intent_class\"])\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column():\n",
    "        for _ in range(4):\n",
    "            audio, label = generate_audio()\n",
    "            output = gr.Audio(audio, label=label)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. visualize some examples\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "array = example[\"audio\"][\"array\"]\n",
    "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 3. Preprocessing an audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Resampling the audio data to the model‚Äôs expected sampling rate.\n",
    "from datasets import Audio\n",
    "\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# check it is resampled to the desired sampling rate:\n",
    "minds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filtering the dataset\n",
    "# e.g. limiting the audio examples to a certain duration\n",
    "MAX_DURATION_IN_SECONDS = 20.0\n",
    "\n",
    "\n",
    "def is_audio_length_in_range(input_length):\n",
    "    return input_length < MAX_DURATION_IN_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the filter\n",
    "# use librosa to get example's duration from the audio file\n",
    "new_column = [librosa.get_duration(path=x) for x in minds[\"path\"]]\n",
    "minds = minds.add_column(\"duration\", new_column)\n",
    "\n",
    "# use ü§ó Datasets' `filter` method to apply the filtering function\n",
    "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
    "\n",
    "# remove the temporary helper column\n",
    "minds = minds.remove_columns([\"duration\"])\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pre-processing audio data\n",
    "# preparing the data in the right format for model training.\n",
    "# convert the raw data into input features.\n",
    "# e.g. Whisper feature extractor: https://huggingface.co/papers/2212.04356\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# pre-process a single audio example by passing it through the feature_extractor.\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    features = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=True\n",
    "    )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the data preparation function to all of our training examples\n",
    "minds = minds.map(prepare_dataset)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have log-mel spectrograms as input_features in the dataset.\n",
    "# Let‚Äôs visualize it for one of the examples in the minds dataset:\n",
    "import numpy as np\n",
    "\n",
    "example = minds[0]\n",
    "input_features = example[\"input_features\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(\n",
    "    np.asarray(input_features[0]),\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    "    sr=feature_extractor.sampling_rate,\n",
    "    hop_length=feature_extractor.hop_length,\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. for simplicity, load the feature extractor and tokenizer for Whisper \n",
    "# via the so-called processor.\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 4. Streaming large audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. enable streaming mod\n",
    "gigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"xs\", streaming=True)\n",
    "\n",
    "# you can no longer access individual samples using Python indexing\n",
    "# you have to iterate over the dataset.\n",
    "next(iter(gigaspeech[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. preview several examples from a large dataset\n",
    "gigaspeech_head = gigaspeech[\"train\"].take(2)\n",
    "list(gigaspeech_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition Architecture\n",
    "- Similar to MT architectures, **ASR** uses an `encoder-decoder framework (RNNs or Transformers)`,\n",
    "  - mapping log mel spectral features to letters or wordpieces.  \n",
    "- ![Schematic architecture for an encoder-decoder speech recognizer.](./images/stts/aed.png)\n",
    "- **Attention-Based Encoder-Decoder (AED)**, also known as `listen attend and spell (LAS)`,\n",
    "  - maps acoustic feature sequences $F = (f_1, f_2, \\cdots, f_t)$ into output sequences like letters or wordpieces $Y=(‚ü®\\text{SOS}‚ü©, y_1, \\cdots, y_m, ‚ü®\\text{EOS}‚ü©)$.\n",
    "- To shorten long acoustic sequences (e.g., 200 frames for a 2-second word) to match much shorter text sequences (5 letters), a subsampling step, **length compression**, is applied.\n",
    "  - $F=(f_1, f_2, \\cdots, f_t) ‚Ü¶ X=(x_1,‚ãØ, x_n),\\; n ‚â™ f$\n",
    "  - The simplest algorithm **low frame rate compression** \n",
    "    - stacks acoustic vectors (e.g., concatenating 3 frames into one) to reduce sequence length, \n",
    "    - creating longer vectors at coarser intervals.  \n",
    "  - After compression, the architecture uses `RNNs (LSTMs) or Transformers`, with possible beam search integration for decoding.\n",
    "    - $\\displaystyle p(y_1, \\cdots, y_m) = ‚àè_{i=1}^{m} p(y_i | y_1, ‚ãØ, y_{i-1}, X)$\n",
    "    - Greedy decoding: $\\displaystyle \\hat{y}_i = \\substack{\\text{argmax} \\\\ \\text{char}‚àà\\text{Alphabet}} p(\\text{char}|y_1, ‚ãØ, y_{i-1}, X)$\n",
    "- ASR models can improve by rescoring hypotheses $Y$ using a larger external language model and interpolating its score with the encoder-decoder score.  \n",
    "  - Use beam search to generate an **n-best list** of sentence hypotheses, then rescore each using a **language model**. \n",
    "    - Combine the encoder-decoder score and language model score with a tunable weight Œª. \n",
    "    - $\\text{score}(Y|X) = \\dfrac{\\log P(Y|X)}{|Y|_c} + \\lambda \\log P_{\\text{LM}}(Y)$ \n",
    "    - The sentence length bias is addressed by normalizing probabilities by the number of characters $|Y|_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "- Encoder-decoders are trained with `cross-entropy loss`, calculated at each decoding step as:  \n",
    "   - $L_{\\text{CE}} = -\\log p(y_i | y_1, \\ldots, y_{i-1}, X)$ \n",
    "   - The total sentence loss is the sum over all tokens:  \n",
    "     - $L_{\\text{CE}} = -\\sum_{i=1}^{m} \\log p(y_i | y_1, \\ldots, y_{i-1}, X)$\n",
    "\n",
    "- **Teacher Forcing**: \n",
    "  - Training typically uses gold token history $y_i$, \n",
    "  - but can mix gold outputs with predictions $\\hat{y}_i$, e.g., \n",
    "    - using 90% gold and 10% decoder output of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectionist Temporal Classification (CTC) \n",
    "- **CTC** offers an alternative approach by `outputting a character for each input frame`,\n",
    "  - ensuring the output matches the input length, \n",
    "  - then collapsing sequences of identical letters into a shorter sequence.  \n",
    "- A naive collapsing function removes consecutive duplicate letters, \n",
    "- ![A naive algorithm for collapsing an alignment between input and letters.](./images/stts/collapse.png)\n",
    "  - but this can misrepresent words (e.g., \"dinner\" transcribed as \"diner\") \n",
    "  - and struggles with aligning silence in the input.  \n",
    "- CTC resolves these issues by introducing a **blank symbol ‚ê£** to the transcription alphabet,\n",
    "  - which helps handle silences and prevents incorrect letter collapsing across blanks. \n",
    "- ![The CTC collapsing function B](./images/stts/ctc.png) \n",
    "  - The collapsing function $B:a‚Ü¶y$ maps alignments $A$ to outputs $Y$ \n",
    "    - by removing blanks and collapsing repeated letters, \n",
    "    - enabling more accurate transcription.  \n",
    "  - The function $B$ is **many-to-one**, meaning multiple alignments can produce the same output. \n",
    "    - For example, several alignments can yield the word \"dinner.\" \n",
    "    - ![Three other legitimate alignments producing the transcript dinner](./images/stts/align.png) \n",
    "  - The inverse function $B^{-1}(Y)$ represents all possible alignments that can generate a given output $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTC Inference\n",
    "- CTC assumes conditional independence at each time step, calculating the **CTC Alignment Probability** $P_{\\text{CTC}}(A|X)$ of an alignment $\\hat{A}=\\{\\hat{a}_1,\\cdots, \\hat{a}_n\\}$ as:  \n",
    "   - $P_{\\text{CTC}}(A|X) = \\prod_{t=1}^T p(a_t|X)$  \n",
    "   - The best alignment is chosen greedily for each time step $t$ as:  \n",
    "     - $\\displaystyle \\hat{a}_t = \\arg\\max_{c \\in C} p_t(c|X)$  \n",
    "\n",
    "- CTC uses an encoder-only model, generating a hidden state $h_t$ for each time step and decoding via a softmax over the character vocabulary. \n",
    "  - The sequence $A$ is then passed to the collapsing function $B$ to produce the output sequence $Y$.\n",
    "  - ![Inference with CTC](./images/stts/infer.png)\n",
    "\n",
    "- The most probable alignment may not correspond to the most probable collapsed output $Y$, \n",
    "  - as multiple alignments can lead to the same $Y$. \n",
    "  - To find the most probable $Y$, sum over the probabilities of all possible alignments:  \n",
    "     - $\\displaystyle P_{\\text{CTC}}(Y|X) = \\sum_{A \\in B^{-1}(Y)} P(A|X)$ \n",
    "     - $\\displaystyle =\\sum_{A \\in B^{-1}(Y)} ‚àè_{t=1}^T p(a_t | h_t)$\n",
    "     - $\\displaystyle \\hat{Y} = \\arg\\max_{Y} P_{\\text{CTC}}(Y|X)$\n",
    "\n",
    "  - Summing over all alignments is computationally expensive. \n",
    "    - Instead, an approximate sum is achieved using a Viterbi beam search, \n",
    "    - focusing on high-probability alignments mapping to the same output.  \n",
    "\n",
    "- Due to the independence assumption, CTC does not learn a language model. \n",
    "  - To enhance predictions, interpolate a language model score $P_{\\text{LM}}(Y)$ and a length factor $L(Y)$ with trained weights:  \n",
    "   - $\\text{score}_{\\text{CTC}}(Y|X) = \\log P_{\\text{CTC}}(Y|X) + \\lambda_1 \\log P_{\\text{LM}}(Y) + \\lambda_2 L(Y)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTC Training\n",
    "- The CTC-based ASR system uses negative log-likelihood loss over a dataset $D$, defined as:  \n",
    "   - $\\displaystyle L_{\\text{CTC}} = -\\sum_{(X, Y) \\in D} \\log P_{\\text{CTC}}(Y|X)$  \n",
    "\n",
    "- Computing $P_{\\text{CTC}}(Y|X)$ requires summing probabilities over all alignments that collapse to $Y$:  \n",
    "   - $\\displaystyle P_{\\text{CTC}}(Y|X) = \\sum_{A \\in B^{-1}(Y)} \\prod_{t=1}^T p(a_t|h_t)$  \n",
    "   - This can be efficiently computed using [dynamic programming and a forward-backward algorithm](https://distill.pub/2017/ctc/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CTC and Encoder-Decoder\n",
    "- The encoder-decoder cross-entropy loss and CTC loss can be combined during training, weighted by a tunable parameter $Œª$:  \n",
    "   - $\\displaystyle L = -\\lambda \\log P_{\\text{enc-dec}}(Y|X) - (1 - \\lambda) \\log P_{\\text{CTC}}(Y|X)$  \n",
    "- ![Combining the CTC and encoder-decoder loss functions.](./images/stts/ctclm.png)\n",
    "- For inference, the combined losses are integrated with a language model (or length penalty), yielding the output sequence:  \n",
    "   - $\\displaystyle \\hat{Y} = \\arg\\max_Y [\\lambda \\log P_{\\text{enc-dec}}(Y|X) - (1 - \\lambda) \\log P_{\\text{CTC}}(Y|X) + \\gamma \\log P_{\\text{LM}}(Y)]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Models: RNN-T for improving CTC\n",
    "- **CTC Limitations and Streaming Advantage**: \n",
    "  - Due to its strong independence assumption, CTC models are less accurate than attention-based encoder-decoder models. \n",
    "  - However, CTC supports streaming, allowing word recognition as the user speaks, unlike attention models that require the entire input sequence to compute attention context.  \n",
    "\n",
    "- To overcome the conditional independence limitation of CTC and incorporate output history,\n",
    "  - the [RNN-Transducer (RNN-T)](https://lorenlugosch.github.io/posts/2020/11/transducer/) integrates a CTC acoustic model with a language model (predictor) that conditions on previous outputs.  \n",
    "- ![The RNN-T model computing the output token distribution at time t](./images/stts/rnnt.png)\n",
    "\n",
    "- At each time step $t$:  \n",
    "  - The CTC encoder computes a hidden state $h_t^{\\text{enc}}$ from the input sequence $x_1, \\dots, x_t$.  \n",
    "  - The predictor (language model) processes the output history $y_{<u_t}$ (excluding blanks) to produce $h_u^{\\text{pred}}$.  \n",
    "  - These hidden states are combined and passed through a softmax layer to predict the next character.    \n",
    "     - $\\displaystyle P_{\\text{RNN-T}}(Y|X) = \\sum_{A \\in B^{-1}(Y)} P(A|X) = \\sum_{A \\in B^{-1}(Y)} \\prod_{t=1}^T p(a_t | h_t, y_{<u_t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Evaluation: Word Error Rate\n",
    "- **Word Error Rate (WER) Definition** evaluates how the `hypothesized word string` from a speech recognizer differs from a `reference transcription`:  \n",
    "   - $\\text{Word Error Rate (WER)} = 100 \\times \\dfrac{\\text{Insertions} + \\text{Substitutions} + \\text{Deletions}}{\\text{Total Words in Correct Transcript}}$\n",
    "     - WER can exceed 100% due to insertions.\n",
    "\n",
    "- **WER Calculation Process**:  \n",
    "  - Compute the **minimum edit distance** (substitutions, insertions, deletions) between the hypothesized and reference strings.  \n",
    "  - üçé An alignment with 6 substitutions, 3 insertions, and 1 deletion out of 13 reference words results in a WER of $\\frac{6+3+1}{13} \\times 100 = 76.9\\%$.\n",
    "\n",
    "- **Sentence Error Rate** computes the percentage of sentences with at least one word error, providing an additional perspective on recognition performance.\n",
    "\n",
    "- **Evaluation Tool - [Score Lite (Sclite)](https://github.com/usnistgov/SCTK)**:  a script from NIST, automates WER computation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice [Hugging Face Audio course](https://huggingface.co/learn/audio-course)\n",
    "- [Unit 5. Automatic Speech Recognition](https://huggingface.co/learn/audio-course/chapter5/introduction)\n",
    "  - üìù 1. Pre-trained models for speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Probing CTC Models\n",
    "# such as Wav2Vec2, HuBERT and XLSR, they are small and fast\n",
    "# but prone to phonetic spelling errors. \n",
    "\n",
    "# 1.1 load a small excerpt of the LibriSpeech ASR dataset\n",
    "# to demonstrate Wav2Vec2‚Äôs speech transcription capabilities:\n",
    "# \n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 explore one of the 73 audio samples \n",
    "from IPython.display import Audio\n",
    "\n",
    "sample = dataset[2]\n",
    "\n",
    "print(sample[\"text\"])\n",
    "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 use the official Wav2Vec2 base checkpoint fine-tuned on 100 hours of LibriSpeech data:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-100h\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe a sample\n",
    "# find the wrong words due to the shortcoming of a CTC model.\n",
    "# prone to phonetic spelling errors \n",
    "# due it almost entirely bases its prediction on the acoustic input\n",
    "pipe(sample[\"audio\"].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Graduation to Seq2Seq\n",
    "# which support casing and punctuation\n",
    "# load the Whisper Base checkpoint\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the previous sample\n",
    "pipe(sample[\"audio\"], max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it on the Multilingual LibriSpeech (MLS) dataset\n",
    "dataset = load_dataset(\n",
    "    \"facebook/multilingual_librispeech\", \"spanish\", split=\"test\", streaming=True\n",
    ")\n",
    "sample = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the text transcription and take a listen to the audio segment:\n",
    "print(sample[\"transcript\"])\n",
    "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass a copy of the audio sample, so that we can re-use the same audio sample\n",
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\": \"transcribe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper can also do translation\n",
    "pipe(sample[\"audio\"], max_new_tokens=256, generate_kwargs={\"task\": \"translate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Long-Form Transcription and Timestamps\n",
    "# Whisper is inherently designed to work with 30 second samples\n",
    "# padding shorter and truncating longer\n",
    "\n",
    "# 3.1 concatenate audio sample to 5 minutes\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "target_length_in_m = 5\n",
    "\n",
    "# convert from minutes to seconds (* 60) to num samples (* sampling rate)\n",
    "sampling_rate = pipe.feature_extractor.sampling_rate\n",
    "target_length_in_samples = target_length_in_m * 60 * sampling_rate\n",
    "\n",
    "# iterate over our streaming dataset, concatenating samples until we hit our target\n",
    "long_audio = []\n",
    "for sample in dataset:\n",
    "    long_audio.extend(sample[\"audio\"][\"array\"])\n",
    "    if len(long_audio) > target_length_in_samples:\n",
    "        break\n",
    "\n",
    "long_audio = np.asarray(long_audio)\n",
    "\n",
    "# how did we do?\n",
    "seconds = len(long_audio) / 16000\n",
    "minutes, seconds = divmod(seconds, 60)\n",
    "print(f\"Length of audio sample is {minutes} minutes {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 transcribe the long audio sample with chunking and batching \n",
    "pipe(\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Predict segment-level timestamps for the audio data\n",
    "pipe(\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30,\n",
    "    batch_size=8,\n",
    "    return_timestamps=True,\n",
    ")[\"chunks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 2. [Choosing a dataset](https://huggingface.co/blog/audio-datasets#a-tour-of-audio-datasets-on-the-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 3. Evaluation and metrics for speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Word Error Rate\n",
    "\n",
    "reference = \"the cat sat on the mat\"\n",
    "prediction = \"the cat sit on the\"\n",
    "\n",
    "# WER = (S+I+D)/N = (1+0+1)/6=1/3\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
    "print(wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Word Accuracy\n",
    "# WAcc = 1 - WER\n",
    "\n",
    "# 3.3 Character Error Rate (CER)\n",
    "# For the example in 3.1\n",
    "# CER = (S+I+D)/N = (1+0+3)/14=2/7\n",
    "\n",
    "# the WER requires systems to have greater understanding of the context of the predictions\n",
    "# for word-based languages such as English\n",
    "# Where for character-based languages such as Chinese, CER is preferred\n",
    "# ‚ö†Ô∏è A Chinese character is equivalent to an English word\n",
    "# e.g. Êú® ‚àΩ treeÔºåÁü≥ ‚àΩ stone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"‰∏úÊñπÂá∫Áé∞‰∫ÜÁ¨¨‰∏ÄÁºïÊõôÂÖâ„ÄÇ\"\n",
    "prediction = \"‰∏úÊñπÂá∫Áé∞‰∫ÜÂºü‰∏ÄÊêÇÈò≥ÂÖâ\"\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "cer = cer_metric.compute(references=[reference], predictions=[prediction])\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 orthography and normalization\n",
    "# orthography - train with and predict casing and punctuation\n",
    "# normalization - remove any casing and punctuation\n",
    "# Wav2Vec2:  HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAUS AND ROSE BEEF LOOMING BEFORE US SIMALYIS DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\n",
    "# Whisper:   He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 the normaliser of Whisper\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "prediction = \" He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\"\n",
    "normalized_prediction = normalizer(prediction)\n",
    "\n",
    "normalized_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized WER is usually lower than orthographic WER\n",
    "# It is recommended training on orthographic text and \n",
    "#   evaluating on normalised text to get the best of both worlds.\n",
    "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
    "normalized_referece = normalizer(reference)\n",
    "\n",
    "wer = wer_metric.compute(\n",
    "    references=[normalized_referece], predictions=[normalized_prediction]\n",
    ")\n",
    "wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Putting it all together\n",
    "# pre-trained models, dataset selection and evaluation.\n",
    "\n",
    "# 1) Load whisper-small\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) login onto HuggingFace to download common voice dataset\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# or if from command line\n",
    "# huggingface-cli login\n",
    "# or \n",
    "# export HF_API_TOKEN=\"your_token_here\"\n",
    "\n",
    "# pip install soundfile librosa\n",
    "\n",
    "from datasets import load_dataset\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"zh-CN\", split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) pick out the needed dataset columns\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) compute the baseline CER without normalization\n",
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "cer_ortho = 100 * cer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "cer_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) compute the normalized CER\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "# compute normalised CER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]\n",
    "\n",
    "# filtering step to only evaluate the samples that correspond to non-zero references\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i]\n",
    "    for i in range(len(all_predictions_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i]\n",
    "    for i in range(len(all_references_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "\n",
    "cer = 100 * cer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "cer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 4. How to fine-tune an ASR system with the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linking the notebook to the Hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "# or in terminal\n",
    "# export HF_API_TOKEN=\"your_token_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"zh-CN\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"zh-CN\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select needed columns\n",
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Extractor, Tokenizer and Processor\n",
    "# the Whisper model has an associated feature extractor and tokenizer, \n",
    "# called WhisperFeatureExtractor and WhisperTokenizer respectively.\n",
    "# these two objects are wrapped under a single class, called the WhisperProcessor\n",
    "\n",
    "# 1) see all possible languages supported by Whisper\n",
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "\n",
    "TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) load our processor from the pre-trained checkpoint\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"chinese\", task=\"transcribe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Pre-Process the Data\n",
    "# Pay particular attention to the \"audio\" column\n",
    "\n",
    "common_voice[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample audio samples on-the-fly\n",
    "# change the sampling rate to 16kHz expected by the Whisper model.\n",
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. write a function to prepare our data ready for the model:\n",
    "# a. We load and resample the audio data on a sample-by-sample basis \n",
    "#   by calling sample[\"audio\"]. As explained above, ü§ó Datasets performs \n",
    "#   any necessary resampling operations on the fly.\n",
    "# b. We use the feature extractor to compute the log-mel spectrogram \n",
    "#   input features from our 1-dimensional audio array.\n",
    "# c. We encode the transcriptions to label ids through the use of the tokenizer.\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        text=example[\"sentence\"],\n",
    "    )\n",
    "\n",
    "    # compute input length of audio sample in seconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example\n",
    "\n",
    "# 4.2 apply the data preparation function to all of our training examples\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 filter any training data with audio samples longer than 30s\n",
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "\n",
    "# apply our filter function to all samples of our training dataset \n",
    "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much training data being removed\n",
    "common_voice[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training and Evaluation\n",
    "# 5.1 Define a data collator\n",
    "# perform both the feature extractor and the tokenizer operations:\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  initialise the data collator \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Evaluation metrics\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes our model predictions and returns the CER metric.\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # compute orthographic cer\n",
    "    cer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # compute normalised CER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    cer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"cer_ortho\": cer_ortho, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Load a pre-trained checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adust model parameters\n",
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"chinese\", task=\"transcribe\", use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Define the training arguments\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",  # save locally\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    # report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 forward the training arguments to the ü§ó Trainer along with \n",
    "# our model, dataset, data collator and compute_metrics function:\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model\n",
    "from transformers import pipeline\n",
    "# update with your model id\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"./\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build a demo with Gradio\n",
    "# 6.1 load the model\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"./\"  # update with your model id\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=model_id, device='cuda')\n",
    "\n",
    "# 6.2 define the serve fun\n",
    "def transcribe_speech(filepath):\n",
    "    output = pipe(\n",
    "        filepath,\n",
    "        max_new_tokens=256,\n",
    "        generate_kwargs={\n",
    "            \"task\": \"transcribe\",\n",
    "            \"language\": \"sinhalese\",\n",
    "        },  # update with the language you've fine-tuned on\n",
    "        chunk_length_s=30,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 use the Gradio blocks feature to launch two tabs on our demo:\n",
    "#  one for microphone transcription, and the other for file upload.\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")\n",
    "\n",
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"upload\", type=\"filepath\"),\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 launch the Gradio demo \n",
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe, file_transcribe],\n",
    "        [\"Transcribe Microphone\", \"Transcribe Audio File\"],\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS\n",
    "\n",
    "- `Text-to-speech (TTS)` systems convert text into audio waveforms, \n",
    "  - useful for dialogue systems, games, and education.  \n",
    "- Unlike automatic speech recognition (ASR) systems, TTS systems are typically `speaker-dependent`, \n",
    "  - requiring less data but `focusing on a single voice`, \n",
    "  - such as the 24-hour LJ (Linda Johnson) speech corpus.  \n",
    "- TTS involves \n",
    "  - (1) an `encoder-decoder` model to map text to mel spectrograms \n",
    "  - (2) a `vocoder` to convert mel spectrograms into waveforms.  \n",
    "- TTS algorithms are computationally intensive, driving research on optimization and acceleration.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS Preprocessing: Text normalization\n",
    "- TTS systems handle `non-standard words` differently based on context, \n",
    "  - requiring `verbalization (spoken form)` to match meaning, called `semiotic` classes\n",
    "  - e.g., numbers, dates, monetary amounts, abbreviations:\n",
    "    - \"1750\" as *seventeen fifty* for years \n",
    "    - but *one seven five zero* for passwords.\n",
    "    - $3.2 billion: three point two billion dollars\n",
    "    - N.Y.:  New York\n",
    "  - Grammatical properties in some languages further affect normalization rules.\n",
    "    - e.g., gender in French or case in German\n",
    "\n",
    "- **Common Semiotic Classes:** \n",
    "  - cardinal/ordinal numbers, dates, times, monetary values, percentages, abbreviations, and acronyms, \n",
    "  - each requiring specific `verbalization strategies`.  \n",
    "\n",
    "- **Two Normalization Methods:** \n",
    "  - `Rule-based systems` use tokenization and verbalization rules, like [Kestral](https://doi.org/10.1017/S1351324914000175), which classify and verbalize input, \n",
    "    - but are brittle and require maintenance.  \n",
    "  - `Encoder-Decoder Models` treat normalization as a translation task, mapping text to verbalized output, \n",
    "    - but require labeled training data for accurate performance. \n",
    "    - While effective, these models can produce erratic errors, \n",
    "      - such as misinterpreting \"45 minutes\" as \"forty-five meters.\"  \n",
    "    - `Lightweight covering grammars` may be used to constrain decoding and reduce normalization errors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS: Spectrogram prediction\n",
    "- [Tacotron2](https://huggingface.co/papers/1712.05884) is a TTS architecture that builds on [Tacotron](https://huggingface.co/papers/1703.10135) and [Wavenet](https://paperswithcode.com/paper/wavenet-a-generative-model-for-raw-audio), \n",
    "- ![The Tacotron2 architecture](./images/stts/taco.png)\n",
    "- It uses an `encoder-decoder` with attention to map graphemes to mel spectrograms, followed by a `vocoder` to generate waveforms.  \n",
    "  - The `encoder` processes input graphemes into hidden representations using 512-dimensional embeddings, \n",
    "    - convolutional layers for letter context, and a biLSTM for final encoding.  \n",
    "  - The `decoder` uses previous mel spectrum predictions, \n",
    "    - passes them through a pre-net, \n",
    "    - combines them with attention context, \n",
    "    - processes the result through LSTM layers to predict 80-dimensional log-mel filterbank vectors.\n",
    "      - and through another linear layer to a sigmoid to make a ‚Äústop token prediction‚Äù decision.\n",
    "- Tacotron2 uses `teacher forcing`, where the decoder is fed gold-standard mel features at each step, ensuring accurate training outputs.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS: Vocoding\n",
    "- The `vocoder`, based on WaveNet, converts log-mel spectrograms into time-domain waveforms,\n",
    "  - producing 8-bit mu-law compressed audio samples.  \n",
    "- `WaveNet` uses an autoregressive model with dilated convolutions, \n",
    "  - enabling predictions based only on past inputs while increasing the receptive field exponentially with depth.  \n",
    "  - ![Dilated convolutions](./images/stts/dicon.png)\n",
    "  - Tacotron2 employs 12 convolutional layers in two cycles, \n",
    "    - with dilation values of 1, 2, 4, 8, 16, and 32, \n",
    "    - to model long-range dependencies effectively.  \n",
    "\n",
    "- WaveNet predicts audio samples as 8-bit values using a 256-way categorical classifier, \n",
    "  - with outputs processed via softmax for each sample.  \n",
    "\n",
    "- The spectrogram predictor and vocoder are `trained separately`, \n",
    "  - with the vocoder trained using ground truth-aligned spectral features and audio output.  \n",
    "\n",
    "- **Challenges in WaveNet:** \n",
    "  - Predicting 8-bit values is less effective than 16-bit, \n",
    "  - requiring advanced decoding methods like mixtures of distributions for higher quality audio generation.  \n",
    "\n",
    "- **Efficiency Improvements:** \n",
    "  - Non-autoregressive generation techniques are explored to reduce latency, \n",
    "  - enabling faster and more practical audio synthesis.  \n",
    "\n",
    "- **Additional Features:** \n",
    "  - WaveNet integrates gated activation functions, residual, and skip connections, \n",
    "  - offering further enhancements to model performance and accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTS Evaluation\n",
    "- Speech synthesis systems are primarily `evaluated by human listeners`, \n",
    "  - since an effective automatic metric for evaluation remains an open research challenge.  \n",
    "- Listeners rate synthesized utterances with a `Mean Opinion Score (MOS)` on a scale (typically 1‚Äì5) to assess quality; \n",
    "  - often tested for significance using statistical methods like [paired t-tests](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/paired-sample-t-test/).  \n",
    "  - or [A/B tests](https://en.wikipedia.org/wiki/A/B_testing), where listeners choose their preferred version of the same sentence, with results aggregated across multiple samples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Speech Tasks\n",
    "- **Wake Word Detection:** Identifies specific words/phrases to activate voice assistants while ensuring privacy by minimizing transmitted speech; \n",
    "  - uses ASR-like feature extraction and operates on embedded devices for efficiency.  \n",
    "\n",
    "- **Speaker Diarization:** Segments audio to determine \"who spoke when,\" \n",
    "  - employing voice activity detection (VAD), speaker embeddings, and clustering; \n",
    "  - useful for meeting transcription and medical interactions, with recent advancements focusing on end-to-end approaches.  \n",
    "\n",
    "- **Speaker Recognition:** Identifies individuals through their voice, encompassing \n",
    "  - speaker verification: binary decision for authentication,\n",
    "  - speaker identification: matching against a database.  \n",
    "\n",
    "- **Language Identification:** Determines the spoken language in audio files, \n",
    "  - aiding in tasks like routing callers to language-specific operators or services.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ [Unit 6. From text to speech](https://huggingface.co/learn/audio-course/chapter6/introduction)\n",
    "- üìù [Text-to-speech datasets](https://huggingface.co/learn/audio-course/chapter6/tts_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù [Pre-trained models for text-to-speech](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SpeechT5\n",
    "# 1.1 load the processor and model\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "# 1.2 tokenize the input texgt\n",
    "inputs = processor(text=\"Don't count the days, make the days count.\", return_tensors=\"pt\")\n",
    "\n",
    "# 1.3 load X-vector of speaker embeddings\n",
    "from datasets import load_dataset\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "\n",
    "import torch\n",
    "\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 generate a log mel spectrogram\n",
    "spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 convert spectrogram to speech wave with a vocoder\n",
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "# play the speech\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(speech, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Bark\n",
    "# 2.1 load the model and its processor.\n",
    "from transformers import BarkModel, BarkProcessor\n",
    "\n",
    "model = BarkModel.from_pretrained(\"suno/bark-small\")\n",
    "sampling_rate = model.generation_config.sample_rate\n",
    "\n",
    "processor = BarkProcessor.from_pretrained(\"suno/bark-small\")\n",
    "\n",
    "# add a speaker embedding\n",
    "inputs = processor(\"This is a test!\", voice_preset=\"v2/en_speaker_3\")\n",
    "speech_output = model.generate(**inputs).cpu().numpy()\n",
    "\n",
    "Audio(speech_output, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ready-to-use multilingual speeches\n",
    "# try it in Chinese, let's also add a Chinese speaker embedding\n",
    "inputs = processor(\"ÁúüÊòØÂ§™Áæé‰∫Ü!\", voice_preset=\"v2/zh_speaker_1\")\n",
    "\n",
    "speech_output = model.generate(**inputs).cpu().numpy()\n",
    "Audio(speech_output, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate non-verbal communications such as laughing, sighing and crying. \n",
    "inputs = processor(\n",
    "    \"[clears throat] This is a test ... and I just took a long pause.\",\n",
    "    voice_preset=\"v2/zh_speaker_1\",\n",
    ")\n",
    "\n",
    "speech_output = model.generate(**inputs).cpu().numpy()\n",
    "Audio(speech_output, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate music by adding ‚ô™ musical notes ‚ô™ around your words.\n",
    "inputs = processor(\n",
    "    \"‚ô™ In the mighty jungle, I'm trying to generate barks.\",\n",
    ")\n",
    "\n",
    "speech_output = model.generate(**inputs).cpu().numpy()\n",
    "Audio(speech_output, rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bark supports batch processing,\n",
    "input_list = [\n",
    "    \"[clears throat] Hello uh ..., my dog is cute [laughter]\",\n",
    "    \"Let's try generating speech, with Bark, a text-to-speech model\",\n",
    "    \"‚ô™ In the jungle, the mighty jungle, the lion barks tonight ‚ô™\",\n",
    "]\n",
    "\n",
    "# also add a speaker embedding\n",
    "inputs = processor(input_list, voice_preset=\"v2/en_speaker_3\")\n",
    "\n",
    "speech_output = model.generate(**inputs).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listen to the outputs one by one.\n",
    "from IPython.display import Audio\n",
    "\n",
    "# first one\n",
    "sampling_rate = model.generation_config.sample_rate\n",
    "Audio(speech_output[0], rate=sampling_rate)\n",
    "\n",
    "# second\n",
    "Audio(speech_output[1], rate=sampling_rate)\n",
    "\n",
    "# third\n",
    "Audio(speech_output[2], rate=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Massive Multilingual Speech (MMS)\n",
    "# it can synthesize speech in over 1,100 languages.\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "\n",
    "# 3.1 load the model and tokenizer\n",
    "from transformers import VitsModel, VitsTokenizer\n",
    "\n",
    "model = VitsModel.from_pretrained(\"facebook/mms-tts-deu\")\n",
    "tokenizer = VitsTokenizer.from_pretrained(\"facebook/mms-tts-deu\")\n",
    "\n",
    "text_example = (\n",
    "    \"Ich bin Schnappi das kleine Krokodil, komm aus √Ñgypten das liegt direkt am Nil.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 generate waveform output\n",
    "import torch\n",
    "\n",
    "inputs = tokenizer(text_example, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "speech = outputs[\"waveform\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(speech, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù [Fine-tuning SpeechT5](https://huggingface.co/learn/audio-course/chapter6/fine-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part is very time consuming, GPU is required\n",
    "!nvidia-smi\n",
    "\n",
    "# install required libraries\n",
    "!pip install transformers datasets soundfile speechbrain accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load the dataset\n",
    "#  VoxPopuli is a large-scale multilingual speech corpus consisting of data \n",
    "# sourced from 2009-2020 European Parliament event recordings. \n",
    "# It contains labelled audio-transcription data for 15 European languages. \n",
    "# we will be using the Dutch language subset,\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"train\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpeechT5 expects audio data to have a sampling rate of 16 kHz\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing the data\n",
    "# 2.1 loading the appropriate processor that contains both tokenizer and feature extractor\n",
    "from transformers import SpeechT5Processor\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 get the tokenizer\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# investigate a sample\n",
    "# the SpeechT5 tokenizer doesn‚Äôt have any tokens for numbers\n",
    "# `normalized_text` since it write out the numbers as text.\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Extract all characters\n",
    "# SpeechT5 was trained on the English language, \n",
    "# it may not recognize certain characters in the Dutch dataset\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"normalized_text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters specific to Dutch\n",
    "dataset_vocab - tokenizer_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 define a function that maps these characters to valid tokens\n",
    "replacements = [\n",
    "    (\"√†\", \"a\"),\n",
    "    (\"√ß\", \"c\"),\n",
    "    (\"√®\", \"e\"),\n",
    "    (\"√´\", \"e\"),\n",
    "    (\"√≠\", \"i\"),\n",
    "    (\"√Ø\", \"i\"),\n",
    "    (\"√∂\", \"o\"),\n",
    "    (\"√º\", \"u\"),\n",
    "]\n",
    "\n",
    "\n",
    "def cleanup_text(inputs):\n",
    "    for src, dst in replacements:\n",
    "        inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(cleanup_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Speakers\n",
    "# The VoxPopuli dataset includes speech from multiple speakers\n",
    "# 3.1 count the number of unique speakers \n",
    "# and the number of examples each speaker contributes to the dataset.\n",
    "from collections import defaultdict\n",
    "\n",
    "speaker_counts = defaultdict(int)\n",
    "\n",
    "for speaker_id in dataset[\"speaker_id\"]:\n",
    "    speaker_counts[speaker_id] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of speakers, # speakers less than 100 samples, # speakers no less than 500 samples\n",
    "len(speaker_counts), len([i for i in speaker_counts if speaker_counts[i]<100]), len([i for i in speaker_counts if speaker_counts[i]>=500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the distribution of speakers and examples in the data.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(speaker_counts.values(), bins=20)\n",
    "plt.ylabel(\"Speakers\")\n",
    "plt.xlabel(\"Examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve training efficiency and balance the dataset, \n",
    "# we can limit the data to speakers with between 100 and 400 examples.\n",
    "def select_speaker(speaker_id):\n",
    "    return 100 <= speaker_counts[speaker_id] <= 400\n",
    "\n",
    "\n",
    "dataset = dataset.filter(select_speaker, input_columns=[\"speaker_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speakers remained, samples left\n",
    "len(set(dataset[\"speaker_id\"])), len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Speaker embeddings\n",
    "# To enable the TTS model to differentiate between multiple speakers, \n",
    "# you‚Äôll need to create a speaker embedding for each example. \n",
    "# e.g. use the pre-trained spkrec-xvect-voxceleb model from SpeechBrain.\n",
    "# For optimal results, train an X-vector model on the target speech.\n",
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Processing the dataset\n",
    "# process the data into the format the model expects\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        text=example[\"normalized_text\"],\n",
    "        audio_target=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the processing on a single example\n",
    "processed_example = prepare_dataset(dataset[0])\n",
    "list(processed_example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speaker embeddings should be a 512-element vector:\n",
    "processed_example[\"speaker_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels should be a log-mel spectrogram with 80 mel bins.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(processed_example[\"labels\"].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 apply the processing function to the entire dataset.\n",
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 remove example longer than the maximum input length the model can handle (600 tokens)\n",
    "# we remove anything over 200 tokens here\n",
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "\n",
    "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a basic train/test split:\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Data collator\n",
    "# combine multiple examples into a batch with a custom data collator.\n",
    "# This collator will pad shorter sequences with padding tokens\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(\n",
    "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
    "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
    "        )\n",
    "\n",
    "        # not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor(\n",
    "                [len(feature[\"input_values\"]) for feature in label_features]\n",
    "            )\n",
    "            target_lengths = target_lengths.new(\n",
    "                [\n",
    "                    length - length % model.config.reduction_factor\n",
    "                    for length in target_lengths\n",
    "                ]\n",
    "            )\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a data collator\n",
    "data_collator = TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train the model\n",
    "# 7.1 load the pretrained model freshly for fine-tuning\n",
    "from transformers import SpeechT5ForTextToSpeech\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(model.generate, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Define the training arguments.\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    # report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Instantiate the Trainer object and pass the model, dataset, and data collator to it.\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 launch the training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.5 Inference\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\n",
    "    \"./\" # find and fill with your local checkpoint\n",
    ")\n",
    "\n",
    "# pick an example\n",
    "example = dataset[\"test\"][304]\n",
    "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n",
    "\n",
    "# Define some input text and tokenize it.\n",
    "text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n",
    "\n",
    "# Preprocess the input text:\n",
    "inputs = processor(text=text, return_tensors=\"pt\")\n",
    "\n",
    "# Instantiate a vocoder and generate speech:\n",
    "from transformers import SpeechT5HifiGan\n",
    "\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "\n",
    "# listen to the result\n",
    "from IPython.display import Audio\n",
    "Audio(speech.numpy(), rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù [Evaluating text-to-speech models](https://huggingface.co/learn/audio-course/chapter6/evaluation)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
