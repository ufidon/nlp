{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/srs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/srs.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Automatic Speech Recognition and Text-to-Speech\n",
    "\n",
    "üìù SALP chapter 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- An early know `automatic speech recognition (ASR)` example is [Radio Rex](https://en.wikipedia.org/wiki/Virtual_assistant) appeared in the 1920s, \n",
    "  - a toy responding to specific sounds like the `vowel [eh]` in \"Rex.\"  \n",
    "- Despite limitations in diverse environments, **modern ASR** `converts speech waveforms into text` and is widely used in\n",
    "  - smart appliances, personal assistants, call routing, transcription, and assisting individuals with disabilities.  \n",
    "- [Wolfgang von Kempelen‚Äôs late 18th-century speech synthesizer](https://en.wikipedia.org/wiki/Wolfgang_von_Kempelen%27s_speaking_machine) marked the first `text-to-speech (TTS)` system using mechanical components.  \n",
    "  - **Modern TTS** maps `text to audio waveforms`, aiding communication for visually impaired users and individuals with neurological disorders.  \n",
    "- ASR and TTS share core algorithmic principles:\n",
    "  - encoder-decoder models, \n",
    "  - Connectionist Temporal Classification (CTC) loss functions, \n",
    "  - word error rate evaluation, \n",
    "  - and acoustic feature extraction.  \n",
    "\n",
    "üî≠ Explore \n",
    "- [ASR models](https://www.gladia.io/blog/best-open-source-speech-to-text-models)\n",
    "- [TTS models](https://www.bentoml.com/blog/exploring-the-world-of-open-source-text-to-speech-models)\n",
    "\n",
    "üèÉ Play\n",
    "- [Whisper: a general-purpose speech recognition model](https://github.com/openai/whisper)\n",
    "- [OpenVoice: versatile instant voice cloning](https://github.com/myshell-ai/OpenVoice)\n",
    "- [UltraVox: a fast multimodal LLM for real-time voice](https://github.com/fixie-ai/ultravox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Automatic Speech Recognition Task\n",
    "- **Dimensions of Variation in ASR:**\n",
    "  - Vocabulary size:\n",
    "    - Small vocabularies (e.g., yes/no, digits) are highly accurate.\n",
    "    - Large vocabularies (up to 60,000 words) in open-ended tasks are more difficult.\n",
    "  - Speaker and context:\n",
    "    - Human-to-machine speech (dictation/dialogue systems) is easier.\n",
    "    - Read speech (e.g., audiobooks) is relatively easy.\n",
    "    - Conversational speech between humans is the hardest due to faster, less clear speech.\n",
    "  - Channel and noise:\n",
    "    - Quiet environments with head-mounted microphones are ideal.\n",
    "    - Noisy settings (e.g., streets, open car windows) complicate recognition.\n",
    "  - Accent and speaker characteristics:\n",
    "    - Recognition is better for accents/dialects similar to the training data.\n",
    "    - Regional/ethnic dialects and children's speech are more challenging.\n",
    "\n",
    "- **Key ASR Corpora:**\n",
    "  - [LibriSpeech](https://huggingface.co/datasets/facebook/multilingual_librispeech):\n",
    "    - Open-source, read-speech dataset with 1,000+ hours of audiobooks.\n",
    "    - Divided into \"clean\" (high quality) and \"other\" (lower quality) portions.\n",
    "  - [Switchboard Corpus](https://huggingface.co/datasets/cgpotts/swda):\n",
    "    - Prompted telephone conversations between strangers (~240 hours).\n",
    "    - Extensive linguistic labeling (e.g., dialogue acts, prosody).\n",
    "  - [CALLHOME Corpus](https://huggingface.co/datasets/talkbank/callhome):\n",
    "    - Unscripted 30-minute telephone conversations (friends/family).\n",
    "    - Focus on natural, casual speech.\n",
    "  - [Santa Barbara Corpus](https://www.linguistics.ucsb.edu/research/santa-barbara-corpus):\n",
    "    - Everyday spoken interactions across the US (e.g., conversations, town halls).\n",
    "    - Anonymized transcripts.\n",
    "  - [CORAAL](https://huggingface.co/datasets/Padomin/coraal-asr):\n",
    "    - Sociolinguistic interviews with African American speakers.\n",
    "    - Focus on African American Language (AAL).\n",
    "  - [CHiME Challenge](https://www.chimechallenge.org/):\n",
    "    - Datasets for robust ASR tasks in noisy, real environments (e.g., dinner parties).\n",
    "  - [AISHELL-1 Corpus](https://paperswithcode.com/dataset/aishell-1):\n",
    "    - 170 hours of Mandarin read speech from various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for ASR: Log Mel Spectrum\n",
    "- ASR converts waveforms into [log mel spectrum](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53) feature vectors.\n",
    "  - representing information from small time windows of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and Quantization\n",
    "- Speech recognizers process air pressure changes caused by vocalizations, visualized as waveforms showing air pressure over time. \n",
    "  - ![A waveform of an instance of the vowel iy](./images/stts/wave.png)\n",
    "- Analog sound waves are digitized through \n",
    "  - **sampling**: measuring wave amplitude at intervals,\n",
    "  - **quantization**: converting measurements into integers.  \n",
    "- **Nyquist frequency** defines the maximum measurable frequency as half the sampling rate; \n",
    "  - 8 kHz is sufficient for telephone speech, \n",
    "  - while 16 kHz is used for microphone speech.  \n",
    "- Different sampling rates (e.g., 8 kHz vs. 16 kHz) cannot be mixed in ASR training/testing, \n",
    "  - requiring downsampling for consistency.  \n",
    "- Quantization stores amplitudes as integers (e.g., 8-bit or 16-bit), \n",
    "  - with log compression (e.g., ¬µ-law) optimizing for human auditory sensitivity to smaller intensities.  \n",
    "- Audio files vary by sample rate, sample size, number of channels (mono/stereo), and storage type (linear vs. compressed).  \n",
    "- Common file formats include \n",
    "  - `.wav`, a subset of Microsoft [Resource Interchange File Format (RIFF)](https://en.wikipedia.org/wiki/Resource_Interchange_File_Format-based), \n",
    "  - Apple‚Äôs [Audio Interchange File Format (AIFF)](https://en.wikipedia.org/wiki/Audio_Interchange_File_Format), \n",
    "  - and [raw headerless formats](https://en.wikipedia.org/wiki/Raw_audio_format).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "- `Spectral features` are extracted from small windows of speech, \n",
    "  - treating the signal as stationary within each window, \n",
    "  - despite speech being non-stationary overall.  \n",
    "- A **frame** represents the speech within each window, determined by three parameters:\n",
    "  -  window size (duration in ms), \n",
    "  -  frame stride (overlap/offset between windows), \n",
    "  -  and window shape.  \n",
    "- `Windowing` is performed by multiplying the signal $s(n)$ at time $n$ by the window function $w(n)$ at $n$, producing a windowed waveform $y[n]$.\n",
    "  - $y[n]=w[n]s[n]$\n",
    "- Common window shapes include \n",
    "  - **rectangular**: simple but causes boundary issues in Fourier analysis\n",
    "  - **Hamming**: smoothes boundary discontinuities for better feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
