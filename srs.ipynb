{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/srs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/srs.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Automatic Speech Recognition and Text-to-Speech\n",
    "\n",
    "üìù SALP chapter 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- An early know `automatic speech recognition (ASR)` example is [Radio Rex](https://en.wikipedia.org/wiki/Virtual_assistant) appeared in the 1920s, \n",
    "  - a toy responding to specific sounds like the `vowel [eh]` in \"Rex.\"  \n",
    "- Despite limitations in diverse environments, **modern ASR** `converts speech waveforms into text` and is widely used in\n",
    "  - smart appliances, personal assistants, call routing, transcription, and assisting individuals with disabilities.  \n",
    "- [Wolfgang von Kempelen‚Äôs late 18th-century speech synthesizer](https://en.wikipedia.org/wiki/Wolfgang_von_Kempelen%27s_speaking_machine) marked the first `text-to-speech (TTS)` system using mechanical components.  \n",
    "  - **Modern TTS** maps `text to audio waveforms`, aiding communication for visually impaired users and individuals with neurological disorders.  \n",
    "- ASR and TTS share core algorithmic principles:\n",
    "  - encoder-decoder models, \n",
    "  - Connectionist Temporal Classification (CTC) loss functions, \n",
    "  - word error rate evaluation, \n",
    "  - and acoustic feature extraction.  \n",
    "\n",
    "üî≠ Explore \n",
    "- [ASR models](https://www.gladia.io/blog/best-open-source-speech-to-text-models)\n",
    "- [TTS models](https://www.bentoml.com/blog/exploring-the-world-of-open-source-text-to-speech-models)\n",
    "\n",
    "üèÉ Play\n",
    "- [Whisper: a general-purpose speech recognition model](https://github.com/openai/whisper)\n",
    "- [OpenVoice: versatile instant voice cloning](https://github.com/myshell-ai/OpenVoice)\n",
    "- [UltraVox: a fast multimodal LLM for real-time voice](https://github.com/fixie-ai/ultravox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Automatic Speech Recognition Task\n",
    "- **Dimensions of Variation in ASR:**\n",
    "  - Vocabulary size:\n",
    "    - Small vocabularies (e.g., yes/no, digits) are highly accurate.\n",
    "    - Large vocabularies (up to 60,000 words) in open-ended tasks are more difficult.\n",
    "  - Speaker and context:\n",
    "    - Human-to-machine speech (dictation/dialogue systems) is easier.\n",
    "    - Read speech (e.g., audiobooks) is relatively easy.\n",
    "    - Conversational speech between humans is the hardest due to faster, less clear speech.\n",
    "  - Channel and noise:\n",
    "    - Quiet environments with head-mounted microphones are ideal.\n",
    "    - Noisy settings (e.g., streets, open car windows) complicate recognition.\n",
    "  - Accent and speaker characteristics:\n",
    "    - Recognition is better for accents/dialects similar to the training data.\n",
    "    - Regional/ethnic dialects and children's speech are more challenging.\n",
    "\n",
    "- **Key ASR Corpora:**\n",
    "  - [LibriSpeech](https://huggingface.co/datasets/facebook/multilingual_librispeech):\n",
    "    - Open-source, read-speech dataset with 1,000+ hours of audiobooks.\n",
    "    - Divided into \"clean\" (high quality) and \"other\" (lower quality) portions.\n",
    "  - [Switchboard Corpus](https://huggingface.co/datasets/cgpotts/swda):\n",
    "    - Prompted telephone conversations between strangers (~240 hours).\n",
    "    - Extensive linguistic labeling (e.g., dialogue acts, prosody).\n",
    "  - [CALLHOME Corpus](https://huggingface.co/datasets/talkbank/callhome):\n",
    "    - Unscripted 30-minute telephone conversations (friends/family).\n",
    "    - Focus on natural, casual speech.\n",
    "  - [Santa Barbara Corpus](https://www.linguistics.ucsb.edu/research/santa-barbara-corpus):\n",
    "    - Everyday spoken interactions across the US (e.g., conversations, town halls).\n",
    "    - Anonymized transcripts.\n",
    "  - [CORAAL](https://huggingface.co/datasets/Padomin/coraal-asr):\n",
    "    - Sociolinguistic interviews with African American speakers.\n",
    "    - Focus on African American Language (AAL).\n",
    "  - [CHiME Challenge](https://www.chimechallenge.org/):\n",
    "    - Datasets for robust ASR tasks in noisy, real environments (e.g., dinner parties).\n",
    "  - [AISHELL-1 Corpus](https://paperswithcode.com/dataset/aishell-1):\n",
    "    - 170 hours of Mandarin read speech from various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice [Hugging Face Audio course](https://huggingface.co/learn/audio-course)\n",
    "- üìñ [Unit 2. A gentle introduction to audio applications](https://huggingface.co/learn/audio-course/chapter2/introduction)\n",
    "  - üìù 1. Investigate audio applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Audio classification with a pipeline\n",
    "# 1.1 Load the dataset minds14\n",
    "# It contains recordings of people asking an e-banking system questions \n",
    "#   in several languages and dialects\n",
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# 1.2 Create a classifier\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"anton-l/xtreme_s_xlsr_300m_minds14\",\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste an example\n",
    "example = minds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifying\n",
    "classifier(example[\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the actual label for this example is:\n",
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "id2label(example[\"intent_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Automatic speech recognition with a pipeline\n",
    "# using the same MINDS-14 dataset as before\n",
    "\n",
    "# transcribe an audio recording using the automatic-speech-recognition pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try an example\n",
    "example = minds[0]\n",
    "asr(example[\"audio\"][\"array\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare it to the actual transcription\n",
    "example[\"english_transcription\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Try it on Chinese\n",
    "from datasets import load_dataset\n",
    "from datasets import Audio\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"zh-CN\", split=\"train\")\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste an example\n",
    "example = minds[0]\n",
    "example[\"transcription\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a pre-trained ASR model for Chinese language on the ü§ó Hub\n",
    "from transformers import pipeline\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", model=\"jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn\", device='cuda')\n",
    "asr(example[\"audio\"][\"array\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Audio generation with a pipeline\n",
    "# upgrade transformers\n",
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Generating speech\n",
    "from transformers import pipeline\n",
    "\n",
    "# https://huggingface.co/suno/bark-small\n",
    "pipe = pipeline(\"text-to-speech\", model=\"suno/bark-small\", device='cuda')\n",
    "\n",
    "# try a text\n",
    "text = \"Ladybugs have had important roles in culture and religion, being associated with luck, love, fertility and prophecy. \"\n",
    "output = pipe(text)\n",
    "\n",
    "# play the speech\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(output[\"audio\"], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different language\n",
    "zh_text = \"ÊúàËêΩ‰πåÂïºÈúúÊª°Â§©ÔºåÊ±üÊû´Ê∏îÁÅ´ÂØπÊÑÅÁú†„ÄÇ ÂßëËãèÂüéÂ§ñÂØíÂ±±ÂØ∫ÔºåÂ§úÂçäÈíüÂ£∞Âà∞ÂÆ¢Ëàπ„ÄÇ\"\n",
    "output = pipe(zh_text)\n",
    "Audio(output[\"audio\"], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate audio with non-verbal communications and singing.\n",
    "song = \"‚ô™ In the jungle, the mighty jungle, the ladybug was seen. ‚ô™ \"\n",
    "output = pipe(song)\n",
    "Audio(output[\"audio\"], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Generating music\n",
    "# https://huggingface.co/facebook/musicgen-small\n",
    "music_pipe = pipeline(\"text-to-audio\", model=\"facebook/musicgen-small\", device='cuda')\n",
    "\n",
    "prompt = \"90s rock song with electric guitar and heavy drums\"\n",
    "\n",
    "# generate the music\n",
    "forward_params = {\"max_new_tokens\": 512}\n",
    "\n",
    "output = music_pipe(prompt, forward_params=forward_params)\n",
    "Audio(output[\"audio\"][0], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for ASR: Log Mel Spectrum\n",
    "- ASR converts waveforms into [log mel spectrum](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53) feature vectors.\n",
    "  - representing information from small time windows of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling and Quantization\n",
    "- Speech recognizers process air pressure changes caused by vocalizations, visualized as waveforms showing air pressure over time. \n",
    "  - ![A waveform of an instance of the vowel iy](./images/stts/wave.png)\n",
    "    - A waveform of an instance of the vowel `i` in `Ààbe…™bi`\n",
    "- Analog sound waves are digitized through \n",
    "  - **sampling**: measuring wave amplitude at intervals,\n",
    "  - **quantization**: converting measurements into integers.  \n",
    "- **Nyquist frequency** defines the maximum measurable frequency as half the sampling rate; \n",
    "  - 8 kHz is sufficient for telephone speech, \n",
    "  - while 16 kHz is used for microphone speech.  \n",
    "- Different sampling rates (e.g., 8 kHz vs. 16 kHz) cannot be mixed in ASR training/testing, \n",
    "  - requiring downsampling for consistency.  \n",
    "- Quantization stores amplitudes as integers (e.g., 8-bit or 16-bit), \n",
    "  - with log compression (e.g., ¬µ-law) optimizing for human auditory sensitivity to smaller intensities.  \n",
    "- Audio files vary by sample rate, sample size, number of channels (mono/stereo), and storage type (linear vs. compressed).  \n",
    "- Common file formats include \n",
    "  - [.wav](https://ccrma.stanford.edu/courses/422-winter-2014/projects/WaveFormat/), a subset of Microsoft [Resource Interchange File Format (RIFF)](https://en.wikipedia.org/wiki/Resource_Interchange_File_Format-based), \n",
    "  - Apple‚Äôs [Audio Interchange File Format (AIFF)](https://en.wikipedia.org/wiki/Audio_Interchange_File_Format), \n",
    "  - and [raw headerless formats](https://en.wikipedia.org/wiki/Raw_audio_format).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowing\n",
    "- `Spectral features` are extracted from small windows of speech, \n",
    "  - treating the signal as stationary within each window, \n",
    "  - despite speech being non-stationary overall.  \n",
    "- A **frame** represents the speech within each window, determined by three parameters:\n",
    "  - window size, or fame size with width in ms, \n",
    "  - frame stride, offset, or shift between successive windows, \n",
    "  - and window shape.\n",
    "  - ![Windowing, showing a 25 ms rectangular window with a 10ms stride](./images/stts/win.png)  \n",
    "- `Windowing` is performed by multiplying the signal $s(n)$ at time $n$ by the window function $w(n)$ at $n$, producing a windowed waveform $y[n]$.\n",
    "  - $y[n]=w[n]s[n]$\n",
    "- Common window shapes include \n",
    "  - **rectangular**: simple but causes boundary issues in Fourier analysis\n",
    "  - **Hamming**: smoothes boundary discontinuities for better feature extraction.\n",
    "- ![Windowing a sine wave with the rectangular or Hamming windows](./images/stts/winshape.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Fourier Transform\n",
    "- The **Discrete Fourier Transform (DFT)** is used to extract spectral information from a windowed discrete-time signal $x[n]$. \n",
    "  - The output $X[k]$ represents the magnitude and phase of $N$ discrete frequency components, enabling visualization of the signal spectrum.\n",
    "- ![dft](./images/stts/dft.png)\n",
    "  - Left: A 25 ms Hamming-windowed portion of a signal from the vowel `i` in `Ààbe…™bi`\n",
    "  - Right: its spectrum computed by a DFT.\n",
    "\n",
    "- The **Fast Fourier Transform (FFT)** is an efficient algorithm for computing the DFT, optimized for $N$ values that are powers of 2.\n",
    "  - $X[k] = \\sum_{n=0}^{N-1} x[n]e^{-j\\frac{2\\pi kn}{N}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel Filter Bank and Log\n",
    "- The **FFT results** provide the energy at each frequency band, \n",
    "  - but human hearing is biased toward low frequencies, \n",
    "    - aiding recognition of critical low-frequency features (e.g., vowels or nasals) over high-frequency features (e.g., fricatives). \n",
    "  - Incorporating this bias enhances speech recognition.\n",
    "\n",
    "- The **mel scale**, an auditory frequency scale, models human perception of pitch. \n",
    "  - It spaces sounds perceptually equidistant in pitch,\n",
    "  - with the mel frequency $m$ calculated from the raw frequency $f$ as:  \n",
    "    - $\\text{mel}(f) = 1127 \\ln(1 + \\dfrac{f}{700})$\n",
    "\n",
    "- A **mel filter bank**, composed of logarithmically spaced triangular filters, \n",
    "  - collects energy with fine resolution at low frequencies and coarse resolution at high frequencies. \n",
    "  - This approach creates a **mel spectrum**, representing the perceptual energy distribution.\n",
    "- ![The mel filter bank](./images/stts/melbank.png)\n",
    "\n",
    "- Applying a **logarithmic transformation** to the mel spectrum values mirrors the human logarithmic response to signal levels. \n",
    "  - This reduces sensitivity to variations in input power, such as changes in the speaker's distance from the microphone, stabilizing feature estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice [Hugging Face Audio course](https://huggingface.co/learn/audio-course)\n",
    "- [Unit 1. Working with audio data](https://huggingface.co/learn/audio-course/chapter1/introduction)\n",
    "  - üìù 1. Visualize audio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. install the librosa used to plot the waveform for an audio signal\n",
    "!pip install librosa\n",
    "\n",
    "# 2. load audio data\n",
    "import librosa\n",
    "\n",
    "array, sampling_rate = librosa.load(librosa.ex(\"trumpet\"))\n",
    "\n",
    "# 3. plot audio waveform\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. plot frequency spectrum, i.e. frequency domain representation\n",
    "# power spectrum, which measures energy rather than amplitude; \n",
    "#   this is simply a spectrum with the amplitude values squared.\n",
    "import numpy as np\n",
    "\n",
    "dft_input = array[:4096]\n",
    "\n",
    "# calculate the DFT\n",
    "window = np.hanning(len(dft_input))\n",
    "windowed_input = dft_input * window\n",
    "dft = np.fft.rfft(windowed_input)\n",
    "\n",
    "# get the amplitude spectrum in decibels\n",
    "amplitude = np.abs(dft)\n",
    "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "# get the frequency bins\n",
    "frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "plt.plot(frequency, amplitude_db)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Amplitude (dB)\")\n",
    "plt.xscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot audio spetrogram\n",
    "# A spectrogram plots the frequency content of an audio signal as it changes over time. \n",
    "# It allows you to see time, frequency, and amplitude all on one graph. \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "D = librosa.stft(array)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Plot Mel spectrogram\n",
    "#  it shows the frequency content of an audio signal over time, \n",
    "# but on a different frequency axis.\n",
    "S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sampling_rate, fmax=8000)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 2. Load and explore an audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install the Datasets library\n",
    "!pip install datasets[audio]\n",
    "\n",
    "# 2. load and explore and audio dataset called MINDS-14\n",
    "# https://huggingface.co/datasets/PolyAI/minds14\n",
    "# It contains recordings of people asking an e-banking system questions \n",
    "# in several languages and dialects.\n",
    "from datasets import load_dataset\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taste an example\n",
    "example = minds[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intent_class is a classification category of the audio recording.\n",
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "id2label(example[\"intent_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. remove unused features\n",
    "columns_to_remove = [\"lang_id\", \"english_transcription\"]\n",
    "minds = minds.remove_columns(columns_to_remove)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. listen to a few examples\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_audio():\n",
    "    example = minds.shuffle()[0]\n",
    "    audio = example[\"audio\"]\n",
    "    return (\n",
    "        audio[\"sampling_rate\"],\n",
    "        audio[\"array\"],\n",
    "    ), id2label(example[\"intent_class\"])\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column():\n",
    "        for _ in range(4):\n",
    "            audio, label = generate_audio()\n",
    "            output = gr.Audio(audio, label=label)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. visualize some examples\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "array = example[\"audio\"][\"array\"]\n",
    "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 3. Preprocessing an audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Resampling the audio data to the model‚Äôs expected sampling rate.\n",
    "from datasets import Audio\n",
    "\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# check it is resampled to the desired sampling rate:\n",
    "minds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Filtering the dataset\n",
    "# e.g. limiting the audio examples to a certain duration\n",
    "MAX_DURATION_IN_SECONDS = 20.0\n",
    "\n",
    "\n",
    "def is_audio_length_in_range(input_length):\n",
    "    return input_length < MAX_DURATION_IN_SECONDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the filter\n",
    "# use librosa to get example's duration from the audio file\n",
    "new_column = [librosa.get_duration(path=x) for x in minds[\"path\"]]\n",
    "minds = minds.add_column(\"duration\", new_column)\n",
    "\n",
    "# use ü§ó Datasets' `filter` method to apply the filtering function\n",
    "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
    "\n",
    "# remove the temporary helper column\n",
    "minds = minds.remove_columns([\"duration\"])\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pre-processing audio data\n",
    "# preparing the data in the right format for model training.\n",
    "# convert the raw data into input features.\n",
    "# e.g. Whisper feature extractor: https://huggingface.co/papers/2212.04356\n",
    "\n",
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# pre-process a single audio example by passing it through the feature_extractor.\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    features = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=True\n",
    "    )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the data preparation function to all of our training examples\n",
    "minds = minds.map(prepare_dataset)\n",
    "minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have log-mel spectrograms as input_features in the dataset.\n",
    "# Let‚Äôs visualize it for one of the examples in the minds dataset:\n",
    "import numpy as np\n",
    "\n",
    "example = minds[0]\n",
    "input_features = example[\"input_features\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(\n",
    "    np.asarray(input_features[0]),\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    "    sr=feature_extractor.sampling_rate,\n",
    "    hop_length=feature_extractor.hop_length,\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. for simplicity, load the feature extractor and tokenizer for Whisper \n",
    "# via the so-called processor.\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 4. Streaming large audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. enable streaming mod\n",
    "gigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"xs\", streaming=True)\n",
    "\n",
    "# you can no longer access individual samples using Python indexing\n",
    "# you have to iterate over the dataset.\n",
    "next(iter(gigaspeech[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. preview several examples from a large dataset\n",
    "gigaspeech_head = gigaspeech[\"train\"].take(2)\n",
    "list(gigaspeech_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition Architecture\n",
    "- Similar to MT architectures, **ASR** uses an `encoder-decoder framework (RNNs or Transformers)`,\n",
    "  - mapping log mel spectral features to letters or wordpieces.  \n",
    "- ![Schematic architecture for an encoder-decoder speech recognizer.](./images/stts/aed.png)\n",
    "- **Attention-Based Encoder-Decoder (AED)**, also known as `listen attend and spell (LAS)`,\n",
    "  - maps acoustic feature sequences $F = (f_1, f_2, \\cdots, f_t)$ into output sequences like letters or wordpieces $Y=(‚ü®\\text{SOS}‚ü©, y_1, \\cdots, y_m, ‚ü®\\text{EOS}‚ü©)$.\n",
    "- To shorten long acoustic sequences (e.g., 200 frames for a 2-second word) to match much shorter text sequences (5 letters), a subsampling step, **length compression**, is applied.\n",
    "  - $F=(f_1, f_2, \\cdots, f_t) ‚Ü¶ X=(x_1,‚ãØ, x_n),\\; n ‚â™ f$\n",
    "  - The simplest algorithm **low frame rate compression** \n",
    "    - stacks acoustic vectors (e.g., concatenating 3 frames into one) to reduce sequence length, \n",
    "    - creating longer vectors at coarser intervals.  \n",
    "  - After compression, the architecture uses `RNNs (LSTMs) or Transformers`, with possible beam search integration for decoding.\n",
    "    - $\\displaystyle p(y_1, \\cdots, y_m) = ‚àè_{i=1}^{m} p(y_i | y_1, ‚ãØ, y_{i-1}, X)$\n",
    "    - Greedy decoding: $\\displaystyle \\hat{y}_i = \\substack{\\text{argmax} \\\\ \\text{char}‚àà\\text{Alphabet}} p(\\text{char}|y_1, ‚ãØ, y_{i-1}, X)$\n",
    "- ASR models can improve by rescoring hypotheses $Y$ using a larger external language model and interpolating its score with the encoder-decoder score.  \n",
    "  - Use beam search to generate an **n-best list** of sentence hypotheses, then rescore each using a **language model**. \n",
    "    - Combine the encoder-decoder score and language model score with a tunable weight Œª. \n",
    "    - $\\text{score}(Y|X) = \\dfrac{\\log P(Y|X)}{|Y|_c} + \\lambda \\log P_{\\text{LM}}(Y)$ \n",
    "    - The sentence length bias is addressed by normalizing probabilities by the number of characters $|Y|_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "- Encoder-decoders are trained with `cross-entropy loss`, calculated at each decoding step as:  \n",
    "   - $L_{\\text{CE}} = -\\log p(y_i | y_1, \\ldots, y_{i-1}, X)$ \n",
    "   - The total sentence loss is the sum over all tokens:  \n",
    "     - $L_{\\text{CE}} = -\\sum_{i=1}^{m} \\log p(y_i | y_1, \\ldots, y_{i-1}, X)$\n",
    "\n",
    "- **Teacher Forcing**: \n",
    "  - Training typically uses true token history $y_i$, \n",
    "  - but can mix true outputs with predictions $\\hat{y}_i$, e.g., \n",
    "    - using 90% true and 10% decoder output of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectionist Temporal Classification (CTC) \n",
    "- **CTC** offers an alternative approach by `outputting a character for each input frame`,\n",
    "  - ensuring the output matches the input length, \n",
    "  - then collapsing sequences of identical letters into a shorter sequence.  \n",
    "- A naive collapsing function removes consecutive duplicate letters, \n",
    "- ![A naive algorithm for collapsing an alignment between input and letters.](./images/stts/collapse.png)\n",
    "  - but this can misrepresent words (e.g., \"dinner\" transcribed as \"diner\") \n",
    "  - and struggles with aligning silence in the input.  \n",
    "- CTC resolves these issues by introducing a **blank symbol ‚ê£** to the transcription alphabet,\n",
    "  - which helps handle silences and prevents incorrect letter collapsing across blanks. \n",
    "- ![The CTC collapsing function B](./images/stts/ctc.png) \n",
    "  - The collapsing function $B:a‚Ü¶y$ maps alignments $A$ to outputs $Y$ \n",
    "    - by removing blanks and collapsing repeated letters, \n",
    "    - enabling more accurate transcription.  \n",
    "  - The function $B$ is **many-to-one**, meaning multiple alignments can produce the same output. \n",
    "    - For example, several alignments can yield the word \"dinner.\" \n",
    "    - ![Three other legitimate alignments producing the transcript dinner](./images/stts/align.png) \n",
    "  - The inverse function $B^{-1}(Y)$ represents all possible alignments that can generate a given output $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTC Inference\n",
    "- CTC assumes conditional independence at each time step, calculating the **CTC Alignment Probability** $P_{\\text{CTC}}(A|X)$ of an alignment $\\hat{A}=\\{\\hat{a}_1,\\cdots, \\hat{a}_n\\}$ as:  \n",
    "   - $P_{\\text{CTC}}(A|X) = \\prod_{t=1}^T p(a_t|X)$  \n",
    "   - The best alignment is chosen greedily for each time step $t$ as:  \n",
    "     - $\\displaystyle \\hat{a}_t = \\arg\\max_{c \\in C} p_t(c|X)$  \n",
    "\n",
    "- CTC uses an encoder-only model, generating a hidden state $h_t$ for each time step and decoding via a softmax over the character vocabulary. \n",
    "  - The sequence $A$ is then passed to the collapsing function $B$ to produce the output sequence $Y$.\n",
    "  - ![Inference with CTC](./images/stts/infer.png)\n",
    "\n",
    "- The most probable alignment may not correspond to the most probable collapsed output $Y$, \n",
    "  - as multiple alignments can lead to the same $Y$. \n",
    "  - To find the most probable $Y$, sum over the probabilities of all possible alignments:  \n",
    "     - $\\displaystyle P_{\\text{CTC}}(Y|X) = \\sum_{A \\in B^{-1}(Y)} P(A|X)$ \n",
    "     - $\\displaystyle =\\sum_{A \\in B^{-1}(Y)} ‚àè_{t=1}^T p(a_t | h_t)$\n",
    "     - $\\displaystyle \\hat{Y} = \\arg\\max_{Y} P_{\\text{CTC}}(Y|X)$\n",
    "\n",
    "  - Summing over all alignments is computationally expensive. \n",
    "    - Instead, an approximate sum is achieved using a Viterbi beam search, \n",
    "    - focusing on high-probability alignments mapping to the same output.  \n",
    "\n",
    "- Due to the independence assumption, CTC does not learn a language model. \n",
    "  - To enhance predictions, interpolate a language model score $P_{\\text{LM}}(Y)$ and a length factor $L(Y)$ with trained weights:  \n",
    "   - $\\text{score}_{\\text{CTC}}(Y|X) = \\log P_{\\text{CTC}}(Y|X) + \\lambda_1 \\log P_{\\text{LM}}(Y) + \\lambda_2 L(Y)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CTC Training\n",
    "- The CTC-based ASR system uses negative log-likelihood loss over a dataset $D$, defined as:  \n",
    "   - $\\displaystyle L_{\\text{CTC}} = -\\sum_{(X, Y) \\in D} \\log P_{\\text{CTC}}(Y|X)$  \n",
    "\n",
    "- Computing $P_{\\text{CTC}}(Y|X)$ requires summing probabilities over all alignments that collapse to $Y$:  \n",
    "   - $\\displaystyle P_{\\text{CTC}}(Y|X) = \\sum_{A \\in B^{-1}(Y)} \\prod_{t=1}^T p(a_t|h_t)$  \n",
    "   - This can be efficiently computed using [dynamic programming and a forward-backward algorithm](https://distill.pub/2017/ctc/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CTC and Encoder-Decoder\n",
    "- The encoder-decoder cross-entropy loss and CTC loss can be combined during training, weighted by a tunable parameter $Œª$:  \n",
    "   - $\\displaystyle L = -\\lambda \\log P_{\\text{enc-dec}}(Y|X) - (1 - \\lambda) \\log P_{\\text{CTC}}(Y|X)$  \n",
    "- ![Combining the CTC and encoder-decoder loss functions.](./images/stts/ctclm.png)\n",
    "- For inference, the combined losses are integrated with a language model (or length penalty), yielding the output sequence:  \n",
    "   - $\\displaystyle \\hat{Y} = \\arg\\max_Y [\\lambda \\log P_{\\text{enc-dec}}(Y|X) - (1 - \\lambda) \\log P_{\\text{CTC}}(Y|X) + \\gamma \\log P_{\\text{LM}}(Y)]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Models: RNN-T for improving CTC\n",
    "- **CTC Limitations and Streaming Advantage**: \n",
    "  - Due to its strong independence assumption, CTC models are less accurate than attention-based encoder-decoder models. \n",
    "  - However, CTC supports streaming, allowing word recognition as the user speaks, unlike attention models that require the entire input sequence to compute attention context.  \n",
    "\n",
    "- To overcome the conditional independence limitation of CTC and incorporate output history,\n",
    "  - the [RNN-Transducer (RNN-T)](https://lorenlugosch.github.io/posts/2020/11/transducer/) integrates a CTC acoustic model with a language model (predictor) that conditions on previous outputs.  \n",
    "- ![The RNN-T model computing the output token distribution at time t](./images/stts/rnnt.png)\n",
    "\n",
    "- At each time step $t$:  \n",
    "  - The CTC encoder computes a hidden state $h_t^{\\text{enc}}$ from the input sequence $x_1, \\dots, x_t$.  \n",
    "  - The predictor (language model) processes the output history $y_{<u_t}$ (excluding blanks) to produce $h_u^{\\text{pred}}$.  \n",
    "  - These hidden states are combined and passed through a softmax layer to predict the next character.    \n",
    "     - $\\displaystyle P_{\\text{RNN-T}}(Y|X) = \\sum_{A \\in B^{-1}(Y)} P(A|X) = \\sum_{A \\in B^{-1}(Y)} \\prod_{t=1}^T p(a_t | h_t, y_{<u_t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASR Evaluation: Word Error Rate\n",
    "- **Word Error Rate (WER) Definition** evaluates how the `hypothesized word string` from a speech recognizer differs from a `reference transcription`:  \n",
    "   - $\\text{Word Error Rate (WER)} = 100 \\times \\dfrac{\\text{Insertions} + \\text{Substitutions} + \\text{Deletions}}{\\text{Total Words in Correct Transcript}}$\n",
    "     - WER can exceed 100% due to insertions.\n",
    "\n",
    "- **WER Calculation Process**:  \n",
    "  - Compute the **minimum edit distance** (substitutions, insertions, deletions) between the hypothesized and reference strings.  \n",
    "  - üçé An alignment with 6 substitutions, 3 insertions, and 1 deletion out of 13 reference words results in a WER of $\\frac{6+3+1}{13} \\times 100 = 76.9\\%$.\n",
    "\n",
    "- **Sentence Error Rate** computes the percentage of sentences with at least one word error, providing an additional perspective on recognition performance.\n",
    "\n",
    "- **Evaluation Tool - [Score Lite (Sclite)](https://github.com/usnistgov/SCTK)**:  a script from NIST, automates WER computation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ Practice [Hugging Face Audio course](https://huggingface.co/learn/audio-course)\n",
    "- [Unit 5. Automatic Speech Recognition](https://huggingface.co/learn/audio-course/chapter5/introduction)\n",
    "  - üìù 1. Pre-trained models for speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Probing CTC Models\n",
    "# such as Wav2Vec2, HuBERT and XLSR, they are small and fast\n",
    "# but prone to phonetic spelling errors. \n",
    "\n",
    "# 1.1 load a small excerpt of the LibriSpeech ASR dataset\n",
    "# to demonstrate Wav2Vec2‚Äôs speech transcription capabilities:\n",
    "# \n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 explore one of the 73 audio samples \n",
    "from IPython.display import Audio\n",
    "\n",
    "sample = dataset[2]\n",
    "\n",
    "print(sample[\"text\"])\n",
    "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 use the official Wav2Vec2 base checkpoint fine-tuned on 100 hours of LibriSpeech data:\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-100h\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe a sample\n",
    "# find the wrong words due to the shortcoming of a CTC model.\n",
    "# prone to phonetic spelling errors \n",
    "# due it almost entirely bases its prediction on the acoustic input\n",
    "pipe(sample[\"audio\"].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Graduation to Seq2Seq\n",
    "# which support casing and punctuation\n",
    "# load the Whisper Base checkpoint\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-base\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the previous sample\n",
    "pipe(sample[\"audio\"], max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it on the Multilingual LibriSpeech (MLS) dataset\n",
    "dataset = load_dataset(\n",
    "    \"facebook/multilingual_librispeech\", \"spanish\", split=\"test\", streaming=True\n",
    ")\n",
    "sample = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the text transcription and take a listen to the audio segment:\n",
    "print(sample[\"transcript\"])\n",
    "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass a copy of the audio sample, so that we can re-use the same audio sample\n",
    "pipe(sample[\"audio\"].copy(), max_new_tokens=256, generate_kwargs={\"task\": \"transcribe\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whisper can also do translation\n",
    "pipe(sample[\"audio\"], max_new_tokens=256, generate_kwargs={\"task\": \"translate\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Long-Form Transcription and Timestamps\n",
    "# Whisper is inherently designed to work with 30 second samples\n",
    "# padding shorter and truncating longer\n",
    "\n",
    "# 3.1 concatenate audio sample to 5 minutes\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "target_length_in_m = 5\n",
    "\n",
    "# convert from minutes to seconds (* 60) to num samples (* sampling rate)\n",
    "sampling_rate = pipe.feature_extractor.sampling_rate\n",
    "target_length_in_samples = target_length_in_m * 60 * sampling_rate\n",
    "\n",
    "# iterate over our streaming dataset, concatenating samples until we hit our target\n",
    "long_audio = []\n",
    "for sample in dataset:\n",
    "    long_audio.extend(sample[\"audio\"][\"array\"])\n",
    "    if len(long_audio) > target_length_in_samples:\n",
    "        break\n",
    "\n",
    "long_audio = np.asarray(long_audio)\n",
    "\n",
    "# how did we do?\n",
    "seconds = len(long_audio) / 16000\n",
    "minutes, seconds = divmod(seconds, 60)\n",
    "print(f\"Length of audio sample is {minutes} minutes {seconds:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 transcribe the long audio sample with chunking and batching \n",
    "pipe(\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30,\n",
    "    batch_size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Predict segment-level timestamps for the audio data\n",
    "pipe(\n",
    "    long_audio,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"task\": \"transcribe\"},\n",
    "    chunk_length_s=30,\n",
    "    batch_size=8,\n",
    "    return_timestamps=True,\n",
    ")[\"chunks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 2. [Choosing a dataset](https://huggingface.co/blog/audio-datasets#a-tour-of-audio-datasets-on-the-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 3. Evaluation and metrics for speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Word Error Rate\n",
    "\n",
    "reference = \"the cat sat on the mat\"\n",
    "prediction = \"the cat sit on the\"\n",
    "\n",
    "# WER = (S+I+D)/N = (1+0+1)/6=1/3\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
    "print(wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Word Accuracy\n",
    "# WAcc = 1 - WER\n",
    "\n",
    "# 3.3 Character Error Rate (CER)\n",
    "# For the example in 3.1\n",
    "# CER = (S+I+D)/N = (1+0+3)/14=2/7\n",
    "\n",
    "# the WER requires systems to have greater understanding of the context of the predictions\n",
    "# for word-based languages such as English\n",
    "# Where for character-based languages such as Chinese, CER is preferred\n",
    "# ‚ö†Ô∏è A Chinese character is equivalent to an English word\n",
    "# e.g. Êú® ‚àΩ treeÔºåÁü≥ ‚àΩ stone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"‰∏úÊñπÂá∫Áé∞‰∫ÜÁ¨¨‰∏ÄÁºïÊõôÂÖâ„ÄÇ\"\n",
    "prediction = \"‰∏úÊñπÂá∫Áé∞‰∫ÜÂºü‰∏ÄÊêÇÈò≥ÂÖâ\"\n",
    "\n",
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "cer = cer_metric.compute(references=[reference], predictions=[prediction])\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 orthography and normalization\n",
    "# orthography - train with and predict casing and punctuation\n",
    "# normalization - remove any casing and punctuation\n",
    "# Wav2Vec2:  HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAUS AND ROSE BEEF LOOMING BEFORE US SIMALYIS DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\n",
    "# Whisper:   He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 the normaliser of Whisper\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "prediction = \" He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\"\n",
    "normalized_prediction = normalizer(prediction)\n",
    "\n",
    "normalized_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized WER is usually lower than orthographic WER\n",
    "# It is recommended training on orthographic text and \n",
    "#   evaluating on normalised text to get the best of both worlds.\n",
    "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
    "normalized_referece = normalizer(reference)\n",
    "\n",
    "wer = wer_metric.compute(\n",
    "    references=[normalized_referece], predictions=[normalized_prediction]\n",
    ")\n",
    "wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Putting it all together\n",
    "# pre-trained models, dataset selection and evaluation.\n",
    "\n",
    "# 1) Load whisper-small\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) login onto HuggingFace to download common voice dataset\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# or if from command line\n",
    "# huggingface-cli login\n",
    "# or \n",
    "# export HF_API_TOKEN=\"your_token_here\"\n",
    "\n",
    "# pip install soundfile librosa\n",
    "\n",
    "from datasets import load_dataset\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"zh-CN\", split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) pick out the needed dataset columns\n",
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\"},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) compute the baseline CER without normalization\n",
    "from evaluate import load\n",
    "\n",
    "cer_metric = load(\"cer\")\n",
    "\n",
    "cer_ortho = 100 * cer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "cer_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) compute the normalized CER\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "# compute normalised WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]\n",
    "\n",
    "# filtering step to only evaluate the samples that correspond to non-zero references\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i]\n",
    "    for i in range(len(all_predictions_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i]\n",
    "    for i in range(len(all_references_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù 4. How to fine-tune an ASR system with the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Linking the notebook to the Hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "# or in terminal\n",
    "# export HF_API_TOKEN=\"your_token_here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"zh-CN\", split=\"train+validation\"\n",
    ")\n",
    "common_voice[\"test\"] = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"zh-CN\", split=\"test\"\n",
    ")\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select needed columns\n",
    "common_voice = common_voice.select_columns([\"audio\", \"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Extractor, Tokenizer and Processor\n",
    "# the Whisper model has an associated feature extractor and tokenizer, \n",
    "# called WhisperFeatureExtractor and WhisperTokenizer respectively.\n",
    "# these two objects are wrapped under a single class, called the WhisperProcessor\n",
    "\n",
    "# 1) see all possible languages supported by Whisper\n",
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "\n",
    "TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) load our processor from the pre-trained checkpoint\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    \"openai/whisper-small\", language=\"chinese\", task=\"transcribe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Pre-Process the Data\n",
    "# Pay particular attention to the \"audio\" column\n",
    "\n",
    "common_voice[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample audio samples on-the-fly\n",
    "# change the sampling rate to 16kHz expected by the Whisper model.\n",
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. write a function to prepare our data ready for the model:\n",
    "# a. We load and resample the audio data on a sample-by-sample basis \n",
    "#   by calling sample[\"audio\"]. As explained above, ü§ó Datasets performs \n",
    "#   any necessary resampling operations on the fly.\n",
    "# b. We use the feature extractor to compute the log-mel spectrogram \n",
    "#   input features from our 1-dimensional audio array.\n",
    "# c. We encode the transcriptions to label ids through the use of the tokenizer.\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        audio=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        text=example[\"sentence\"],\n",
    "    )\n",
    "\n",
    "    # compute input length of audio sample in seconds\n",
    "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "\n",
    "    return example\n",
    "\n",
    "# 4.2 apply the data preparation function to all of our training examples\n",
    "common_voice = common_voice.map(\n",
    "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 filter any training data with audio samples longer than 30s\n",
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length\n",
    "\n",
    "# apply our filter function to all samples of our training dataset \n",
    "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how much training data being removed\n",
    "common_voice[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Training and Evaluation\n",
    "# 5.1 Define a data collator\n",
    "# perform both the feature extractor and the tokenizer operations:\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  initialise the data collator \n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Evaluation metrics\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that takes our model predictions and returns the CER metric.\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # compute orthographic cer\n",
    "    cer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # compute normalised CER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    cer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"cer_ortho\": cer_ortho, \"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 Load a pre-trained checkpoint\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adust model parameters\n",
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"chinese\", task=\"transcribe\", use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 Define the training arguments\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",  # save locally\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=500,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"cer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 forward the training arguments to the ü§ó Trainer along with \n",
    "# our model, dataset, data collator and compute_metrics function:\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the model\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=\"./\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Build a demo with Gradio\n",
    "# 6.1 load the model\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"./\"  # update with your model id\n",
    "pipe = pipeline(\"automatic-speech-recognition\", model=model_id, device='cuda')\n",
    "\n",
    "# 6.2 define the serve fun\n",
    "def transcribe_speech(filepath):\n",
    "    output = pipe(\n",
    "        filepath,\n",
    "        max_new_tokens=256,\n",
    "        generate_kwargs={\n",
    "            \"task\": \"transcribe\",\n",
    "            \"language\": \"sinhalese\",\n",
    "        },  # update with the language you've fine-tuned on\n",
    "        chunk_length_s=30,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    return output[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 use the Gradio blocks feature to launch two tabs on our demo:\n",
    "#  one for microphone transcription, and the other for file upload.\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")\n",
    "\n",
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"upload\", type=\"filepath\"),\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 launch the Gradio demo \n",
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe, file_transcribe],\n",
    "        [\"Transcribe Microphone\", \"Transcribe Audio File\"],\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ [Unit 6. From text to speech](https://huggingface.co/learn/audio-course/chapter6/introduction)\n",
    "- üìù [Text-to-speech datasets](https://huggingface.co/learn/audio-course/chapter6/tts_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù Pre-trained models for text-to-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù Fine-tuning SpeechT5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìù Evaluating text-to-speech models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
