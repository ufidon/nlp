{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/01.tned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/01.tned.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Text Normalization, Edit Distance\n",
    "\n",
    "üìù SALP chapter 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Sentence Segmentation?\n",
    "- **Sentence Segmentation** is the process of dividing a corpus (a large collection of text) into individual sentences.\n",
    "  - Typically, sentences are separated by **punctuation marks** such as periods (.), exclamation marks (!), or question marks (?).\n",
    "- Challenges\n",
    "  - **Abbreviations:** e.g., \"Dr.\", \"etc.\", \"Mr.\" could be mistaken as sentence boundaries.\n",
    "  - **Quotes and Parentheses:** Sentences within quotes or parentheses can create ambiguity.\n",
    "  - **Ellipses:** Ellipsis (...) can be mistaken as multiple sentence boundaries.\n",
    "\n",
    "- üçé Segment text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Mr. Smith went to Washington. He enjoyed his time there.\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmenting Sentences into Words\n",
    "- **Word Tokenization** is the process of breaking down a sentence into individual words or tokens.\n",
    "- Word tokenization helps in identifying the `basic units of meaning` in a text.\n",
    "- Challenges\n",
    "  - **Contractions:** e.g., \"don't\" can be split into \"do\" and \"n't\".\n",
    "  - **Hyphenated Words:** e.g., \"state-of-the-art\" could be considered one word or multiple words.\n",
    "- üçé segment a sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentence = \"Mr. Smith went to Washington.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation in Segmentation\n",
    "- **Sentence Segmentation:** Periods, exclamation marks, and question marks usually denote the end of a sentence.\n",
    "- **Word Tokenization:** Punctuation marks like commas, semicolons, and quotes are often separated from the words they are attached to.\n",
    "- üçé\n",
    "  - Sentence: \"Hello, world!\"\n",
    "  - Tokenized: [\"Hello\", \",\", \"world\", \"!\"]\n",
    "- Whether to keep or discard punctuation depends on the task. \n",
    "  - For sentiment analysis, punctuation can carry important emotional context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spoken Language Components\n",
    "- **Utterances:** A complete unit of speech that may not be grammatically correct.\n",
    "- **Disfluencies:** Interruptions in the flow of speech, e.g., \"uh,\" \"um,\" \"you know.\"\n",
    "- **Fragments:** Incomplete sentences or phrases.\n",
    "- **Fillers:** Words or sounds that fill gaps in speech but do not add meaning, e.g., \"like,\" \"you know.\"\n",
    "- **Filled Pauses:** Prolonged sounds such as \"uhhh\" or \"ummm.\"\n",
    "- üçé\n",
    "  - Utterance: \"Well, I... I think, um, it's great!\"\n",
    "  - Tokenized: [\"Well\", \",\", \"I\", \"...\", \"I\", \"think\", \",\", \"um\", \",\", \"it's\", \"great\", \"!\"]\n",
    "- Disfluencies may signal that the speaker is restarting the clause or idea\n",
    "  - helpful in speech recognition in predicting the upcoming word\n",
    "  - treated as regular words in such cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Types vs. Word Instances\n",
    "- Word Types\n",
    "  - **Word Types** refer to distinct words in a text, ignoring repetition.\n",
    "  - üçé: In the text \"cat cat dog,\" \"cat\" and \"dog\" are the word types.\n",
    "- Word Instances\n",
    "  - **Word Instances** refer to the total occurrences of words in a text.\n",
    "  - üçé: In the text \"cat cat dog,\" there are 3 word instances: two \"cat\" and one \"dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Sensitivity in Word Types\n",
    "- **Case Sensitivity:** \n",
    "  - In some NLP tasks, \"Word\" and \"word\" may be considered different types.\n",
    "  - This distinction is important in cases where capitalization changes meaning\n",
    "    - e.g., \"Apple\" (company) vs. \"apple\" (fruit).\n",
    "- **Normalization:** \n",
    "  - Often, texts are normalized to lowercase to treat \"Word\" and \"word\" as the same type for tasks like `word frequency analysis`.\n",
    "\n",
    "- üçé case sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"Word\", \"word\", \"Apple\", \"apple\"]\n",
    "lowercase_words = [word.lower() for word in words]\n",
    "print(set(lowercase_words)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Corpus Name**       | **Word Instances** | **Word Types**  | **Description**                                                                 |\n",
    "|-----------------------|--------------------|-----------------|---------------------------------------------------------------------------------|\n",
    "| **Brown Corpus**      | ~1,000,000         | ~50,000         | The first million-word electronic corpus of American English.                   |\n",
    "| **Penn Treebank**     | ~4,500,000         | ~100,000        | Annotated corpus of English, primarily used for training NLP models.            |\n",
    "| **British National Corpus (BNC)** | ~100,000,000      | ~600,000        | A wide range of modern British English (spoken and written).                    |\n",
    "| **COCA (Corpus of Contemporary American English)** | ~1,000,000,000   | ~450,000        | Largest freely available corpus of American English, covering 1990-2019.        |\n",
    "| **Google Ngrams**     | ~500,000,000,000   | ~13,588,391     | Dataset of n-grams (1 to 5 words) from scanned books dating from 1500-2008.     |\n",
    "| **Reuters Corpus**    | ~810,000           | ~30,000         | Collection of news documents used for text classification and NLP research.     |\n",
    "| **Wikipedia Corpus**  | ~1,000,000,000+    | ~20,000,000+    | Continuously updated corpus of Wikipedia articles, covering a vast range of topics. |\n",
    "| **Gutenberg Corpus**  | ~25,000,000        | ~200,000        | Collection of classic literature and historical texts available as eBooks.      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ Explore [Google N-grams Corpus](https://books.google.com/ngrams/info) \n",
    "- an extensive dataset consisting of n-grams (sequences of 1 to 5 words) derived from a large corpus of books scanned by Google\n",
    "- contain around 500 billion word instances, over 13 million unique word types\n",
    "- [Google books syntactic N-grams](https://commondatastorage.googleapis.com/books/syntactic-ngrams/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herdan‚Äôs Law (Heaps‚Äô Law)\n",
    "- crucial for predicting vocabulary growth in linguistics and NLP\n",
    "- as a text corpus grows, vocabulary size increases at a diminishing rate\n",
    "- **Formal Definition:**\n",
    "  - the vocabulary size $V$ grows as a sublinear power-law function of the number of word tokens $N$ in the corpus:\n",
    "  - $ V(N) = k \\times N^\\beta $\n",
    "    - $ V(N) $ is the number of distinct word types (vocabulary size).\n",
    "    - $ N $ is the total number of word tokens.\n",
    "    - $ k $ is a constant that depends on the corpus.\n",
    "    - $ 0 < \\beta <1 $ is an exponent that usually ranges between 0.4 and 0.6.\n",
    "-  **Key Points:**\n",
    "   - **Sublinear Growth**: \n",
    "     - As more words are added to the corpus, the rate of new word types (unique words) discovered decreases. \n",
    "     - Initially, the vocabulary grows quickly, but over time, as more text is added, the discovery of new unique words slows down.\n",
    "   - **Vocabulary Saturation**: \n",
    "     - In any given language, there is a finite number of commonly used words. \n",
    "     - As the corpus size increases, fewer and fewer new words are encountered, and the vocabulary growth rate diminishes.\n",
    "   - **Implications for Language Models**: \n",
    "     - Even with large corpora, the vocabulary size will not grow indefinitely and will eventually plateau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordform and Lemma\n",
    "- A **wordform** is the specific written or spoken version of a word\n",
    "  - including its tense, case, number, or mood, depending on the language.\n",
    "  - üçé\n",
    "    - *Runs, running, ran* are different wordforms of the verb *run*.\n",
    "    - *Cats, cat's, cats'* are different wordforms of the noun *cat*\n",
    "- A **lemma** is the canonical or dictionary form of a word\n",
    "  - representing all its possible wordforms.\n",
    "  - the base or root form used to look up words in dictionaries.\n",
    "  - often used in linguistic analysis to group different wordforms together.\n",
    "  - üçé\n",
    "    - The lemma for the wordforms *runs, running, ran* is *run*.\n",
    "    - The lemma for the wordforms *cats, cat's, cats'* is *cat*.\n",
    "- The process of converting a wordform into its lemma is called **lemmatization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unix Tools for Word Tokenization\n",
    "- `tr` (translate): Convert or delete characters in text.\n",
    "- `cut`: Remove sections from each line of files or input.\n",
    "- `awk`: Pattern scanning and processing language.\n",
    "- `grep`: Search for patterns in text.\n",
    "  - üçé Tokenizing a text file\n",
    "    ```bash\n",
    "    grep -oE '\\w+' input.txt > tokens.txt\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 1. `tr` replaces spaces with newlines\n",
    "echo \"Hello, world! How are you?\" | tr -s ' ' '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 2. `cut` splits the text using a space as a delimiter and extracts the first word.\n",
    "echo \"Hello, world! How are you?\" | cut -d ' ' -f 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 3. `awk` prints each word separately by iterating over fields (`$i`)\n",
    "echo \"Hello, world! How are you?\" | awk '{for(i=1;i<=NF;i++) print $i}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 4. `grep` finds and prints each word matching the regex pattern `\\w+` (alphanumeric characters).\n",
    "echo \"Hello, world! How are you?\" | grep -oE '\\w+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 5. Combining `tr` and `awk`\n",
    "# - `tr -s ' ' '\\n'` tokenizes by spaces.\n",
    "# - `awk '{print tolower($0)}'` converts each token to lowercase.\n",
    "\n",
    "echo \"Hello, world! How are you?\" | tr -s ' ' '\\n' | awk '{print tolower($0)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 0. Download [Shakespeare's](https://www.gutenberg.org/cache/epub/100/pg100.txt) works in a text file, save it as `sh.txt`\n",
    "wget -O sh.txt https://www.gutenberg.org/cache/epub/100/pg100.txt \n",
    "# 1. ‚Äòsqueezed‚Äô a series of non-alphabetic characters in a row into a single newline\n",
    "tr -sc 'A-Za-z' '\\n' < sh.txt # sh.txt contains all Shakespeare's works\n",
    "# 2. find the vocabulary of Shakespeare's works\n",
    "tr -sc 'A-Za-z' '\\n' < sh.txt | sort | uniq -c\n",
    "# 3. convert Shakespeare's vocabulary into lowercase\n",
    "tr -sc 'A-Za-z' '\\n' < sh.txt | tr A-Z a-z | sort | uniq -c\n",
    "# 4. find frequent words\n",
    "tr -sc 'A-Za-z' '\\n' < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r > corpus.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Methods in NLP\n",
    "- Top-down (Rule-based) Tokenization\n",
    "  - Uses predefined rules to split text into tokens\n",
    "  - Typically faster but less flexible than machine learning approaches\n",
    "  - Penn Treebank Tokenization\n",
    "    - Developed for the [Penn Treebank Project](https://catalog.ldc.upenn.edu/LDC99T42)\n",
    "    - Specific rules for English text\n",
    "\n",
    "üçé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Don't hesitate to email john.doe@example.com\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üçé NLTK RegexpTokenizer\n",
    "- Uses regular expressions for flexible tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"Hello, world! How's it going?\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chinese, Korean, and Japanese Tokenization\n",
    "\n",
    "- These languages don't use spaces between words\n",
    "- Requires specialized algorithms\n",
    "- Chinese Tokenization\n",
    "  - Uses techniques like maximum matching or statistical models\n",
    "- Korean Tokenization\n",
    "  - Considers complex morphological structure\n",
    "- Japanese Tokenization\n",
    "  - Uses dictionary-based approaches or statistical models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if jieba is installed on Google Colab\n",
    "import sys\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "jieba_installed = 'jieba' in sys.modules\n",
    "\n",
    "if in_colab and not jieba_installed:\n",
    "    print(\"jieba is not installed. Installing now...\")\n",
    "    %pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üçé 1. Tokenize Chinese using jieba library\n",
    "\n",
    "import jieba\n",
    "\n",
    "text = \"ÊàëÂñúÊ¨¢Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ\"\n",
    "tokens = jieba.cut(text)\n",
    "print(list(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom-up Tokenization Algorithms\n",
    "- also called Subword tokenization methods \n",
    "- efficiently handle rare words and out-of-vocabulary terms\n",
    "- [Byte-Pair Encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding)\n",
    "  - Merges the most frequent pair of bytes or characters until a desired vocabulary size is reached\n",
    "  - üçé Word `\"lower\"` could be split into `\"low\"` + `\"er\"` if `\"lower\"` is rare but `\"low\"` and `\"er\"` are common.\n",
    "- `Unigram Language Modeling (SentencePiece)`\n",
    "  - Instead of merging, it starts with a large set of potential subword units.\n",
    "  - Find the subset of subword units that maximizes the likelihood of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Comparison: BPE vs. Unigram Language Modeling**\n",
    "- **BPE**:\n",
    "  - **Deterministic**: Iteratively merges most frequent pairs.\n",
    "  - **Example**: `\"lowered\"` ‚Üí `\"low\"` + `\"ered\"`.\n",
    "\n",
    "- **Unigram**:\n",
    "  - **Probabilistic**: Selects subwords to maximize likelihood.\n",
    "  - **Example**: `\"lowered\"` ‚Üí `\"low\"` + `\"ered\"` based on likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üçé 1. BPE in Python\n",
    "\n",
    "from tokenizers import Tokenizer, models, trainers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "trainer = trainers.BpeTrainer(vocab_size=5000)\n",
    "tokenizer.train([\"corpus.txt\"], trainer)\n",
    "encoded = tokenizer.encode(\"lowered\")\n",
    "print(encoded.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üçé 2. Unigram in Python\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(input='corpus.txt', model_prefix='m', vocab_size=5000, model_type='unigram')\n",
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "print(sp.encode_as_pieces(\"lowered\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ‚ö†Ô∏è In neural networks\n",
    "  - Words are `NOT` used as the internal unit of representation at all! \n",
    "  - The input strings are tokenized into words even `only parts` of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-Pair Encoding (BPE)\n",
    "\n",
    "- Initially developed as a text compression algorithm.\n",
    "- Adopted by OpenAI for tokenization in the GPT models.\n",
    "- used in GPT, GPT-2, RoBERTa, BART, DeBERTa.\n",
    "- It iteratively merges `the most frequent pairs of symbols` in a sequence to reduce the overall number of symbols\n",
    "\n",
    "\n",
    "### **Byte-Level BPE**\n",
    "\n",
    "- **Purpose**: Handles all characters by using bytes instead of Unicode.\n",
    "- **Base Vocabulary Size**: 256 (for all byte values).\n",
    "- **Advantage**: Avoids unknown tokens for unseen characters.\n",
    "\n",
    "\n",
    "### Steps of the BPE Algorithm:\n",
    "1. **Initial Symbol Representation**: Treat each character (or byte) in the text as an individual symbol.\n",
    "2. **Count Symbol Pairs**: Identify all pairs of consecutive symbols and count their occurrences.\n",
    "3. **Merge the Most Frequent Pair**: Replace the most frequent pair of symbols with a new symbol (typically the concatenation of the two symbols).\n",
    "4. **Repeat**: Repeat steps 2 and 3 until the desired number of merges is reached or no more frequent pairs exist.\n",
    "5. **Tokenization**: After merging, tokenize the text using the final set of symbols.\n",
    "\n",
    "### üçé Example:\n",
    "Let's apply Byte Pair Encoding (BPE) to the following corpus:\n",
    "\n",
    "```\n",
    "low low lower new new newer better better best\n",
    "```\n",
    "\n",
    "### Step-by-Step BPE Tokenization on the Corpus\n",
    "\n",
    "1. **Initial Corpus**:\n",
    "    - We split the words into character sequences with an underscore `_` representing the end of each word. The initial vocabulary is:\n",
    "    \n",
    "    ```\n",
    "    low:     'l o w _' (2 occurrences)\n",
    "    lower:   'l o w e r _' (1 occurrence)\n",
    "    new:     'n e w _' (2 occurrences)\n",
    "    newer:   'n e w e r _' (1 occurrence)\n",
    "    better:  'b e t t e r _' (2 occurrences)\n",
    "    best:    'b e s t _' (1 occurrence)\n",
    "    ```\n",
    "\n",
    "    Initial vocabulary and frequencies:\n",
    "    ```\n",
    "    'l o w _'      -> 2\n",
    "    'l o w e r _'  -> 1\n",
    "    'n e w _'      -> 2\n",
    "    'n e w e r _'  -> 1\n",
    "    'b e t t e r _' -> 2\n",
    "    'b e s t _'    -> 1\n",
    "    ```\n",
    "\n",
    "2. **Step 1: Count symbol pairs**:\n",
    "    - Count the pairs of adjacent symbols in the vocabulary:\n",
    "    ```\n",
    "    ('l', 'o'):3, ('o', 'w'):3, ('w', '_'):4, ('r', '_'):4, ('e', 'r'):4, \n",
    "    ('w', 'e'):2, ('n', 'e'):3, ('e', 'w'):3, ('b', 'e'):3, ('t', 't'):2, ('t', 'e'):2, \n",
    "    ('e', 't'):2, ('t', '_'):1, ('e', 's'):1, ('s', 't'):1\n",
    "    ```\n",
    "\n",
    "    The first most frequent pair is `('w', '_')`.\n",
    "\n",
    "3. **Step 2: Merge the most frequent pair**:\n",
    "    - Replace all occurrences of `('w', '_')` with `w_`:\n",
    "    ```\n",
    "    Merge: ('w', '_'): 4\n",
    "    Old: {'l o w _': 2, 'l o w e r _': 1, 'n e w _': 2, 'n e w e r _': 1, 'b e t t e r _': 2, 'b e s t _': 1}\n",
    "    New: {'l o w_': 2, 'l o w e r _': 1, 'n e w_': 2, 'n e w e r _': 1, 'b e t t e r _': 2, 'b e s t _': 1}\n",
    "    ```\n",
    "\n",
    "4. **Step 3: Repeat the process**:\n",
    "\n",
    "    **New symbol pairs**:\n",
    "    ```\n",
    "    ('l', 'o'):3, ('o', 'w_'):2, ('r', '_'):4, ('e', 'r'):4, ('o', 'w'):1, \n",
    "    ('w', 'e'):2, ('e', 'w_'):2, ('n', 'e'):3, ('e', 'w'):1, ('b', 'e'):3, ('t', 't'):2, \n",
    "    ('t', 'e'):2, ('e', 't'):2, ('t', '_'):1, ('e', 's'):1, ('s', 't'):1\n",
    "    ```\n",
    "\n",
    "    - The first most frequent pair is `('r', '_')`.\n",
    "\n",
    "    **Merge `('lo', 'w')`**:\n",
    "    ```\n",
    "    'low _'      -> 2\n",
    "    'low e r _'  -> 1\n",
    "    'n e w _'    -> 2\n",
    "    'n e w e r _' -> 1\n",
    "    'b e t t e r _' -> 2\n",
    "    'b e s t _'  -> 1\n",
    "    ```\n",
    "\n",
    "    **New symbol pairs**:\n",
    "    ```\n",
    "    ('low', '_'): 3\n",
    "    ('w', 'e'): 1\n",
    "    ('e', 'r'): 3\n",
    "    ('r', '_'): 3\n",
    "    ('n', 'e'): 3\n",
    "    ('e', 'w'): 3\n",
    "    ('b', 'e'): 3\n",
    "    ('e', 't'): 2\n",
    "    ('t', 't'): 2\n",
    "    ('t', 'e'): 2\n",
    "    ('s', 't'): 1\n",
    "    ('t', '_'): 3\n",
    "    ```\n",
    "\n",
    "    - The most frequent pair is `('e', 'r')`.\n",
    "\n",
    "    **Merge `('e', 'r')`**:\n",
    "    ```\n",
    "    'low _'      -> 2\n",
    "    'low er _'   -> 1\n",
    "    'n e w _'    -> 2\n",
    "    'n e w er _' -> 1\n",
    "    'b e t t er _' -> 2\n",
    "    'b e s t _'  -> 1\n",
    "    ```\n",
    "\n",
    "    **New symbol pairs**:\n",
    "    ```\n",
    "    ('low', '_'): 3\n",
    "    ('er', '_'): 3\n",
    "    ('n', 'e'): 3\n",
    "    ('e', 'w'): 3\n",
    "    ('b', 'e'): 3\n",
    "    ('e', 't'): 2\n",
    "    ('t', 't'): 2\n",
    "    ('s', 't'): 1\n",
    "    ('t', '_'): 3\n",
    "    ```\n",
    "\n",
    "    - The most frequent pair is `('n', 'e')`.\n",
    "\n",
    "    **Merge `('n', 'e')`**:\n",
    "    ```\n",
    "    'low _'      -> 2\n",
    "    'low er _'   -> 1\n",
    "    'ne w _'     -> 2\n",
    "    'ne w er _'  -> 1\n",
    "    'b e t t er _' -> 2\n",
    "    'b e s t _'  -> 1\n",
    "    ```\n",
    "\n",
    "### Final Tokenized Vocabulary:\n",
    "- `'low _'`      -> 2\n",
    "- `'low er _'`   -> 1\n",
    "- `'ne w _'`     -> 2\n",
    "- `'ne w er _'`  -> 1\n",
    "- `'b e t t er _' -> 2\n",
    "- `'b e s t _'`  -> 1\n",
    "\n",
    "### Python Implementation of BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairs: ('l', 'o'):3, ('o', 'w'):3, ('w', '_'):4, ('r', '_'):4, ('e', 'r'):4, \n",
      "('w', 'e'):2, ('n', 'e'):3, ('e', 'w'):3, ('b', 'e'):3, ('t', 't'):2, ('t', 'e'):2, \n",
      "('e', 't'):2, ('t', '_'):1, ('e', 's'):1, ('s', 't'):1, \n",
      "\n",
      "Merge: ('w', '_'): 4\n",
      "Old: {'l o w _': 2, 'l o w e r _': 1, 'n e w _': 2, 'n e w e r _': 1, 'b e t t e r _': 2, 'b e s t _': 1}\n",
      "New: {'l o w_': 2, 'l o w e r _': 1, 'n e w_': 2, 'n e w e r _': 1, 'b e t t e r _': 2, 'b e s t _': 1}\n",
      "\n",
      "Pairs: ('l', 'o'):3, ('o', 'w_'):2, ('r', '_'):4, ('e', 'r'):4, ('o', 'w'):1, \n",
      "('w', 'e'):2, ('e', 'w_'):2, ('n', 'e'):3, ('e', 'w'):1, ('b', 'e'):3, ('t', 't'):2, \n",
      "('t', 'e'):2, ('e', 't'):2, ('t', '_'):1, ('e', 's'):1, ('s', 't'):1, \n",
      "\n",
      "Merge: ('r', '_'): 4\n",
      "Old: {'l o w_': 2, 'l o w e r _': 1, 'n e w_': 2, 'n e w e r _': 1, 'b e t t e r _': 2, 'b e s t _': 1}\n",
      "New: {'l o w_': 2, 'l o w e r_': 1, 'n e w_': 2, 'n e w e r_': 1, 'b e t t e r_': 2, 'b e s t _': 1}\n",
      "\n",
      "Pairs: ('l', 'o'):3, ('o', 'w_'):2, ('e', 'r_'):4, ('o', 'w'):1, ('w', 'e'):2, \n",
      "('e', 'w_'):2, ('n', 'e'):3, ('e', 'w'):1, ('b', 'e'):3, ('t', 't'):2, ('t', 'e'):2, \n",
      "('e', 't'):2, ('t', '_'):1, ('e', 's'):1, ('s', 't'):1, \n",
      "\n",
      "Merge: ('e', 'r_'): 4\n",
      "Old: {'l o w_': 2, 'l o w e r_': 1, 'n e w_': 2, 'n e w e r_': 1, 'b e t t e r_': 2, 'b e s t _': 1}\n",
      "New: {'l o w_': 2, 'l o w er_': 1, 'n e w_': 2, 'n e w er_': 1, 'b e t t er_': 2, 'b e s t _': 1}\n",
      "\n",
      "Pairs: ('l', 'o'):3, ('o', 'w_'):2, ('o', 'w'):1, ('w', 'er_'):2, ('e', 'w_'):2, \n",
      "('n', 'e'):3, ('e', 'w'):1, ('b', 'e'):3, ('t', 't'):2, ('t', 'er_'):2, ('e', 't'):2, \n",
      "('t', '_'):1, ('e', 's'):1, ('s', 't'):1, \n",
      "\n",
      "Merge: ('l', 'o'): 3\n",
      "Old: {'l o w_': 2, 'l o w er_': 1, 'n e w_': 2, 'n e w er_': 1, 'b e t t er_': 2, 'b e s t _': 1}\n",
      "New: {'lo w_': 2, 'lo w er_': 1, 'n e w_': 2, 'n e w er_': 1, 'b e t t er_': 2, 'b e s t _': 1}\n",
      "\n",
      "Final vocabulary:\n",
      "{'lo w_': 2, 'lo w er_': 1, 'n e w_': 2, 'n e w er_': 1, 'b e t t er_': 2, 'b e s t _': 1}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Returns a set of symbol pairs in a word.\"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "def print_dic(d:dict, rowlen = 6):\n",
    "    \"\"\"Print a dictionary in multiple lines\"\"\"\n",
    "    r = 1\n",
    "    for k,v in d.items():\n",
    "        if(r%rowlen == 0):\n",
    "            print()\n",
    "\n",
    "        print(k, \":\", v, sep='', end=', ')\n",
    "        r = r+1\n",
    "    print()\n",
    "\n",
    "def merge_pair(pair, vocab):\n",
    "    \"\"\"Merge all occurrences of the most frequent pair in the vocabulary.\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "def bpe_tokenizer(vocab, num_merges):\n",
    "    \"\"\"Main BPE tokenization function.\"\"\"\n",
    "    for _ in range(num_merges):\n",
    "        # Get the frequency of all pairs in the vocabulary\n",
    "        pair_counts = defaultdict(int)\n",
    "        for word, freq in vocab.items():\n",
    "            pairs = get_pairs(word.split())\n",
    "            for pair in pairs:\n",
    "                pair_counts[pair] += freq\n",
    "        \n",
    "        if not pair_counts:\n",
    "            break\n",
    "\n",
    "        # Find the most frequent pair\n",
    "        best_pair = max(pair_counts, key=pair_counts.get)\n",
    "        print(\"\\nPairs:\", end=' ')\n",
    "        print_dic(pair_counts)\n",
    "        print(\"\\nMerge:\", f'{best_pair}: {pair_counts[best_pair]}')\n",
    "        \n",
    "        # Merge the most frequent pair in the vocabulary\n",
    "        print('Old:', vocab)\n",
    "        vocab = merge_pair(best_pair, vocab)\n",
    "        print('New:', vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "# Toy example: Vocabulary with word frequencies\n",
    "vocab = {\n",
    "    'l o w _': 2,\n",
    "    'l o w e r _': 1,\n",
    "    'n e w _': 2,\n",
    "    'n e w e r _': 1,\n",
    "    'b e t t e r _': 2,\n",
    "    'b e s t _': 1\n",
    "}\n",
    "\n",
    "# Perform 4 merges\n",
    "num_merges = 4\n",
    "final_vocab = bpe_tokenizer(vocab, num_merges)\n",
    "\n",
    "\n",
    "# Display the final tokenized vocab\n",
    "print(\"\\nFinal vocabulary:\")\n",
    "print(final_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Algorithm\n",
    "\n",
    "New inputs are tokenized closely following the BPE training process:\n",
    "\n",
    "1. **Normalization**: The input text is normalized.\n",
    "2. **Pre-tokenization**: The normalized text is split into words.\n",
    "3. **Character Splitting**: Each word is split into individual characters.\n",
    "4. **Applying Merge Rules**: The learned BPE merge rules are applied in sequence to the split characters.\n",
    "\n",
    "### üçé**Example**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simplified BPE implementation from [HuggingFace](https://huggingface.co/learn/nlp-course/chapter6/5)\n",
    "\n",
    "1. **Create a Corpus**: \n",
    "   - Example sentences are used to form a basic corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Pre-tokenization**:\n",
    "   - Use a pre-trained tokenizer (like GPT-2) to split the corpus into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Compute Word Frequencies**:\n",
    "   - Calculate the frequency of each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Compute Base Vocabulary**:\n",
    "   - Extract and sort all unique characters (alphabet) in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Include special tokens like GPT-2‚Äôs `<|endoftext|>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Split Words into Characters**:\n",
    "   - Initialize word splits by breaking words into individual characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Compute Pair Frequencies**:\n",
    "   - Write a function to calculate the frequency of character pairs, which is essential for training the BPE merges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - Example output for pair frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the most frequent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first merge to learn is ('ƒ†', 't') -> 'ƒ†t'\n",
    "  - add 'ƒ†t' to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {(\"ƒ†\", \"t\"): \"ƒ†t\"}\n",
    "vocab.append(\"ƒ†t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply that merge in the splits dictionary with another function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits\n",
    "\n",
    "# The result of the first merge\n",
    "splits = merge_pair(\"ƒ†\", \"t\", splits)\n",
    "print(splits[\"ƒ†trained\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loop until we have learned all the merges rules\n",
    "  - suppose a vocab size of 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 19 merge rules learned \n",
    "   - the initial vocabulary had a size of 31 ‚Äî 30 characters in the alphabet, \n",
    "   - plus the special token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merges), \":\\n\", merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The vocabulary is composed of \n",
    "  - the special token, \n",
    "  - the initial alphabet, \n",
    "  - and all the results of the merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To tokenize a new text, \n",
    "  - we pre-tokenize it, split it, \n",
    "  - then apply all the merge rules learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try this on any text composed of characters in the alphabet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(\"This is natural language processing BPE algorithm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram Algorithm\n",
    "\n",
    "- Used in SentencePiece, applied in models like AlBERT, T5, mBART, Big Bird, and XLNet.\n",
    "- Unlike BPE and WordPiece, Unigram starts with a large vocabulary and prunes it down.\n",
    "\n",
    "\n",
    "### **Training Algorithm**\n",
    "\n",
    "- Begins with a large vocabulary\n",
    "  - Options for building initial vocabulary:\n",
    "    - Use the most common substrings in pre-tokenized words.\n",
    "    - Apply BPE with a large vocabulary size.\n",
    "- Vocabulary Pruning\n",
    "  - Computes loss for the corpus using the current vocabulary.\n",
    "  - Calculates the impact on loss if each symbol is removed.\n",
    "  - Removes the least impactful symbols until the desired vocabulary size is achieved.\n",
    "    - Base characters are never removed to ensure any word can be tokenized.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tokenization Process**\n",
    "- **Concept**:\n",
    "  - Unigram is a language model where each token is independent.\n",
    "  - Probability of a token = (Frequency of the token) / (Sum of all frequencies in the vocabulary).\n",
    "- üçé **Example**: \n",
    "  - Given the corpus: \n",
    "    - (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "  - Initial vocabulary of all strict substrings: \n",
    "    - [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"]\n",
    "  - Subword frequencies:\n",
    "    - (\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16)\n",
    "    - (\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5)\n",
    "  - ‚à¥ the sum of all frequencies is 210\n",
    "    - the probability of the subword \"ug\" is thus 20/210\n",
    "    - the probability of the subword \"ugs\" is thus 5/210\n",
    "\n",
    "---\n",
    "\n",
    "### **Finding Optimal Tokenization**\n",
    "- **Probability Computation**:\n",
    "  - Given a word, `segment it into tokens in all the possible segmentations` \n",
    "    - compute the probability p of each according to the Unigram model\n",
    "    - all tokens are considered independent, this probability is just the product of the probability of each token\n",
    "- Tokenization with the highest probability is selected.\n",
    "- üçé **Example**\n",
    "  - Given word \"pug\", all its possible segmentations\n",
    "    - p(['p','u','g']) = p('p') √ó p('u') √ó p('g') = (5/210)√ó(36/210)√ó(20/210) = 0.000389\n",
    "    - p(['p','ug']) = p('p') √ó p('ug') = (5/210)√ó(20/210) = 0.0022676\n",
    "    - p(['pu','g']) = p('pu') √ó p('g') = (5/210)√ó(20/210) = 0.0022676\n",
    "  - Tokenizations with the least tokens possible will have the highest probability\n",
    "    - Segmenations in a tie choose the first encountered\n",
    "\n",
    "---\n",
    "\n",
    "### **Viterbi Algorithm**\n",
    "- **Purpose**:\n",
    "  - Finds the most probable segmentation for a given word.\n",
    "- **Process**:\n",
    "  - Constructs a graph of possible segmentations.\n",
    "  - Determines the best path through the graph, ending in the highest score.\n",
    "- üçé **Example**\n",
    "  - Given the previous vocabulary, for each position in `unhug`, the subwords with the `best scores ending there` are the following\n",
    "    - Character 0 (u): \"u\" (score 0.171429)\n",
    "    - Character 1 (n): \"un\" (score 0.076191)\n",
    "    - Character 2 (h): \"un\" \"h\" (score 0.005442)\n",
    "    - Character 3 (u): \"un\" \"hu\" (score 0.005442)\n",
    "    - Character 4 (g): \"un\" \"hug\" (score 0.005442)\n",
    "  - ‚à¥  \"unhug\" would be tokenized as [\"un\", \"hug\"].\n",
    "\n",
    "---\n",
    "\n",
    "### **Back to Training**\n",
    "- **Loss Computation**:\n",
    "  - `Loss` is the negative log likelihood of tokenization scores across the corpus.\n",
    "    - $\\displaystyle ùìÅ(corpus) = ‚àë_{w‚ààcorpus} -f(w)\\log(p(w))$\n",
    "- üçé **Example**: Loss computation for the previous corpus `(\"hug\", \"pug\", \"pun\", \"bun\", \"hugs\")`.\n",
    "  - \"hug\": [\"hug\"] (score 0.071428)\n",
    "  - \"pug\": [\"pu\", \"g\"] (score 0.007710)\n",
    "  - \"pun\": [\"pu\", \"n\"] (score 0.006168)\n",
    "  - \"bun\": [\"bu\", \"n\"] (score 0.001451)\n",
    "  - \"hugs\": [\"hug\", \"s\"] (score 0.001701)\n",
    "- ‚à¥ loss = 10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementing Unigram**\n",
    "\n",
    "- **Objective**: Implement the Unigram algorithm in code.\n",
    "- **Note**: This is a basic implementation, not optimized for efficiency.\n",
    "\n",
    "\n",
    "\n",
    "### **Corpus for Unigram Implementation**\n",
    "- **Tokenizer**: [`xlnet-base-cased`](https://arxiv.org/pdf/1906.08237)\n",
    "- **Example Corpus**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "\n",
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Counting Word Frequencies**\n",
    "\n",
    "- **Count the occurrences of each word**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'‚ñÅThis': 3,\n",
       "             '‚ñÅis': 2,\n",
       "             '‚ñÅthe': 1,\n",
       "             '‚ñÅHugging': 1,\n",
       "             '‚ñÅFace': 1,\n",
       "             '‚ñÅCourse.': 1,\n",
       "             '‚ñÅchapter': 1,\n",
       "             '‚ñÅabout': 1,\n",
       "             '‚ñÅtokenization.': 1,\n",
       "             '‚ñÅsection': 1,\n",
       "             '‚ñÅshows': 1,\n",
       "             '‚ñÅseveral': 1,\n",
       "             '‚ñÅtokenizer': 1,\n",
       "             '‚ñÅalgorithms.': 1,\n",
       "             '‚ñÅHopefully,': 1,\n",
       "             '‚ñÅyou': 1,\n",
       "             '‚ñÅwill': 1,\n",
       "             '‚ñÅbe': 1,\n",
       "             '‚ñÅable': 1,\n",
       "             '‚ñÅto': 1,\n",
       "             '‚ñÅunderstand': 1,\n",
       "             '‚ñÅhow': 1,\n",
       "             '‚ñÅthey': 1,\n",
       "             '‚ñÅare': 1,\n",
       "             '‚ñÅtrained': 1,\n",
       "             '‚ñÅand': 1,\n",
       "             '‚ñÅgenerate': 1,\n",
       "             '‚ñÅtokens.': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initializing Vocabulary**\n",
    "\n",
    "- **Create frequency dictionaries for characters and subwords**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # Loop through the subwords of length at least 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sort subwords by frequency**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅt', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('‚ñÅa', 5),\n",
       " ('‚ñÅto', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('‚ñÅT', 3),\n",
       " ('‚ñÅTh', 3),\n",
       " ('‚ñÅThi', 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_subwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building the Initial Vocabulary**\n",
    "\n",
    "- **Combine characters with top subwords to get an initial vocabulary of size 300**\n",
    "- **Note**: SentencePiece uses Enhanced Suffix Array (ESA) for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Converting Frequencies to Probabilities**\n",
    "\n",
    "- **Convert frequencies to probabilities using logarithms**\n",
    "  - Because it‚Äôs more numerically stable to add logarithms than to multiply small numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenizing Words with Viterbi Algorithm**\n",
    "\n",
    "- **Main function to tokenize words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    # computes the best segmentation of each substring of the word\n",
    "    # store one dictionary per position in the word (from 0 to its total length), \n",
    "    # with two keys: \n",
    "    #   - the index of the start of the last token in the best segmentation, \n",
    "    #   - and the score of the best segmentation\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    # 1. goes over each start position\n",
    "    for start_idx in range(len(word)):\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        # 2. tries all substrings beginning at that start position\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing the Tokenizer**\n",
    "\n",
    "- **Test the tokenizer with sample words**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
      "(['This'], 6.288267030694535)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing Model Loss**\n",
    "\n",
    "- **Function to compute the model's loss on the corpus**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.10377642940875"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss\n",
    "\n",
    "compute_loss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing Scores for Tokens**\n",
    "\n",
    "- **Function to compute scores by removing each token**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try it on given tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_scores(model)\n",
    "# \"ll\" is used in the tokenization of \"Hopefully\", \n",
    "# and removing it may leads to use the token \"l\" twice instead, \n",
    "# so it will have a positive loss\n",
    "print(scores[\"ll\"]) \n",
    "\n",
    "# \"his\" is only used inside the word \"This\", \n",
    "# which is tokenized as itself, \n",
    "# so it will have a zero loss\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pruning the Vocabulary**\n",
    "\n",
    "- **Loop until vocabulary size is reduced to the desired size**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Tokenization Function**\n",
    "\n",
    "- **Tokenize text using the trained model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅThis',\n",
       " '‚ñÅis',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅHugging',\n",
       " '‚ñÅFace',\n",
       " '‚ñÅ',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "tokenize(\"This is the Hugging Face course.\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ **Exercise on Token Removal**\n",
    "- **Impact on Loss**:\n",
    "  - Demonstrates how removing tokens affects the loss.\n",
    "  - Example: Removing \"pu\" vs. removing \"hug\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of the Minimum Edit Distance Algorithm\n",
    "\n",
    "The **minimum edit distance** algorithm calculates the number of edits (insertions, deletions, or substitutions) required to transform one string into another. The algorithm is commonly implemented using **dynamic programming**. The edit distance between two strings can be used to measure how similar they are, with a smaller distance indicating higher similarity.\n",
    "\n",
    "#### Types of operations:\n",
    "- **Insertion**: Insert a character into the string.\n",
    "- **Deletion**: Remove a character from the string.\n",
    "- **Substitution**: Replace one character with another.\n",
    "\n",
    "### Algorithm Steps:\n",
    "\n",
    "1. Create a matrix (or table) `dp` of size `(m+1) x (n+1)`, where `m` is the length of the first string `A` and `n` is the length of the second string `B`.\n",
    "2. Initialize the first row and column of the matrix to represent the edit distance for transforming a string into an empty string.\n",
    "3. For each character pair in both strings, fill the matrix according to the following rules:\n",
    "   - If the characters are equal, no operation is needed, so use the previous diagonal value.\n",
    "   - If the characters are different, choose the minimum of:\n",
    "     - Inserting a character,\n",
    "     - Deleting a character,\n",
    "     - Substituting a character,\n",
    "   and add 1 to the result.\n",
    "4. The value at `dp[m][n]` will be the minimum edit distance between the two strings.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Suppose we want to find the minimum edit distance between the words `\"intention\"` and `\"execution\"`.\n",
    "\n",
    "### Step-by-step Table Calculation\n",
    "\n",
    "|     |    | e  | x  | e  | c  | u  | t  | i  | o  | n  |\n",
    "|-----|----|----|----|----|----|----|----|----|----|----|\n",
    "|     |  0 | 1  | 2  | 3  | 4  | 5  | 6  | 7  | 8  | 9  |\n",
    "| i   |  1 | 1  | 2  | 3  | 4  | 5  | 5  | 6  | 7  | 8  |\n",
    "| n   |  2 | 2  | 2  | 3  | 4  | 5  | 6  | 6  | 7  | 7  |\n",
    "| t   |  3 | 3  | 3  | 3  | 4  | 5  | 5  | 6  | 7  | 8  |\n",
    "| e   |  4 | 3  | 4  | 3  | 4  | 5  | 6  | 6  | 7  | 8  |\n",
    "| n   |  5 | 4  | 4  | 4  | 4  | 5  | 6  | 7  | 7  | 7  |\n",
    "| t   |  6 | 5  | 5  | 5  | 5  | 5  | 6  | 7  | 8  | 8  |\n",
    "| i   |  7 | 6  | 6  | 6  | 6  | 6  | 6  | 6  | 7  | 8  |\n",
    "| o   |  8 | 7  | 7  | 7  | 7  | 7  | 7  | 7  | 6  | 7  |\n",
    "| n   |  9 | 8  | 8  | 8  | 8  | 8  | 8  | 8  | 7  | 6  |\n",
    "\n",
    "Here:\n",
    "- Insertion and deletion are represented by increments in row and column values.\n",
    "- Substitution occurs when the characters do not match.\n",
    "\n",
    "The final minimum edit distance between `\"intention\"` and `\"execution\"` is **5** (found in the bottom-right corner).\n",
    "\n",
    "### Python Implementation\n",
    "\n",
    "```python\n",
    "def minimum_edit_distance(A, B):\n",
    "    m, n = len(A), len(B)\n",
    "    \n",
    "    # Create a (m+1) x (n+1) DP matrix\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Initialize the first row and column\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i  # Cost of converting A[0..i] to an empty string\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j  # Cost of converting an empty string to B[0..j]\n",
    "\n",
    "    # Fill the DP table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if A[i - 1] == B[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]  # Characters match, no cost\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i - 1][j] + 1,   # Deletion\n",
    "                               dp[i][j - 1] + 1,   # Insertion\n",
    "                               dp[i - 1][j - 1] + 1)  # Substitution\n",
    "\n",
    "    # The minimum edit distance is in the bottom-right corner\n",
    "    return dp[m][n], dp\n",
    "\n",
    "# Test example\n",
    "A = \"intention\"\n",
    "B = \"execution\"\n",
    "distance, dp_table = minimum_edit_distance(A, B)\n",
    "\n",
    "# Print results\n",
    "print(f\"Minimum Edit Distance between '{A}' and '{B}' is {distance}\")\n",
    "print(\"DP Table:\")\n",
    "for row in dp_table:\n",
    "    print(row)\n",
    "```\n",
    "\n",
    "### Output:\n",
    "\n",
    "```\n",
    "Minimum Edit Distance between 'intention' and 'execution' is 5\n",
    "DP Table:\n",
    "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "[1, 1, 2, 3, 4, 5, 6, 6, 7, 8]\n",
    "[2, 2, 2, 3, 4, 5, 6, 6, 7, 7]\n",
    "[3, 3, 3, 3, 4, 5, 5, 6, 7, 8]\n",
    "[4, 3, 4, 3, 4, 5, 6, 6, 7, 8]\n",
    "[5, 4, 4, 4, 4, 5, 6, 7, 7, 7]\n",
    "[6, 5, 5, 5, 5, 5, 6, 7, 8, 8]\n",
    "[7, 6, 6, 6, 6, 6, 6, 6, 7, 8]\n",
    "[8, 7, 7, 7, 7, 7, 7, 7, 6, 7]\n",
    "[9, 8, 8, 8, 8, 8, 8, 8, 7, 6]\n",
    "```\n",
    "\n",
    "In this Python implementation, the minimum edit distance between `\"intention\"` and `\"execution\"` is calculated as **5**. The table `dp_table` shows the dynamic programming table used to compute the result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
