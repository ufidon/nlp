{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/mt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/mt.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "**Machine Translation**\n",
    "\n",
    "- üìù SALP chapter 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine translation (MT)** leverages computers to translate text between languages, focusing on `practical tasks` rather than complex literary translation.\n",
    "\n",
    "  - **Primary Uses of MT**: MT is widely used for information access, such as translating online instructions, recipes, and articles, and helps bridge the digital divide by making information more accessible to speakers of lower-resourced languages.\n",
    "  - **Computer-Aided Translation (CAT)**: MT supports human translators by generating draft translations that are refined in a post-editing phase, often as part of localization efforts.\n",
    "  - **Real-Time Communication and Image Translation**: MT now enables on-the-fly speech translation and image-based translations (e.g., translating text on menus or signs captured by a phone camera).\n",
    "  - **Encoder-Decoder Network Architecture**: MT relies on encoder-decoder networks to manage language differences, such as word order and grammatical structures, effectively mapping complex input sequences to output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Divergences and Typology\n",
    "- **Language Universals**: Despite the diversity of around 7,000 languages, some elements are universal or statistically common across languages, such as words for basic human functions and structures like nouns, verbs, questions, and commands, reflecting language's role as a communicative tool.\n",
    "\n",
    "- **Linguistic Diversity and Typology**: Languages vary significantly, especially in lexical choices and sentence structure. These differences, studied in linguistic typology, influence machine translation, as understanding both unique and systematic language differences helps improve MT models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Order Typology\n",
    "- **Word Order Variations**: Languages differ in sentence structure; \n",
    "  - SVO (e.g., English), \n",
    "  - SOV (e.g., Japanese), \n",
    "  - VSO (e.g., Arabic) \n",
    "  - Orders impact word and phrase placement, such as prepositions vs. postpositions.\n",
    "\n",
    "![word order differences](./images/mt/wo.png)\n",
    "\n",
    "- **Modifier Placement Differences**: Modifier positions vary by language, with adjectives before nouns in English but after nouns in Spanish, affecting translation structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical Divergences\n",
    "- **Word Translation Context**: Translating words accurately depends on context, as many words, like \"bass\" or \"wall,\" have multiple meanings across languages, necessitating disambiguation in machine translation.\n",
    "\n",
    "- **Grammatical Constraints**: Languages impose different grammatical rules, such as gender and plurality. For instance, translating into French requires specifying adjective gender, which may not be present in English.\n",
    "\n",
    "- **Complex Mappings and Lexical Gaps**: Some concepts translate differently depending on context (e.g., \"leg\" as body part vs. journey stage in French). Certain words lack direct equivalents across languages, leading to challenges in conveying precise meanings.\n",
    "\n",
    "![Word overlap](./images/mt/ol.png)\n",
    "\n",
    "- **Event Description Differences**: Languages vary in how they describe events, with \"verb-framed\" languages like Spanish marking direction on the verb, while \"satellite-framed\" languages like English use particles to indicate direction, impacting translation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Typology\n",
    "- **Morpheme Use**: Languages range from single-morpheme words (e.g., Vietnamese) to complex words combining many morphemes (e.g., Yupik).\n",
    "\n",
    "- **Morpheme Boundaries**: Agglutinative languages (e.g., Turkish) have clear morpheme separations, while fusion languages (e.g., Russian) combine multiple meanings in one affix, requiring subword models for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referential density\n",
    "- **Pronoun Omission**: Some languages (e.g., Spanish, Chinese, Japanese) often omit pronouns (\"pro-drop\"), requiring listeners to infer the subject, while languages like English use explicit pronouns.\n",
    "\n",
    "- **Referential Density**: Languages with frequent pronoun omission (e.g., Chinese, Japanese) are \"referentially sparse\" or \"cold,\" relying on inference, whereas more explicit languages (e.g., English) are \"referentially dense\" or \"hot.\" Translating between these types can be challenging for maintaining clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation using Encoder-Decoder\n",
    "- **MT Architecture**: The standard MT model uses an `encoder-decoder transformer` (`sequence-to-sequence`) to generate target language sentences from source language sentences independently.\n",
    "\n",
    "- **Objective**: MT systems are trained with supervised learning on parallel sentences, maximizing the probability of target tokens $P(y_1, \\dots, y_m | x_1, \\dots, x_n)$ given source tokens $x_1, \\dots, x_n$.\n",
    "\n",
    "- **Encoder-Decoder Process**: The encoder produces an intermediate context $ùê° = \\text{encoder}(x)$ , and the decoder uses $ùê°$ to generate each output token sequentially, $y_{t+1} = \\text{decoder}(ùê°, y_1, \\dots, y_t)$ for $t \\in [1, \\dots, m]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "- **Subword Tokenization**: MT uses shared subword tokenization for source and target languages, enabling translation between languages with different word-separation rules.\n",
    "\n",
    "- **Wordpiece Algorithm**: Wordpiece tokenization, used in BERT, builds vocabulary by merging tokens to maximize language model probability, up to a specified vocabulary size.\n",
    "\n",
    "- **Unigram (SentencePiece) Algorithm**: Unigram tokenization starts with a large vocabulary and reduces it by removing low-probability tokens, creating more meaningful subwords.\n",
    "\n",
    "- **Unigram Advantage**: Unigram tokenization captures semantically relevant tokens better than BPE, avoiding overly small or common token fragments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training data\n",
    "- **Parallel Corpora**: MT models are trained on parallel corpora (bitexts), with large datasets like Europarl, the UN Parallel Corpus, and OpenSubtitles providing millions of sentence pairs in multiple languages.\n",
    "\n",
    "- **Sentence Alignment** takes sentences $e_1, ‚ãØ, e_n$, and $f_1 , ‚ãØ, f_n$ and finds minimal sets of sentences that are translations of each other, including \n",
    "  - single sentence mappings like $(e_1 ,f_1), (e_4 ,f_3), (e_5 ,f_4), (e_6 ,f_6)$ \n",
    "  - as well as 2-1 alignments $(e_2/e_3 ,f_2), (e_7 /e_8 ,f_7)$, \n",
    "  - and null alignments $(f_5 )$.\n",
    "  - `Sentence Alignment` for new corpora requires a cost function to score translation likelihood and an alignment algorithm, often using dynamic programming based on the minimum edit distance.\n",
    "\n",
    "![A sample alignment between sentences in English and French](./images/mt/align.png)\n",
    "\n",
    "- **Multilingual Embedding**: Sentence similarity is scored using cosine similarity in a multilingual embedding space, with the [cost function](https://aclanthology.org/D19-1136.pdf) helping to align sentence spans.\n",
    "\n",
    "- **Corpus Cleanup**: Noisy sentence pairs are removed through rules or by ranking pairs based on their multilingual cosine scores to ensure high-quality training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the Encoder-Decoder Model\n",
    "- **Encoder-Decoder Transformer Architecture**: The standard architecture for MT is the encoder-decoder transformer, consisting of an encoder (standard transformer) and a decoder with an additional cross-attention layer to attend to the source language.\n",
    "\n",
    "![The encoder-decoder transformer architecture for machine translation](./images/mt/de.png)\n",
    "\n",
    "- **Decoding Process**: The decoder generates target language words one by one, conditioned on the source sentence and previously generated words, using techniques like beam search for decoding.\n",
    "\n",
    "- **Cross-Attention Layer**: The decoder includes a cross-attention layer where queries come from the previous decoder layer, and keys and values come from the encoder's output, allowing the decoder to focus on source language tokens.\n",
    "\n",
    "- **Attention Mechanism**: The attention mechanism in the decoder is a mix of cross-attention (to the encoder's output) and causal (left-to-right) multi-head attention, while the encoder‚Äôs multi-head attention can look at the entire source text.\n",
    "\n",
    "![The transformer block for the encoder and the decoder.](./images/mt/dbblk.png)\n",
    "\n",
    "- **Training and Loss Function**: The model is trained autoregressively using cross-entropy loss, with teacher forcing where the decoder is given the actual target token from the training data at each time step, not the model‚Äôs own prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding in MT: Beam Search\n",
    "- **Greedy Decoding Limitation**: Greedy decoding selects the word with the highest probability at each timestep, but it can make wrong choices since it doesn‚Äôt consider future context: \n",
    "  - `yes yes` instead of `ok ok` is generated.\n",
    "  \n",
    "  ![Greedy Decoding Limitation](./images/mt/greedy.png)\n",
    "  \n",
    "  - which beam search addresses by keeping multiple hypotheses.\n",
    "\n",
    "- **Beam Search** is a heuristic search method that keeps k-best possible tokens at each timestep, where k is the beam width, helping balance memory usage and computation.\n",
    "\n",
    "![Beam search decoding with a beam width of k = 2.](./images/mt/beam.png)\n",
    "\n",
    "- **Hypothesis Extension**: At each step, k-best hypotheses are extended by generating all possible next tokens and scoring them based on the probability of the current word and the previous path, pruning to keep only the k-best.\n",
    "\n",
    "- **Log Probability Scoring**: The score of each hypothesis is computed using the chain rule of probability, where the log probability of the full sequence is the sum of the log probabilities of each word conditioned on previous words.\n",
    "\n",
    "- **Handling Different Lengths**: Completed hypotheses might have different lengths, so length normalization methods are used, such as dividing the log probability by the number of words to adjust for language models' tendency to prefer shorter sequences.\n",
    "\n",
    "- **Decoding Process**: Beam search continues until an EOS (End Of Sentence) token is generated, indicating a complete translation. The size of the beam is reduced progressively as hypotheses are completed.\n",
    "\n",
    "- **Final Selection**: The result of beam search is a set of k hypotheses, and the most probable one can be selected for the final translation, or all k hypotheses can be passed to downstream applications.\n",
    "\n",
    "![Scoring for beam search decoding with a beam width of k = 2.](./images/mt/score.png)\n",
    "\n",
    "- **Beam Width in MT**: Typical beam widths for machine translation are between 5 and 10, with each width offering a trade-off between computational cost and translation quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Bayes Risk Decoding\n",
    "- **Minimum Bayes Risk (MBR) decoding** chooses the translation with the least expected error, aiming to maximize a `goodness-of-fit metric` (e.g., chrF, BERTScore) rather than just the highest probability translation.\n",
    "\n",
    "- **Approximating Perfect Translations**: Since the perfect set of translations is unknown, MBR uses a smaller set of candidate translations, selecting the one that is most similar to all others, based on a similarity or alignment function.\n",
    "\n",
    "- **Application in NLP**: MBR decoding, effective in machine translation, has also been successfully applied to other NLP tasks such as speech recognition, summarization, dialogue systems, and image captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating in low-resource situations\n",
    "- **Data Scarcity**: Many languages lack large parallel corpora, especially for low-resource domains.\n",
    "  - **Backtranslation**: Uses monolingual data to generate synthetic parallel text, improving translation for low-resource languages.\n",
    "    - **Backtranslation Effectiveness**: It works well, providing about 2/3 of the gain compared to training with natural bitext.\n",
    "- **Data Quality**: Many parallel corpora for low-resource languages suffer from poor quality due to insufficient native speaker input.\n",
    "  - **Multilingual Models**: Use multiple language pairs to improve translation for low-resource languages by leveraging related, higher-resource languages.\n",
    "    - **Multilingual Data Quality**: Large multilingual models can improve translations but often rely on English-centered corpora.\n",
    "  - **Participatory Design**: Involves native speakers and local experts in developing MT systems for low-resource languages.\n",
    "  - **Evaluation Methods**: Post-editing MT output is suggested for better error measurement and evaluation in low-resource languages.\n",
    "  - **Improved MT Models**: New initiatives are expanding multilingual systems to cover more languages and improve translation quality.\n",
    "- **Socio-Technical Issues**: Low-resource language projects often lack native speaker involvement in content curation and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MT Evaluation](https://machinetranslate.org/metrics)\n",
    "- MT is evaluated on \n",
    "  - **adequacy**: how well the translation conveys the meaning,\n",
    "  - **fluency**: how natural and grammatically correct the translation is.\n",
    "\n",
    "- **Using Human Raters to Evaluate MT**  \n",
    "  - Human raters assess translations based on adequacy and fluency using scales or rankings.\n",
    "  - Training is necessary for raters to distinguish between fluency and adequacy, and to standardize evaluations.\n",
    "  - Post-editing translations is another method to evaluate quality, measuring the difference between original MT output and post-edited text.\n",
    "\n",
    "- **Automatic Evaluation**  \n",
    "  - **[chrF (character F-score)](https://huggingface.co/spaces/evaluate-metric/chrf)** is a robust metric based on character n-gram overlap, and is often more reliable than other metrics.\n",
    "  - **[BLEU (Bilingual Evaluation Understudy)](https://huggingface.co/spaces/evaluate-metric/bleu)** is another popular word-based overlap metric but has limitations in languages with complex morphology or different tokenization.\n",
    "  - **Statistical Significance Testing** using methods like the paired bootstrap test helps assess the significance of differences in scores between two systems.\n",
    "\n",
    "- **Automatic Evaluation: Embedding-Based Methods**  \n",
    "  - Embedding-based metrics like **[BERTSCORE](https://huggingface.co/spaces/evaluate-metric/bertscore)** measure translation quality based on token similarity in embeddings.\n",
    "  \n",
    "  ![The computation of BERTSCORE recall from reference x and candidate xÃÇ](./images/mt/bertscore.png)\n",
    "  \n",
    "  - **[COMET](https://unbabel.github.io/COMET/html/index.html)** and **[BLEURT](https://huggingface.co/spaces/evaluate-metric/bleurt)** are trained on human-labeled datasets to predict translation quality.\n",
    "  - These embedding-based methods address the issue of synonyms and paraphrasing by considering semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install Huggingface core libraries\n",
    "!pip install tokenizers transformers datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/learn/nlp-course/chapter7\n",
    "# 1. Explore a dataset for translating Chinese to English\n",
    "# https://huggingface.co/datasets/suolyer/translate_zh2en\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"suolyer/translate_zh2en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\n",
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets[\"validation\"] = split_datasets.pop(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Explore a model for translation\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "model=\"Helsinki-NLP/opus-mt-zh-en\"\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "translator('‰ªäÂ§©ÊòØ‰∏™Â•ΩÊó•Â≠ê„ÄÇ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator('Âï•Ôºü‰ªäÂ§©ÊòØÂ•ΩÊó•Â≠ê„ÄÇÈ¨ºÊâç‰ø°„ÄÇ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator('‰∏§Â≤∏ÁåøÂ£∞Âïº‰∏ç‰ΩèÔºåËΩªËàüÂ∑≤Ëøá‰∏áÈáçÂ±±„ÄÇ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Process the data\n",
    "# The Helsinki-NLP organization provides more than a thousand models in multiple languages.\n",
    "from transformers import AutoTokenizer\n",
    "model=\"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, return_tensors=\"pt\")\n",
    "\n",
    "zh_sentence = split_datasets[\"train\"][1][\"input\"]\n",
    "en_sentence = split_datasets[\"train\"][1][\"output\"]\n",
    "\n",
    "inputs = tokenizer(zh_sentence, text_target=en_sentence)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_sentence, en_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrong tokenization: tokenize English sentence with a Chinese tokenizer\n",
    "# It results in a lot more tokens due the Chinese tokenizer does't know any English words\n",
    "wrong_targets = tokenizer(en_sentence)\n",
    "print(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function we will apply on the datasets:\n",
    "max_length = 128\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    targets = examples['output']\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, text_target=targets, max_length=max_length, truncation=True\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply that preprocessing in one go on all the splits of our dataset:\n",
    "tokenized_datasets = split_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=split_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuing the model with the Trainer API\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with the padding for dynamic batching by data collator\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on a fiew samples\n",
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the padding value used to pad the labels should be -100\n",
    "# not the padding token of the tokenizer,\n",
    "# to make sure those padded values are ignored in the loss computation.\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the decoder input IDs are shifted versions of the labels\n",
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Metrics\n",
    "# - [sacreBLEU](https://github.com/mjpost/sacrebleu)\n",
    "\n",
    "!pip install sacrebleu evaluate\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good prediction\n",
    "# The score can go from 0 to 100, and higher is better.\n",
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad predictions\n",
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model outputs to texts the metric can use\n",
    "# clean up all the -100s in the labels\n",
    "# the tokenizer will automatically do the same for the padding token\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fine-tuning the model\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"finetuned-Helsinki-NLP-opus-mt-zh-en\",\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate before training to give a baseline\n",
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate after training to see any improvement\n",
    "trainer.evaluate(max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. A custom training loop\n",
    "# Preparing everything for training\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint=\"Helsinki-NLP/opus-mt-zh-en\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # Save and upload\n",
    "    output_dir = './'\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Training in progress epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Using the fine-tuned model for inference\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
    "\n",
    "# Prepare input text for translation\n",
    "input_text = \"\"\"\n",
    "ÊúõÂ∫êÂ±±ÁÄëÂ∏É\n",
    "Âîê¬∑ÊùéÁôΩ\n",
    "Êó•ÁÖßÈ¶ôÁÇâÁîüÁ¥´ÁÉüÔºå\n",
    "ÈÅ•ÁúãÁÄëÂ∏ÉÊåÇÂâçÂ∑ù„ÄÇ\n",
    "È£ûÊµÅÁõ¥‰∏ã‰∏âÂçÉÂ∞∫Ôºå\n",
    "ÁñëÊòØÈì∂Ê≤≥ËêΩ‰πùÂ§©„ÄÇ\n",
    "\"\"\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate translation\n",
    "with torch.no_grad():\n",
    "    translated_tokens = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=128,\n",
    "        num_beams=4\n",
    "    )\n",
    "\n",
    "# Decode and print the translation\n",
    "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "print(f\"Translated text: {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
