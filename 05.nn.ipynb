{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/05.nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/05.nn.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# Neural Networks\n",
    "\n",
    "ðŸ“ SALP chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "- A neural network is a `machine learning model` inspired by biological neurons. \n",
    "- It consists of `layers of connected nodes (neurons)` that process and transform input data.\n",
    "  - **Input Layer:** Receives input features.\n",
    "  - **Hidden Layers:** Transforms the input through weights and activation functions.\n",
    "  - **Output Layer:** Produces final output or predictions.\n",
    "- **Applications:** Image recognition, natural language processing (NLP), speech recognition.\n",
    "\n",
    "---\n",
    "\n",
    "### **Neuron (Unit)**\n",
    "- A single processing unit in a neural network that computes the `weighted sum of its inputs` and applies a `non-linear activation function`.\n",
    "  - $z = \\sigma(w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b) = \\sigma(\\mathbf{w}^T \\mathbf{x} + b)$\n",
    "    - $\\mathbf{x} = [x_1, x_2, \\dots, x_n]$ is the input vector.\n",
    "    - $\\mathbf{w} = [w_1, w_2, \\dots, w_n]$ is the weight vector.\n",
    "    - $b$ is the bias.\n",
    "    - $\\sigma(\\cdot)$ is the activation function (e.g., sigmoid, ReLU, tanh).\n",
    "- **Common Activation Functions:**\n",
    "  - **Sigmoid:** $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$\n",
    "  - **ReLU:** $\\sigma(z) = \\max(0, z)$\n",
    "  - **Tanh:** $\\sigma(z) = \\dfrac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **The XOR Problem**\n",
    "- A binary classification problem where two inputs are classified as `1` if both inputs are different.\n",
    "  - $\\text{XOR} (x_1, x_2) = \\begin{cases}\n",
    "  1, & \\text{if } x_1 \\neq x_2 \\\\\n",
    "  0, & \\text{otherwise}\n",
    "  \\end{cases}$\n",
    "- It is **non-linearly separable**:\n",
    "  - i.e. it cannot be solved by a single-layer of  neurons.\n",
    "  - can be solved with **multi-layer neural network** by learning complex non-linear relationships.\n",
    "- It shows that the need for `multi-layer networks with hidden layers` to solve non-linear problems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Feedforward Neural Networks (FNNs)**\n",
    "- Where information flows in one direction â€” from input to output, without cycles.\n",
    "- **Architecture:**\n",
    "  - **Input Layer â†’ Hidden Layers â†’ Output Layer.**\n",
    "  - Each neuron in a layer is connected to every neuron in the next layer.\n",
    "- **Mathematics of Feedforward Pass:**\n",
    "  - $h^{(l)} = \\sigma(\\mathbf{W}^{(l)} h^{(l-1)} + \\mathbf{b}^{(l)})$\n",
    "    - $h^{(l)}$ is the output of layer $l$ or input data if it's the first layer.\n",
    "    - $\\mathbf{W}^{(l)}$ and $\\mathbf{b}^{(l)}$ are weights and biases for layer $l$.\n",
    "    - $\\sigma$ is the activation function.\n",
    "  - The final output layer computes the predicted value $\\hat{y}$:\n",
    "    - $\\hat{y} = \\sigma(\\mathbf{W}^{(L)} h^{(L-1)} + \\mathbf{b}^{(L)})$\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Neural Networks**\n",
    "- **Goal:** Minimize the difference between predicted output ($\\hat{y}$) and actual label ($y$) using a loss function.\n",
    "  \n",
    "- **Loss Function (Binary Classification):**\n",
    "  - $\\mathcal{L}(\\hat{y}, y) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]$\n",
    "\n",
    "- **Training via Backpropagation:**\n",
    "  - **Forward Pass:** Calculate predictions.\n",
    "  - **Backward Pass:** Compute gradients of the loss function with respect to weights.\n",
    "  - **Update Weights:** Using gradient descent.\n",
    "    - $w := w - \\eta \\dfrac{\\partial \\mathcal{L}}{\\partial w}$\n",
    "  Where $\\eta$ is the learning rate.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimization Algorithms**\n",
    "- **Stochastic Gradient Descent (SGD):**\n",
    "  - Updates weights using one training example at a time.\n",
    "    - $w := w - \\eta \\cdot \\dfrac{\\partial \\mathcal{L}}{\\partial w}$\n",
    "\n",
    "- **Mini-Batch Gradient Descent:**\n",
    "  - Instead of one sample or the whole dataset, updates are done using a `small batch` of data.\n",
    "  - Combines the efficiency of SGD with more stable updates.\n",
    "\n",
    "- **Adam Optimizer (Adaptive Moment Estimation):**\n",
    "  - Uses both the first moment (mean) and second moment (variance) of gradients for better convergence.\n",
    "    - $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "  \\quad \\text{and} \\quad\n",
    "  v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n",
    "    - $w := w - \\eta \\cdot \\dfrac{m_t}{\\sqrt{v_t} + \\epsilon}$\n",
    "      - $g_t$ is the gradient at time $t$,\n",
    "      - $m_t$ and $v_t$ are estimates of the first and second moments of the gradient.\n",
    "\n",
    "- **Comparison of Optimization Algorithms:**\n",
    "  - SGD: Faster updates, may oscillate.\n",
    "  - Adam: Smoother convergence, especially for noisy or sparse gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### ðŸŽ **Backward Differentiation of Sigmoid**\n",
    "- Consider a neural network with:\n",
    "  - Input: $x = [x_1, x_2]$\n",
    "  - Weights: $w = [w_1, w_2]$\n",
    "  - Activation function: Sigmoid.\n",
    "\n",
    "  **Forward Pass:**\n",
    "  - $z = w_1 x_1 + w_2 x_2 + b \\quad \\text{and} \\quad \\hat{y} = \\sigma(z)$\n",
    "\n",
    "  **Backward Pass (Error Term $\\delta$):**\n",
    "  1. Compute the gradient of the loss with respect to the output $\\hat{y}$:\n",
    "  - $\\delta = \\dfrac{\\partial \\mathcal{L}}{\\partial \\hat{y}} \\cdot \\dfrac{\\partial \\hat{y}}{\\partial z} = (\\hat{y} - y) \\cdot \\hat{y}(1 - \\hat{y})$\n",
    "\n",
    "  2. Compute the gradients for weights $w_1$ and $w_2$:\n",
    "  - $\\dfrac{\\partial \\mathcal{L}}{\\partial w_1} = \\delta \\cdot x_1\n",
    "  \\quad \\text{and} \\quad \\dfrac{\\partial \\mathcal{L}}{\\partial w_2} = \\delta \\cdot x_2$\n",
    "\n",
    "- **Weight Update:**\n",
    "  - $w_1 := w_1 - \\eta \\cdot \\dfrac{\\partial \\mathcal{L}}{\\partial w_1}\n",
    "  \\quad \\text{and} \\quad w_2 := w_2 - \\eta \\cdot \\dfrac{\\partial \\mathcal{L}}{\\partial w_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ **Feedforward Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate a dataset for binary classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a simple feedforward neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape=(20,)))  # Input layer + hidden layer\n",
    "model.add(Dense(8, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ **Backward Differentiation of Softmax**\n",
    "#### **Network Structure:**\n",
    "1. **Input layer (x):** $x_1, x_2$ (2 input neurons)\n",
    "2. **Hidden layer (h):** $h_1, h_2$ (2 hidden neurons, ReLU activation)\n",
    "3. **Output layer (y):** $y_1, y_2$ (2 output neurons, softmax activation)\n",
    "\n",
    "#### **Given Data:**\n",
    "- Input: $x = [x_1, x_2] = [0.5, 0.2]$\n",
    "- Initial weights:\n",
    "  - Input to hidden weights: \n",
    "    $W_1 = \\begin{bmatrix}\n",
    "    0.1 & 0.3 \\\\\n",
    "    0.4 & 0.7\n",
    "    \\end{bmatrix}$\n",
    "  - Hidden to output weights:\n",
    "    $W_2 = \\begin{bmatrix}\n",
    "    0.2 & 0.6 \\\\\n",
    "    0.5 & 0.9\n",
    "    \\end{bmatrix}$\n",
    "- Biases:\n",
    "  - Hidden layer biases: $b_1 = [0.1, 0.2]$\n",
    "  - Output layer biases: $b_2 = [0.1, 0.2]$\n",
    "\n",
    "### **1. Forward Pass:**\n",
    "\n",
    "#### **Step 1: Input to Hidden Layer Calculation**\n",
    "Each hidden neuron $h_j$ (before applying activation) is calculated as:\n",
    "\n",
    "- $h_j = x_1 W_1[1, j] + x_2 W_1[2, j] + b_1[j]$\n",
    "\n",
    "For the hidden neurons:\n",
    "\n",
    "- $h_1 = 0.5 \\times 0.1 + 0.2 \\times 0.4 + 0.1 = 0.05 + 0.08 + 0.1 = 0.23$\n",
    "- $h_2 = 0.5 \\times 0.3 + 0.2 \\times 0.7 + 0.2 = 0.15 + 0.14 + 0.2 = 0.49$\n",
    "\n",
    "#### **Step 2: Apply ReLU Activation Function to Hidden Layer**\n",
    "The ReLU activation function is:\n",
    "\n",
    "- $\\text{ReLU}(z) = \\max(0, z)$\n",
    "\n",
    "Thus, for the hidden neurons:\n",
    "\n",
    "- $h_1 = \\text{ReLU}(0.23) = 0.23$\n",
    "- $h_2 = \\text{ReLU}(0.49) = 0.49$\n",
    "\n",
    "#### **Step 3: Hidden to Output Layer Calculation**\n",
    "Each output neuron $y_k$ (before applying softmax) is calculated as:\n",
    "\n",
    "- $y_k = h_1 W_2[1, k] + h_2 W_2[2, k] + b_2[k]$\n",
    "\n",
    "For the output neurons:\n",
    "- $y_1 = 0.23 \\times 0.2 + 0.49 \\times 0.5 + 0.1 = 0.046 + 0.245 + 0.1 = 0.391$\n",
    "- $y_2 = 0.23 \\times 0.6 + 0.49 \\times 0.9 + 0.2 = 0.138 + 0.441 + 0.2 = 0.779$\n",
    "\n",
    "#### **Step 4: Apply Softmax Activation Function to Output Layer**\n",
    "The softmax function converts the output into probabilities:\n",
    "\n",
    "- $\\text{softmax}(y_i) = \\dfrac{e^{y_i}}{\\sum_{j} e^{y_j}}$\n",
    "\n",
    "Thus:\n",
    "\n",
    "- $\\text{softmax}(y_1) = \\dfrac{e^{0.391}}{e^{0.391} + e^{0.779}} = \\dfrac{1.478}{1.478 + 2.18} = \\dfrac{1.478}{3.658} = 0.404$\n",
    "- $\\text{softmax}(y_2) = \\dfrac{e^{0.779}}{e^{0.391} + e^{0.779}} = \\dfrac{2.18}{3.658} = 0.596$\n",
    "\n",
    "Thus, the output probabilities are:\n",
    "\n",
    "- $\\hat{y} = [0.404, 0.596]$\n",
    "\n",
    "### **2. Backward Pass:**\n",
    "\n",
    "#### **Step 1: Compute Loss**\n",
    "For a multi-class classification, the cross-entropy loss is used:\n",
    "\n",
    "- $\\mathcal{L} = -\\sum_{i} y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "Assuming the true labels are $y = [1, 0]$ (i.e., class 1 is the true class), the loss becomes:\n",
    "\n",
    "- $\\mathcal{L} = -(1 \\times \\log(0.404) + 0 \\times \\log(0.596)) = -\\log(0.404) = 0.906$\n",
    "\n",
    "#### **Step 2: Backpropagation (Compute Gradients)**\n",
    "We compute gradients with respect to each weight and update them using gradient descent.\n",
    "\n",
    "The derivative of the loss with respect to output $y_1$:\n",
    "\n",
    "- $\\dfrac{\\partial \\mathcal{L}}{\\partial y_1} = \\hat{y}_1 - y_1 = 0.404 - 1 = -0.596$\n",
    "\n",
    "For $y_2$:\n",
    "\n",
    "- $\\dfrac{\\partial \\mathcal{L}}{\\partial y_2} = \\hat{y}_2 - y_2 = 0.596 - 0 = 0.596$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ Implementation of this Simple Neural Network in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Input data\n",
    "x = np.array([0.5, 0.2])\n",
    "\n",
    "# Weights and biases (initial)\n",
    "W1 = np.array([[0.1, 0.3], [0.4, 0.7]])  # Input to hidden weights\n",
    "b1 = np.array([0.1, 0.2])               # Hidden layer bias\n",
    "\n",
    "W2 = np.array([[0.2, 0.6], [0.5, 0.9]])  # Hidden to output weights\n",
    "b2 = np.array([0.1, 0.2])               # Output layer bias\n",
    "\n",
    "# Forward pass\n",
    "\n",
    "# Step 1: Input to hidden\n",
    "h = np.dot(x, W1) + b1\n",
    "h = np.maximum(0, h)  # ReLU activation\n",
    "\n",
    "# Step 2: Hidden to output\n",
    "y = np.dot(h, W2) + b2\n",
    "\n",
    "# Apply softmax\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "output = softmax(y)\n",
    "print(\"Output probabilities:\", output)\n",
    "\n",
    "# Assume the true label is [1, 0] for the first class\n",
    "y_true = np.array([1, 0])\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "loss = -np.sum(y_true * np.log(output))\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Backward pass (compute gradients)\n",
    "grad_y = output - y_true  # Gradient of loss w.r.t. output\n",
    "\n",
    "# Gradient w.r.t. W2 and b2\n",
    "grad_W2 = np.outer(h, grad_y)\n",
    "grad_b2 = grad_y\n",
    "\n",
    "# Backpropagate through ReLU (only pass gradients where h > 0)\n",
    "grad_h = np.dot(W2, grad_y)\n",
    "grad_h[h <= 0] = 0  # ReLU derivative\n",
    "\n",
    "# Gradient w.r.t. W1 and b1\n",
    "grad_W1 = np.outer(x, grad_h)\n",
    "grad_b1 = grad_h\n",
    "\n",
    "# Print gradients\n",
    "print(\"Gradients for W2:\", grad_W2)\n",
    "print(\"Gradients for W1:\", grad_W1)\n",
    "\n",
    "# Update weights using gradient descent (learning rate = 0.01)\n",
    "learning_rate = 0.01\n",
    "W1 -= learning_rate * grad_W1\n",
    "b1 -= learning_rate * grad_b1\n",
    "W2 -= learning_rate * grad_W2\n",
    "b2 -= learning_rate * grad_b2\n",
    "\n",
    "# Updated weights\n",
    "print(\"Updated W1:\", W1)\n",
    "print(\"Updated W2:\", W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ In PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the 3-layer neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(2, 2)  # Input layer to hidden layer (2 inputs, 2 hidden units)\n",
    "        self.fc1.weight.data = torch.tensor([[0.1, 0.3], [0.4, 0.7]], dtype=torch.float32)\n",
    "        self.fc1.bias.data = torch.tensor([0.1, 0.2], dtype=torch.float32)\n",
    "\n",
    "        self.fc2 = nn.Linear(2, 2)  # Hidden layer to output layer (2 hidden units, 2 outputs)\n",
    "        self.fc2.weight.data = torch.tensor([[0.2, 0.6], [0.5, 0.9]], dtype=torch.float32)\n",
    "        self.fc2.bias.data = torch.tensor([0.1, 0.2], dtype=torch.float32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Hidden layer with ReLU activation\n",
    "        h = F.relu(self.fc1(x))\n",
    "        # Output layer with Softmax activation\n",
    "        y = F.softmax(self.fc2(h), dim=1)\n",
    "        return y\n",
    "\n",
    "# Create a sample input (batch size of 1, 2 input features)\n",
    "x = torch.tensor([[0.5, 0.2]], dtype=torch.float32)\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output after forward pass:\", output)\n",
    "\n",
    "# Let's also check the parameters (weights and biases)\n",
    "print(\"\\nModel Parameters (Before Training):\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data}\")\n",
    "\n",
    "# Example target (true output, one-hot encoded as class index)\n",
    "target = torch.tensor([0])  # Let's assume the true class is the first one\n",
    "\n",
    "# Define loss function (Cross-Entropy Loss)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "# Compute loss\n",
    "loss = loss_fn(output, target)\n",
    "print(\"\\nInitial Loss:\", loss.item())\n",
    "\n",
    "# Backward pass (compute gradients)\n",
    "loss.backward()\n",
    "\n",
    "# Optimizer: SGD (Stochastic Gradient Descent)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Update parameters based on gradients\n",
    "optimizer.step()\n",
    "\n",
    "# After one step of training, let's print the updated weights\n",
    "print(\"\\nModel Parameters (After One Training Step):\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.data}\")\n",
    "\n",
    "# Optionally: Run multiple iterations (epochs) for training\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = loss_fn(output, target)\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Backward pass (compute gradients)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Final output after training\n",
    "output = model(x)\n",
    "print(\"\\nFinal Output after Training:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feedforward Neural Language Model (FFNLM)**\n",
    "- Predicts the next word in a sequence, given previous words.\n",
    "- **Model Structure:**\n",
    "  - **Input:** Vector representations of words (usually via embeddings like Word2Vec or GloVe).\n",
    "  - **Hidden Layers:** Process input through non-linear transformations.\n",
    "  - **Output Layer:** Produces a probability distribution over possible next words using softmax.\n",
    "- **Mathematics:**\n",
    "  - $P(w_t | w_{t-1}, w_{t-2}, \\dots, w_{t-n}) = \\text{softmax}(\\mathbf{W}^{(L)} h^{(L-1)} + \\mathbf{b}^{(L)})$\n",
    "    - $P(w_t)$ is the predicted probability for the next word $w_t$.\n",
    "- **Loss Function (Cross-Entropy):**\n",
    "  - $\\displaystyle\\mathcal{L}(w_t) = - \\sum_{i=1}^{N} y_i \\log(\\hat{y}_i)$\n",
    "    - $y_i$ is the true label and $\\hat{y}_i$ is the predicted probability for word $i$.\n",
    "\n",
    "### ðŸŽ Implement a FFNLM in PyTorch\n",
    "- The `nn.Embedding` layer takes word indices as input and maps them to dense vectors.\n",
    "   - Input: `[word index 1, word index 2]` (2 previous words in context).\n",
    "   - Output: A concatenated embedding vector of the 2 words.\n",
    "- The model predicts the next word based on the previous two words $w_{t-1}$ and $w_{t-2}$:\n",
    "  - $P(w_t | w_{t-1}, w_{t-2}) = \\text{softmax}(\\mathbf{W}^{(2)} h^{(1)} + \\mathbf{b}^{(2)})$\n",
    "    - $h^{(1)} = \\text{ReLU}(\\mathbf{W}^{(1)} [\\text{Embed}(w_{t-1}), \\text{Embed}(w_{t-2})] + \\mathbf{b}^{(1)})$\n",
    "    - $\\mathbf{W}^{(1)}$, $\\mathbf{W}^{(2)}$, $\\mathbf{b}^{(1)}$, and $\\mathbf{b}^{(2)}$ are the learned weights and biases.\n",
    "- For a more realistic language model, try larger datasets and different embeddings like Word2Vec or GloVe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Feedforward Neural Language Model\n",
    "class FeedforwardNeuralLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, context_size):\n",
    "        super(FeedforwardNeuralLM, self).__init__()\n",
    "        \n",
    "        # Embedding layer to convert word indices into vectors\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Feedforward network layers\n",
    "        self.fc1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # Convert input word indices into embeddings\n",
    "        embeds = self.embeddings(inputs).view(1, -1)\n",
    "        \n",
    "        # Pass embeddings through the first fully connected layer\n",
    "        out = F.relu(self.fc1(embeds))\n",
    "        \n",
    "        # Final output layer to predict next word probabilities\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        \n",
    "        return log_probs\n",
    "\n",
    "# Sample data: let's say we have a simple vocabulary\n",
    "vocab = ['i', 'like', 'to', 'play', 'football', 'and', 'tennis']\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "\n",
    "# Model hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 10   # Size of word embeddings\n",
    "hidden_dim = 20      # Size of hidden layer\n",
    "context_size = 2     # How many previous words to consider (context)\n",
    "\n",
    "# Create a language model instance\n",
    "model = FeedforwardNeuralLM(vocab_size, embedding_dim, hidden_dim, context_size)\n",
    "\n",
    "# Define loss function (negative log likelihood) and optimizer\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training data: context-target pairs\n",
    "data = [\n",
    "    (['i', 'like'], 'to'),\n",
    "    (['like', 'to'], 'play'),\n",
    "    (['to', 'play'], 'football'),\n",
    "    (['play', 'football'], 'and'),\n",
    "    (['football', 'and'], 'tennis')\n",
    "]\n",
    "\n",
    "# Prepare the inputs (context words) and targets (next word)\n",
    "def make_context_vector(context, word_to_idx):\n",
    "    return torch.tensor([word_to_idx[w] for w in context], dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for context, target in data:\n",
    "        # Convert context and target to tensor format\n",
    "        context_vector = make_context_vector(context, word_to_idx)\n",
    "        target_idx = torch.tensor([word_to_idx[target]], dtype=torch.long)\n",
    "\n",
    "        # Zero the gradients from the previous iteration\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward pass: get predicted log probabilities\n",
    "        log_probs = model(context_vector)\n",
    "\n",
    "        # Compute loss: how much did we deviate from the true label?\n",
    "        loss = loss_function(log_probs, target_idx)\n",
    "        \n",
    "        # Backpropagate and update the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss for reporting\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n",
    "\n",
    "# Test the model: predict the next word after the context ['like', 'to']\n",
    "with torch.no_grad():\n",
    "    context = ['like', 'to']\n",
    "    context_vector = make_context_vector(context, word_to_idx)\n",
    "    predicted_log_probs = model(context_vector)\n",
    "    predicted_word_idx = torch.argmax(predicted_log_probs, dim=1).item()\n",
    "    predicted_word = idx_to_word[predicted_word_idx]\n",
    "    print(f\"Given context: {context}, Predicted next word: {predicted_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
