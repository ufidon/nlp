{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/07.trans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/07.trans.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# The Transformer\n",
    "\n",
    "📝 SALP chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Transformer Overview**\n",
    "\n",
    "- **Transformers** are the core architecture for **large language models (LLMs)**\n",
    "  - Which are revolutionizing speech and language processing.\n",
    "- Focus: **Left-to-right (causal) language modeling**, where tokens are predicted sequentially based on prior context.\n",
    "- The key mechanism: **Self-attention** (multi-head attention) allows the model to integrate information from surrounding tokens to capture long-range dependencies and contextual relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformer Architecture Components**\n",
    "\n",
    "- **1. Transformer Blocks**: \n",
    "  - Each block contains **multi-head attention**, a **feedforward network**, and **layer normalization**.\n",
    "  - A series of blocks maps input tokens $𝐱_i = (𝐱_1, ..., 𝐱_n)$ to output tokens $(𝐡_i = 𝐡_1, ..., 𝐡_n)$, allowing deep processing over multiple layers.\n",
    "  \n",
    "- **2. Input Encoding**: \n",
    "  - Transforms input tokens into **contextual vectors** using an **embedding matrix $𝐄$** and **positional encoding** to capture token order.\n",
    "\n",
    "- **3. Language Modeling Head**: \n",
    "  - Projects hidden states through an **unembedding matrix $𝐔$**, applying **softmax** over the vocabulary to generate a token prediction.\n",
    "\n",
    "![The architecture of a (left-to-right) transformer](./images/trans/lrtrans.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps in Transformer Exploration**\n",
    "\n",
    "- **Multi-head attention** and detailed transformer block mechanisms are covered in upcoming chapters.\n",
    "- **Pretraining** and **token generation** via **sampling** (Chapter 10).\n",
    "- **Masked language modeling** (Chapter 11) introduces **BERT** models.\n",
    "- Prompting LLMs and aligning with **human preferences** (Chapter 12).\n",
    "- **Encoder-decoder** architecture for **machine translation** (Chapter 13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "- **Problem with Static Embeddings**:\n",
    "  - Word embeddings like **word2vec** are static, i.e., the word’s meaning remains constant across contexts.\n",
    "  - 🍎 The word \"it\" is always represented by the same vector, even though its meaning varies in sentences:\n",
    "    - The chicken didn’t cross the road because **it** was too tired.\n",
    "      - it → chicken\n",
    "    - The chicken didn’t cross the road because **it** was too wide.\n",
    "      - it → road\n",
    "  \n",
    "- **Need for Contextual Representations**: Context-dependent meanings are crucial\n",
    "  - e.g., \"it\" refers to different entities in the two sentences above.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges of Context in Language Models**\n",
    "\n",
    "- **Context Sensitivity**:\n",
    "  - Left-to-right language models struggle with context.\n",
    "    - e.g., The chicken didn’t cross the road because **it**\n",
    "    - At this point, the model can't resolve whether \"it\" refers to the chicken or the road, requiring further context.\n",
    "\n",
    "- **Distant Dependencies**:\n",
    "  - Linguistic relationships, like subject-verb agreement or disambiguation, often span across long distances.\n",
    "  - e.g,: \"The **keys** to the cabinet **are** on the table\" \n",
    "    - The model must understand that \"keys\" (plural) governs \"are\" (plural verb).\n",
    "  - I walked along the **pond**, and noticed one of the trees along the **bank**.\n",
    "    - Bank refers to the side of a pond or river and not a financial institution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformers and Contextual Representations**\n",
    "\n",
    "- **Solution: Transformers**:\n",
    "  - **Transformers** address the need for contextualized word meanings by integrating information from surrounding words.\n",
    "  - Layer-by-layer, transformers build up **contextual embeddings**, refining token meanings by considering neighboring tokens.\n",
    "\n",
    "- **Self-Attention**:\n",
    "  - Attention is the core mechanism allowing transformers to focus on **relevant words** within the context.\n",
    "  - At each layer, token representations are refined by weighing contributions from surrounding tokens in previous layer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Self-Attention in Action: Example with \"It\"**\n",
    "- In the sentence: \"The chicken didn’t cross the road because **it**,\" the attention mechanism weighs heavily on the tokens **chicken** and **road**.\n",
    "- The transformer dynamically adjusts the representation of \"it\" by considering both **chicken** and **road** as possible references.\n",
    "\n",
    "![The self-attention weight distribution α](./images/trans/getit.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Formal Definition of Attention**\n",
    "\n",
    "- **Attention Mechanism**:\n",
    "  - Takes the input token representation $𝐱_i[1×d]$ and a context window of prior inputs $𝐱_1, \\dots, 𝐱_{i-1}$, producing an output $𝐚_i[1×d]$.\n",
    "    - $d$ - model dimensionality\n",
    "  - **Context Window**: In left-to-right models, the model attends only to previous tokens.\n",
    "\n",
    "![Information flow in causal self-attention](./images/trans/casat.png)\n",
    "\n",
    "- $\\text{self-attention}: (𝐱_1, \\dots, 𝐱_n) ↦ (𝐚_1, ⋯, 𝐚_n)$ \n",
    "\n",
    "---\n",
    "\n",
    "### **Simplified Attention: Weighted Sum of Context Vectors**\n",
    "\n",
    "- **Core Idea**: Attention is a **weighted sum** of context vectors.\n",
    "  - **Weighting**: The weight $\\alpha_{ij}$ is computed via the similarity between tokens $𝐱_i$ and $𝐱_j$, typically using the dot product.\n",
    "  - $𝐚_i = \\sum_{j \\leq i} \\alpha_{ij} 𝐱_j$\n",
    "  - $\\alpha_{ij}$: Weight determining how much token $𝐱_j$ contributes to the final representation $𝐚_i$.\n",
    "- **Similarity Scores**:\n",
    "  - Dot product computes similarity:\n",
    "    - $s_{ij} = \\text{score}(𝐱_i, 𝐱_j) = 𝐱_i \\cdot 𝐱_j$\n",
    "  - These scores are normalized using softmax to create a **probability distribution** over the tokens.\n",
    "    - $\\displaystyle \\underset{j≤ i}{\\alpha_{ij}} = \\frac{e^{s_{ij}}}{\\sum_{k \\leq i} e^{s_{ik}}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **A Single Attention Head Using Query, Key, and Value Matrices**\n",
    "- Attention head in transformer refers to specific structured layers\n",
    "  - It represent three different roles that each input embedding plays during attention\n",
    "- **Roles of Query, Key, and Value**:\n",
    "  - **Query** $𝐪_i[1×d_k]$: Represents the current token being attended to.\n",
    "    - $d_k$ - dimension for the key and query vectors\n",
    "  - **Key** $𝐤_i[1×d_k]$: Represents previous tokens compared with the current token.\n",
    "  - **Value** $𝐯_i[1×d_v]$: The content of previous tokens used to update the current token.\n",
    "    - $d_v$ - dimension for the value vectors\n",
    "\n",
    "- **Projections**: \n",
    "  - $𝐪_i = 𝐱_i 𝐖^𝐐, \\quad 𝐤_i = 𝐱_i 𝐖^𝐊, \\quad 𝐯_i = 𝐱_i 𝐖^𝐕$\n",
    "    - weight matrix shapes: $𝐖^𝐐[d×d_k], 𝐖^𝐊[d×d_k], 𝐖^𝐕[d×d_v]$\n",
    "  - These projections enable each token to play different roles in the attention process.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scaling Attention for Stability**\n",
    "\n",
    "- **Dot Product Scaling**: To prevent large dot product values from causing instability, similarity scores are scaled by the square root of the embedding dimension $d_k$:\n",
    "  - $s_{ij} = \\text{score}(𝐱_i, 𝐱_j) = \\dfrac{𝐪_i \\cdot 𝐤_j}{\\sqrt{d_k}}$\n",
    "  - This scaling ensures more stable training and avoids gradient vanishing.\n",
    "- **Final Attention Output**: The output for token $𝐚_i$ is the weighted sum of the value vectors:\n",
    "  - $\\displaystyle 𝐚_i = \\sum_{j \\leq i} \\alpha_{ij} 𝐯_j$\n",
    "    - This sums up the information from previous tokens, weighted by their relevance to the current token $𝐱_i$.\n",
    "    - $𝐪_i = 𝐱_i 𝐖^𝐐, \\quad 𝐤_j = 𝐱_j 𝐖^𝐊, \\quad 𝐯_j = 𝐱_j 𝐖^𝐕$\n",
    "\n",
    "- **Attention Layer**: This computation happens for all tokens simultaneously, creating an output sequence of the same length as the input.\n",
    "- 🍎 Calculating the value of $𝐚_3$ \n",
    "  - the third element of a sequence using causal (left-to-right) self-attention.\n",
    "\n",
    "![Calculating the value of a₃](./images/trans/cala.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Multi-Head Attention: Expanding Model Capability**\n",
    "\n",
    "- **Multiple Attention Heads**: Transformers use $h$ **multiple attention heads** in parallel, each with its own **query, key, and value matrices**.\n",
    "  - Each head focuses on different patterns or relationships within the sequence.\n",
    "  \n",
    "- **Concatenation and Projection**:\n",
    "  - $𝐚_i = (\\textbf{head}_1 \\oplus \\textbf{head}_2 \\dots \\oplus \\textbf{head}_h) 𝐖_O$\n",
    "    - The outputs from all heads are concatenated and projected back to the original dimensionality $d$.\n",
    "    \n",
    "    - $\\displaystyle\\textbf{head}_i^c = ∑_{j≤i}α_{ij}^c 𝐯_j^c,\\quad ∀c (1≤c≤h)$\n",
    "    \n",
    "    - $\\displaystyle \\underset{j≤ i}{\\alpha_{ij}^c} = \\underset{j≤i}{\\text{softmax}}(s^c_{ij}) = \\frac{e^{s^c_{ij}}}{\\sum_{m \\leq i} e^{s^c_{im}}}$\n",
    "    \n",
    "    - $s^c_{ij} = \\text{score}^c(𝐱_i, 𝐱_j) = \\dfrac{𝐪^c_i \\cdot 𝐤^c_j}{\\sqrt{d_k}}$\n",
    "    \n",
    "    - $𝐪^c_i = 𝐱_i 𝐖^{𝐐_c}, \\quad 𝐤^c_j = 𝐱_j 𝐖^{𝐊_c}, \\quad 𝐯^c_j = 𝐱_j 𝐖^{𝐕_c}$\n",
    "\n",
    "- 🍎 The multi-head attention computation for input $𝐱_i$ producing output $𝐚_i$\n",
    "![The multi-head attention computation for input 𝐱ᵢ producing output 𝐚ᵢ](./images/trans/mutihead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Blocks\n",
    "- Self-attention lies at the core of the transformer block\n",
    "- The processing of a token through the transformer block is called a **residual stream**.\n",
    "\n",
    "![The architecture of a transformer block showing the residual stream](./images/trans/block.png)\n",
    "\n",
    "- An input vector $𝐱_i$ is processed as a stream of $d$-dimensional representation\n",
    "- Layer norms before attention and feedforward layers (prenorm architecture)\n",
    "- Each layer adds output back into the residual stream\n",
    "\n",
    "---\n",
    "\n",
    "### **Feedforward Layer(FFN)**\n",
    "- A fully connected, 2-layer network\n",
    "  - One hidden layer, two weight matrices\n",
    "  - Same weights for each token, different across layers\n",
    "- Dimensionality $d_{\\text{ff}}$ of the hidden layer is usually larger than the model dimensionality $d$\n",
    "  - e.g.,  in the original transformer model: $d = 512$, $d_{\\text{ff}} = 2048$\n",
    "\n",
    "- $\\text{FFN}(𝐱_i) = \\text{ReLU}(𝐱_i 𝐖^1 + b_1) 𝐖^2 + b_2$\n",
    "\n",
    "---\n",
    "\n",
    "### **Layer Normalization (Layer Norm)**\n",
    "- Applied to single token embedding vector, not the entire layer\n",
    "- Normalizes vector of dimensionality $d$ to have zero mean and unit variance\n",
    "  - Mean $\\mu$ over the vector components:\n",
    "    - $\\displaystyle\\mu = \\dfrac{1}{d} \\sum_{i=1}^{d} x_i$\n",
    "  - Standard deviation $\\sigma$:\n",
    "    - $\\displaystyle\\sigma = \\sqrt{\\dfrac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2}$\n",
    "  - Normalized vector $\\hat{𝐱}$:\n",
    "    - $\\displaystyle\\hat{𝐱} = \\dfrac{(𝐱 - \\mu)}{\\sigma}$\n",
    "- Keeps values within a range for gradient-based training\n",
    "  - Layer normalization with two learnable parameters $\\gamma$ and $\\beta$:\n",
    "  - $\\displaystyle\\text{LayerNorm}(𝐱) = \\gamma \\dfrac{(𝐱 - \\mu)}{\\sigma} + \\beta$\n",
    "  - $\\gamma$ - gain, and $\\beta$ - offset\n",
    "\n",
    "---\n",
    "\n",
    "### **Putting it All Together: Transformer Block Process**\n",
    "- **Input:** Token embedding vector $𝐱_i$\n",
    "- **Step-by-step process:**\n",
    "  1. Layer Norm:\n",
    "\n",
    "     - $𝐭_i^1 = \\text{LayerNorm}(𝐱_i)$\n",
    "\n",
    "  2. Multi-Head Attention (over all tokens $𝐱_1, \\ldots, 𝐱_N$):\n",
    "\n",
    "     - $𝐭_i^3 = \\text{MultiHeadAttention}(𝐭_i^1, [𝐱_1^1, \\ldots, 𝐱_N^1])$\n",
    "\n",
    "  3. Residual connection:\n",
    "\n",
    "     - $𝐭_i^3 = 𝐭_i^2 + 𝐱_i$\n",
    "\n",
    "  4. Second Layer Norm:\n",
    "\n",
    "     - $𝐭_i^4 = \\text{LayerNorm}(𝐭_i^3)$\n",
    "\n",
    "  5. Feedforward Network:\n",
    "\n",
    "     - $𝐭_i^5 = \\text{FFN}(𝐭_i^4)$\n",
    "\n",
    "  6. Final residual connection (output of the block):\n",
    "\n",
    "     - $𝐡_i = 𝐭_i^5 + 𝐭_i^3$\n",
    "\n",
    "-  An attention head can move information from token A’s residual stream into token B’s residual stream.\n",
    "\n",
    "![pass from one residual stream to another](./images/trans/pass.png)\n",
    "---\n",
    "\n",
    "### **Stacking Transformer Blocks**\n",
    "\n",
    "- Blocks can be stacked (12 layers in GPT-3-small, 96 layers in GPT-3-large)\n",
    "- Residual stream metaphor: input processed across layers\n",
    "- Each block refines token representation, leading to better predictions\n",
    "- Essential for large-scale models like GPT and T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization in Transformers Using A Single Matrix 𝐗\n",
    "- Computation for each token in a transformer block can be parallelized.\n",
    "  - ∵ All the computation in the transformer block computing $𝐡_i$ from the input $𝐱_i$\n",
    "- Input embeddings for all tokens are packed into a matrix $𝐗$ of size $[N \\times d]$\n",
    "  - where $N$ is the number of tokens, and $d$ is the embedding dimensionality.\n",
    "- Common input sizes range from 1K to 32K tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### **Attention Computation for a Single Attention Head**\n",
    "\n",
    "- Multiply $𝐗$ by key, query, and value weight matrices $𝐖^𝐊∈[d×d_k]$,  $𝐖^𝐐∈[d×d_k]$, and $𝐖^𝐕∈[d×d_v]$.\n",
    "- Compute $𝐐\\in \\mathbb{R}^{N \\times d_k}$, $𝐊\\in \\mathbb{R}^{N \\times d_k}$, and $𝐕\\in \\mathbb{R}^{N \\times d_v}$ matrices for the entire input sequence.\n",
    "  - $𝐐 = 𝐗𝐖^𝐐, \\quad 𝐊 = 𝐗𝐖^𝐊, \\quad 𝐕 = 𝐗𝐖^𝐕$\n",
    "\n",
    "- **Efficient 𝐐uery-Key Comparisons**\n",
    "  - Perform all query-key comparisons using matrix multiplication $𝐐𝐊^𝐓∈[N \\times N]$.\n",
    "  - ![all qᵢ · kⱼ comparisons in a single matrix multiple](./images/trans/qk.png)\n",
    "\n",
    "- The entire self-attention step for an entire sequence of $N$ tokens for one head\n",
    "  - $\\displaystyle 𝐀 = \\text{softmax}\\left( \\text{mask}\\left(\\frac{𝐐𝐊^𝐓}{\\sqrt{d_k}}\\right) \\right) 𝐕$\n",
    "  - This computes the attention for all tokens simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "### **Masking Future Tokens**\n",
    "- For language modeling, we prevent access to future tokens by masking the upper-triangular part of the $𝐐𝐊^𝐓$ matrix.\n",
    "  - $M_{ij} = \n",
    "  \\begin{cases}\n",
    "    -\\infty & \\text{if } j > i \\text{, i.e. for the upper-triangular portion} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}$\n",
    "  - ![mask](./images/trans/mask.png)\n",
    "  - This avoids cheating by looking at the following tokens.\n",
    "- The attention computation for a single attention head in parallel\n",
    "\n",
    "![The attention computation for a single attention head in parallel](./images/trans/singleall.png)\n",
    "---\n",
    "\n",
    "### **Parallelizing Multi-Head Attention**\n",
    "- Multiple attention heads run in parallel.\n",
    "- For each head $i$, calculate $𝐐^i[N×d_k]$, $𝐊^i[N×d_k]$, and $𝐕^i[N×d_v]$ \n",
    "  - $𝐐^i = 𝐗𝐖^{𝐐^i}, \\quad 𝐊^i = 𝐗𝐖^{𝐊_i}, \\quad 𝐕^i = 𝐗𝐖^{𝐕_i}$\n",
    "    - Matrix shape: $𝐖^{𝐐_i}∈ ℝ^{d×d_k}$, $𝐖^{𝐊_i}∈ ℝ^{d×d_k}$, and $𝐖^{𝐕_i}∈ ℝ^{d×d_v}$\n",
    "  - $\\textbf{head}_i = \\text{SelfAttention}(𝐐^i, 𝐊^i, 𝐕^i) = \\text{softmax}\\left( \\dfrac{𝐐^i 𝐊^{i𝐓}}{\\sqrt{d_k}} \\right) 𝐕^i$\n",
    "    - $\\textbf{head}_i ∈ N×d_v$, so the concatenation of $h$ attentions as a single output has shape $N×hd_v$\n",
    "\n",
    "- **Final Multi-Head Attention Output:**\n",
    "  - The output of the multi-head attention is concatenated and projected back to dimensionality $[N \\times d]$.\n",
    "  - $\\text{MultiHeadAttention}(𝐗) = (\\textbf{head}_1 \\oplus \\textbf{head}_2 \\oplus \\cdots \\oplus \\textbf{head}_h) 𝐖^𝐎$\n",
    "    - $𝐖^𝐎 \\in \\mathbb{R}^{hd_v \\times d}$ is the linear projection matrix.\n",
    "  - This ensures the dimensionality is preserved for further transformer layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **Putting It All Together with the Parallelized Input Matrix $𝐗$**\n",
    "- The entire layer of $N$ transformer block over the entire $N$ input tokens is parallelized.\n",
    "- Layer normalization and feedforward layers are applied in parallel to each token.\n",
    "\n",
    "   - $𝐎 = \\text{LayerNorm}(𝐗 + \\text{MultiHeadAttention}(𝐗))$\n",
    "   - $𝐇 = \\text{LayerNorm}(𝐎 + \\text{FFN}(𝐎))$\n",
    "\n",
    "- Break it down with one equation for each component computation:\n",
    "   - $𝐓^1 = \\text{MultiHeadAttention}(𝐗)$\n",
    "   - $𝐓^2 = 𝐗 + 𝐓^1$\n",
    "   - $𝐓^3 = \\text{LayerNorm}(𝐓^2)$\n",
    "   - $𝐓^4 = \\text{FFN}(𝐓^3)$\n",
    "   - $𝐓^5 = 𝐓^4 + 𝐓^3$\n",
    "   - $𝐇 = \\text{LayerNorm}(𝐓^5)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input: Embeddings for Token and Position\n",
    "\n",
    "  - The transformer creates two embeddings: \n",
    "    - `token` embedding and `positional` embedding.\n",
    "  - The combined embeddings form the input matrix $𝐗$ of shape $[N \\times d]$\n",
    "    - where $N$ is the number of tokens, and $d$ is the embedding dimensionality.\n",
    "- Token Embeddings from the Vocabulary\n",
    "  - Token embeddings are vectors of dimension $d$ stored in the embedding matrix $𝐄$.\n",
    "  - Each word from the vocabulary $V$ has a corresponding row in $𝐄$.\n",
    "    - $𝐄 \\in \\mathbb{R}^{|V| \\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Selecting the Embedding for a Sequence of Tokens**\n",
    "  - Convert tokens into `vocabulary indices` and select the corresponding rows from the embedding matrix $𝐄$.\n",
    "    - This is equivalent to multiplying a `one-hot vector` with the embedding matrix to retrieve the token embedding.\n",
    "\n",
    "- 🍎 **One-Hot Vector:**\n",
    "  - $[0\\ 0\\ 0\\ 0\\ 1\\ 0\\ 0 \\dots 0]$\n",
    "  - The embedding for token $i$ is selected as $𝐄[i]$.\n",
    "\n",
    "![Selecting the embedding vector for word V₅](./images/trans/seli.png)\n",
    "\n",
    "- The entire token sequence can be selected similarly\n",
    "  \n",
    "![Selecting the embedding matrix for the input sequence of token ids W](./images/trans/selseq.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Positional Embeddings**\n",
    "- Token embeddings are position-independent.\n",
    "- Positional embeddings represent the position of each token in the sequence.\n",
    "  - Stored in `Positional Embedding Matrix`: $𝐄_{pos} \\in \\mathbb{R}^{1 \\times N}$\n",
    "- How to get these positional embeddings?\n",
    "  - Absolute positional embeddings are randomly initialized and learned during training.\n",
    "  - Each position in the sequence has a corresponding embedding.\n",
    "\n",
    "- The final input representation $𝐗[N \\times d]$ is obtained by `adding token embeddings and positional embeddings.`\n",
    "  - $𝐗[i] = 𝐄[\\text{id}(i)] + 𝐏[i]$\n",
    "\n",
    "![A simple way to model position](./images/trans/posem.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations of Absolute Position Embeddings**\n",
    "\n",
    "- Early positions are well-trained due to their frequency in training\n",
    "  - while later positions may be undertrained and may not generalize well.\n",
    "\n",
    "- A more robust alternative is using a `static function` like sine and cosine functions to map integer inputs to real-valued vectors in a way that captures the inherent relationships among the positions.\n",
    "- `Relative positional embeddings` capture relationships between positions and are often implemented in the attention mechanism at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling Head\n",
    "  - A \"head\" refers to additional neural circuitry added to the transformer for specific tasks.\n",
    "  - It enables transformers to predict the next word given a context.\n",
    "- **Word Prediction with Language Models**\n",
    "  - Given a context like “Thanks for all the”, the model computes $P(\\text{fish}|\\text{Thanks for all the})$.\n",
    "  - The language model outputs a distribution over the entire vocabulary for predicting the next word.\n",
    "  - In transformers, the context size is determined by the model’s window, allowing large contexts (e.g., 2K, 4K tokens).\n",
    "\n",
    "---\n",
    "\n",
    "### **Structure of the Language Modeling Head**\n",
    "- The head takes the output embedding $h_L^N$ from the last transformer layer for the last token $N$.\n",
    "- The goal is to produce a probability distribution over the vocabulary for the next token $N+1$.\n",
    "\n",
    "![Word Prediction with Language Models](./images/trans/lmhead.png)\n",
    "\n",
    "- Linear Layer: From Embedding to Logits\n",
    "  - The first step is a linear layer that projects the output embedding $𝐡^𝐋_𝐍[1 \\times d]$ into a logit vector $𝐮[1 \\times |V|]$.\n",
    "  - This vector contains a score for each word in the vocabulary $V$.\n",
    "  - $𝐮 = 𝐡^𝐋_𝐍 𝐄^T$\n",
    "- Weight Tying with the Embedding Matrix\n",
    "  - The same embedding matrix $𝐄[|V| \\times d]$ used to embed tokens is reused as $𝐄^T$ to map embeddings back to the vocabulary space.\n",
    "  - This process is known as **unembedding**.\n",
    "- Softmax: Turning Logits into Probabilities\n",
    "  - The logit vector $𝐮$ is passed through a **softmax** function to produce a probability distribution $𝐲$ over the vocabulary.\n",
    "    - $𝐲 = \\text{softmax}(𝐮)$\n",
    "    - The output $𝐲$ represents the probabilities of each word in the vocabulary being the next word.\n",
    "- Generating Text with the Language Model\n",
    "  - The most probable word (highest $y_k$) can be selected (greedy decoding) or sampled probabilistically using other methods.\n",
    "  - The word corresponding to the selected index $k$ is the generated next word.\n",
    "\n",
    "![A transformer language model (decoder-only)](./images/trans/gen.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **The Logit Lens: A Tool for Model Interpretability**\n",
    "- The **logit lens** is a method to interpret the internal states of a transformer model.\n",
    "-  **How it Works:**\n",
    "   1. Take a vector from **any layer** of the transformer.\n",
    "   2. Multiply it by the **unembedding layer** (transpose of the embedding matrix).\n",
    "   3. Apply a **softmax** to compute a probability distribution over words.\n",
    "   4. This gives insight into the words that internal layers might be predicting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformer Decoder for Language Modeling**\n",
    "- Transformers originally used an **encoder-decoder** architecture.\n",
    "- For causal language modeling, we only use the **decoder** part to generate the next word in sequence.\n",
    "  - This architecture is commonly used for text generation and machine translation.\n",
    "\n",
    "### 🍎 Building a basic transformer-based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to predict\n",
    "model.eval()\n",
    "\n",
    "def predict_next_word(text, model, tokenizer, max_length=50):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    # Generate the next token logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get the predicted next token id\n",
    "    predicted_token_id = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "\n",
    "    # Decode the predicted token id back to a word\n",
    "    predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "    return predicted_word\n",
    "\n",
    "# Test the function\n",
    "input_text = \"The quick brown fox jumps over\"\n",
    "predicted_word = predict_next_word(input_text, model, tokenizer)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Predicted next word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Formal Algorithms for Transformers](https://arxiv.org/pdf/2207.09238)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
