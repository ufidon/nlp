{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/ufidon/nlp/blob/main/07.trans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ufidon/nlp/blob/main/07.trans.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "# The Transformer\n",
    "\n",
    "ğŸ“ SALP chapter 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Transformer Overview**\n",
    "\n",
    "- **Transformers** are the core architecture for **large language models (LLMs)**\n",
    "  - Which are revolutionizing speech and language processing.\n",
    "- Focus: **Left-to-right (causal) language modeling**, where tokens are predicted sequentially based on prior context.\n",
    "- The key mechanism: **Self-attention** (multi-head attention) allows the model to integrate information from surrounding tokens to capture long-range dependencies and contextual relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformer Architecture Components**\n",
    "\n",
    "- **1. Transformer Blocks**: \n",
    "  - Each block contains **multi-head attention**, a **feedforward network**, and **layer normalization**.\n",
    "  - A series of blocks maps input tokens $ğ±_i = (ğ±_1, ..., ğ±_n)$ to output tokens $(ğ¡_i = ğ¡_1, ..., ğ¡_n)$, allowing deep processing over multiple layers.\n",
    "  \n",
    "- **2. Input Encoding**: \n",
    "  - Transforms input tokens into **contextual vectors** using an **embedding matrix $ğ„$** and **positional encoding** to capture token order.\n",
    "\n",
    "- **3. Language Modeling Head**: \n",
    "  - Projects hidden states through an **unembedding matrix $ğ”$**, applying **softmax** over the vocabulary to generate a token prediction.\n",
    "\n",
    "![The architecture of a (left-to-right) transformer](./images/trans/lrtrans.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps in Transformer Exploration**\n",
    "\n",
    "- **Multi-head attention** and detailed transformer block mechanisms are covered in upcoming chapters.\n",
    "- **Pretraining** and **token generation** via **sampling** (Chapter 10).\n",
    "- **Masked language modeling** (Chapter 11) introduces **BERT** models.\n",
    "- Prompting LLMs and aligning with **human preferences** (Chapter 12).\n",
    "- **Encoder-decoder** architecture for **machine translation** (Chapter 13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "- **Problem with Static Embeddings**:\n",
    "  - Word embeddings like **word2vec** are static, i.e., the wordâ€™s meaning remains constant across contexts.\n",
    "  - ğŸ The word \"it\" is always represented by the same vector, even though its meaning varies in sentences:\n",
    "    - The chicken didnâ€™t cross the road because **it** was too tired.\n",
    "      - it â†’ chicken\n",
    "    - The chicken didnâ€™t cross the road because **it** was too wide.\n",
    "      - it â†’ road\n",
    "  \n",
    "- **Need for Contextual Representations**: Context-dependent meanings are crucial\n",
    "  - e.g., \"it\" refers to different entities in the two sentences above.\n",
    "\n",
    "---\n",
    "\n",
    "### **Challenges of Context in Language Models**\n",
    "\n",
    "- **Context Sensitivity**:\n",
    "  - Left-to-right language models struggle with context.\n",
    "    - e.g., The chicken didnâ€™t cross the road because **it**\n",
    "    - At this point, the model can't resolve whether \"it\" refers to the chicken or the road, requiring further context.\n",
    "\n",
    "- **Distant Dependencies**:\n",
    "  - Linguistic relationships, like subject-verb agreement or disambiguation, often span across long distances.\n",
    "  - e.g,: \"The **keys** to the cabinet **are** on the table\" \n",
    "    - The model must understand that \"keys\" (plural) governs \"are\" (plural verb).\n",
    "  - I walked along the **pond**, and noticed one of the trees along the **bank**.\n",
    "    - Bank refers to the side of a pond or river and not a financial institution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformers and Contextual Representations**\n",
    "\n",
    "- **Solution: Transformers**:\n",
    "  - **Transformers** address the need for contextualized word meanings by integrating information from surrounding words.\n",
    "  - Layer-by-layer, transformers build up **contextual embeddings**, refining token meanings by considering neighboring tokens.\n",
    "\n",
    "- **Self-Attention**:\n",
    "  - Attention is the core mechanism allowing transformers to focus on **relevant words** within the context.\n",
    "  - At each layer, token representations are refined by weighing contributions from surrounding tokens in previous layer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Self-Attention in Action: Example with \"It\"**\n",
    "- In the sentence: \"The chicken didnâ€™t cross the road because **it**,\" the attention mechanism weighs heavily on the tokens **chicken** and **road**.\n",
    "- The transformer dynamically adjusts the representation of \"it\" by considering both **chicken** and **road** as possible references.\n",
    "\n",
    "![The self-attention weight distribution Î±](./images/trans/getit.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Formal Definition of Attention**\n",
    "\n",
    "- **Attention Mechanism**:\n",
    "  - Takes the input token representation $ğ±_i[1Ã—d]$ and a context window of prior inputs $ğ±_1, \\dots, ğ±_{i-1}$, producing an output $ğš_i[1Ã—d]$.\n",
    "    - $d$ - model dimensionality\n",
    "  - **Context Window**: In left-to-right models, the model attends only to previous tokens.\n",
    "\n",
    "![Information flow in causal self-attention](./images/trans/casat.png)\n",
    "\n",
    "- $\\text{self-attention}: (ğ±_1, \\dots, ğ±_n) â†¦ (ğš_1, â‹¯, ğš_n)$ \n",
    "\n",
    "---\n",
    "\n",
    "### **Simplified Attention: Weighted Sum of Context Vectors**\n",
    "\n",
    "- **Core Idea**: Attention is a **weighted sum** of context vectors.\n",
    "  - **Weighting**: The weight $\\alpha_{ij}$ is computed via the similarity between tokens $ğ±_i$ and $ğ±_j$, typically using the dot product.\n",
    "  - $ğš_i = \\sum_{j \\leq i} \\alpha_{ij} ğ±_j$\n",
    "  - $\\alpha_{ij}$: Weight determining how much token $ğ±_j$ contributes to the final representation $ğš_i$.\n",
    "- **Similarity Scores**:\n",
    "  - Dot product computes similarity:\n",
    "    - $s_{ij} = \\text{score}(ğ±_i, ğ±_j) = ğ±_i \\cdot ğ±_j$\n",
    "  - These scores are normalized using softmax to create a **probability distribution** over the tokens.\n",
    "    - $\\displaystyle \\underset{jâ‰¤ i}{\\alpha_{ij}} = \\frac{e^{s_{ij}}}{\\sum_{k \\leq i} e^{s_{ik}}}$\n",
    "\n",
    "---\n",
    "\n",
    "### **A Single Attention Head Using Query, Key, and Value Matrices**\n",
    "- Attention head in transformer refers to specific structured layers\n",
    "  - It represent three different roles that each input embedding plays during attention\n",
    "- **Roles of Query, Key, and Value**:\n",
    "  - **Query** $ğª_i[1Ã—d_k]$: Represents the current token being attended to.\n",
    "    - $d_k$ - dimension for the key and query vectors\n",
    "  - **Key** $ğ¤_i[1Ã—d_k]$: Represents previous tokens compared with the current token.\n",
    "  - **Value** $ğ¯_i[1Ã—d_v]$: The content of previous tokens used to update the current token.\n",
    "    - $d_v$ - dimension for the value vectors\n",
    "\n",
    "- **Projections**: \n",
    "  - $ğª_i = ğ±_i ğ–^ğ, \\quad ğ¤_i = ğ±_i ğ–^ğŠ, \\quad ğ¯_i = ğ±_i ğ–^ğ•$\n",
    "    - weight matrix shapes: $ğ–^ğ[dÃ—d_k], ğ–^ğŠ[dÃ—d_k], ğ–^ğ•[dÃ—d_v]$\n",
    "  - These projections enable each token to play different roles in the attention process.\n",
    "\n",
    "---\n",
    "\n",
    "### **Scaling Attention for Stability**\n",
    "\n",
    "- **Dot Product Scaling**: To prevent large dot product values from causing instability, similarity scores are scaled by the square root of the embedding dimension $d_k$:\n",
    "  - $s_{ij} = \\text{score}(ğ±_i, ğ±_j) = \\dfrac{ğª_i \\cdot ğ¤_j}{\\sqrt{d_k}}$\n",
    "  - This scaling ensures more stable training and avoids gradient vanishing.\n",
    "- **Final Attention Output**: The output for token $ğš_i$ is the weighted sum of the value vectors:\n",
    "  - $\\displaystyle ğš_i = \\sum_{j \\leq i} \\alpha_{ij} ğ¯_j$\n",
    "    - This sums up the information from previous tokens, weighted by their relevance to the current token $ğ±_i$.\n",
    "    - $ğª_i = ğ±_i ğ–^ğ, \\quad ğ¤_j = ğ±_j ğ–^ğŠ, \\quad ğ¯_j = ğ±_j ğ–^ğ•$\n",
    "\n",
    "- **Attention Layer**: This computation happens for all tokens simultaneously, creating an output sequence of the same length as the input.\n",
    "- ğŸ Calculating the value of $ğš_3$ \n",
    "  - the third element of a sequence using causal (left-to-right) self-attention.\n",
    "\n",
    "![Calculating the value of aâ‚ƒ](./images/trans/cala.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Multi-Head Attention: Expanding Model Capability**\n",
    "\n",
    "- **Multiple Attention Heads**: Transformers use $h$ **multiple attention heads** in parallel, each with its own **query, key, and value matrices**.\n",
    "  - Each head focuses on different patterns or relationships within the sequence.\n",
    "  \n",
    "- **Concatenation and Projection**:\n",
    "  - $ğš_i = (\\textbf{head}_1 \\oplus \\textbf{head}_2 \\dots \\oplus \\textbf{head}_h) ğ–_O$\n",
    "    - The outputs from all heads are concatenated and projected back to the original dimensionality $d$.\n",
    "    \n",
    "    - $\\displaystyle\\textbf{head}_i^c = âˆ‘_{jâ‰¤i}Î±_{ij}^c ğ¯_j^c,\\quad âˆ€c (1â‰¤câ‰¤h)$\n",
    "    \n",
    "    - $\\displaystyle \\underset{jâ‰¤ i}{\\alpha_{ij}^c} = \\underset{jâ‰¤i}{\\text{softmax}}(s^c_{ij}) = \\frac{e^{s^c_{ij}}}{\\sum_{m \\leq i} e^{s^c_{im}}}$\n",
    "    \n",
    "    - $s^c_{ij} = \\text{score}^c(ğ±_i, ğ±_j) = \\dfrac{ğª^c_i \\cdot ğ¤^c_j}{\\sqrt{d_k}}$\n",
    "    \n",
    "    - $ğª^c_i = ğ±_i ğ–^{ğ_c}, \\quad ğ¤^c_j = ğ±_j ğ–^{ğŠ_c}, \\quad ğ¯^c_j = ğ±_j ğ–^{ğ•_c}$\n",
    "\n",
    "- ğŸ The multi-head attention computation for input $ğ±_i$ producing output $ğš_i$\n",
    "![The multi-head attention computation for input ğ±áµ¢ producing output ğšáµ¢](./images/trans/mutihead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Blocks\n",
    "- Self-attention lies at the core of the transformer block\n",
    "- The processing of a token through the transformer block is called a **residual stream**.\n",
    "\n",
    "![The architecture of a transformer block showing the residual stream](./images/trans/block.png)\n",
    "\n",
    "- An input vector $ğ±_i$ is processed as a stream of $d$-dimensional representation\n",
    "- Layer norms before attention and feedforward layers (prenorm architecture)\n",
    "- Each layer adds output back into the residual stream\n",
    "\n",
    "---\n",
    "\n",
    "### **Feedforward Layer(FFN)**\n",
    "- A fully connected, 2-layer network\n",
    "  - One hidden layer, two weight matrices\n",
    "  - Same weights for each token, different across layers\n",
    "- Dimensionality $d_{\\text{ff}}$ of the hidden layer is usually larger than the model dimensionality $d$\n",
    "  - e.g.,  in the original transformer model: $d = 512$, $d_{\\text{ff}} = 2048$\n",
    "\n",
    "- $\\text{FFN}(ğ±_i) = \\text{ReLU}(ğ±_i ğ–^1 + b_1) ğ–^2 + b_2$\n",
    "\n",
    "---\n",
    "\n",
    "### **Layer Normalization (Layer Norm)**\n",
    "- Applied to single token embedding vector, not the entire layer\n",
    "- Normalizes vector of dimensionality $d$ to have zero mean and unit variance\n",
    "  - Mean $\\mu$ over the vector components:\n",
    "    - $\\displaystyle\\mu = \\dfrac{1}{d} \\sum_{i=1}^{d} x_i$\n",
    "  - Standard deviation $\\sigma$:\n",
    "    - $\\displaystyle\\sigma = \\sqrt{\\dfrac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2}$\n",
    "  - Normalized vector $\\hat{ğ±}$:\n",
    "    - $\\displaystyle\\hat{ğ±} = \\dfrac{(ğ± - \\mu)}{\\sigma}$\n",
    "- Keeps values within a range for gradient-based training\n",
    "  - Layer normalization with two learnable parameters $\\gamma$ and $\\beta$:\n",
    "  - $\\displaystyle\\text{LayerNorm}(ğ±) = \\gamma \\dfrac{(ğ± - \\mu)}{\\sigma} + \\beta$\n",
    "  - $\\gamma$ - gain, and $\\beta$ - offset\n",
    "\n",
    "---\n",
    "\n",
    "### **Putting it All Together: Transformer Block Process**\n",
    "- **Input:** Token embedding vector $ğ±_i$\n",
    "- **Step-by-step process:**\n",
    "  1. Layer Norm:\n",
    "\n",
    "     - $ğ­_i^1 = \\text{LayerNorm}(ğ±_i)$\n",
    "\n",
    "  2. Multi-Head Attention (over all tokens $ğ±_1, \\ldots, ğ±_N$):\n",
    "\n",
    "     - $ğ­_i^3 = \\text{MultiHeadAttention}(ğ­_i^1, [ğ±_1^1, \\ldots, ğ±_N^1])$\n",
    "\n",
    "  3. Residual connection:\n",
    "\n",
    "     - $ğ­_i^3 = ğ­_i^2 + ğ±_i$\n",
    "\n",
    "  4. Second Layer Norm:\n",
    "\n",
    "     - $ğ­_i^4 = \\text{LayerNorm}(ğ­_i^3)$\n",
    "\n",
    "  5. Feedforward Network:\n",
    "\n",
    "     - $ğ­_i^5 = \\text{FFN}(ğ­_i^4)$\n",
    "\n",
    "  6. Final residual connection (output of the block):\n",
    "\n",
    "     - $ğ¡_i = ğ­_i^5 + ğ­_i^3$\n",
    "\n",
    "-  An attention head can move information from token Aâ€™s residual stream into token Bâ€™s residual stream.\n",
    "\n",
    "![pass from one residual stream to another](./images/trans/pass.png)\n",
    "---\n",
    "\n",
    "### **Stacking Transformer Blocks**\n",
    "\n",
    "- Blocks can be stacked (12 layers in GPT-3-small, 96 layers in GPT-3-large)\n",
    "- Residual stream metaphor: input processed across layers\n",
    "- Each block refines token representation, leading to better predictions\n",
    "- Essential for large-scale models like GPT and T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization in Transformers Using A Single Matrix ğ—\n",
    "- Computation for each token in a transformer block can be parallelized.\n",
    "  - âˆµ All the computation in the transformer block computing $ğ¡_i$ from the input $ğ±_i$\n",
    "- Input embeddings for all tokens are packed into a matrix $ğ—$ of size $[N \\times d]$\n",
    "  - where $N$ is the number of tokens, and $d$ is the embedding dimensionality.\n",
    "- Common input sizes range from 1K to 32K tokens.\n",
    "\n",
    "---\n",
    "\n",
    "### **Attention Computation for a Single Attention Head**\n",
    "\n",
    "- Multiply $ğ—$ by key, query, and value weight matrices $ğ–^ğŠâˆˆ[dÃ—d_k]$,  $ğ–^ğâˆˆ[dÃ—d_k]$, and $ğ–^ğ•âˆˆ[dÃ—d_v]$.\n",
    "- Compute $ğ\\in \\mathbb{R}^{N \\times d_k}$, $ğŠ\\in \\mathbb{R}^{N \\times d_k}$, and $ğ•\\in \\mathbb{R}^{N \\times d_v}$ matrices for the entire input sequence.\n",
    "  - $ğ = ğ—ğ–^ğ, \\quad ğŠ = ğ—ğ–^ğŠ, \\quad ğ• = ğ—ğ–^ğ•$\n",
    "\n",
    "- **Efficient ğuery-Key Comparisons**\n",
    "  - Perform all query-key comparisons using matrix multiplication $ğğŠ^ğ“âˆˆ[N \\times N]$.\n",
    "  - ![all qáµ¢ Â· kâ±¼ comparisons in a single matrix multiple](./images/trans/qk.png)\n",
    "\n",
    "- The entire self-attention step for an entire sequence of $N$ tokens for one head\n",
    "  - $\\displaystyle ğ€ = \\text{softmax}\\left( \\text{mask}\\left(\\frac{ğğŠ^ğ“}{\\sqrt{d_k}}\\right) \\right) ğ•$\n",
    "  - This computes the attention for all tokens simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "### **Masking Future Tokens**\n",
    "- For language modeling, we prevent access to future tokens by masking the upper-triangular part of the $ğğŠ^ğ“$ matrix.\n",
    "  - $M_{ij} = \n",
    "  \\begin{cases}\n",
    "    -\\infty & \\text{if } j > i \\text{, i.e. for the upper-triangular portion} \\\\\n",
    "    0 & \\text{otherwise}\n",
    "  \\end{cases}$\n",
    "  - ![mask](./images/trans/mask.png)\n",
    "  - This avoids cheating by looking at the following tokens.\n",
    "- The attention computation for a single attention head in parallel\n",
    "\n",
    "![The attention computation for a single attention head in parallel](./images/trans/singleall.png)\n",
    "---\n",
    "\n",
    "### **Parallelizing Multi-Head Attention**\n",
    "- Multiple attention heads run in parallel.\n",
    "- For each head $i$, calculate $ğ^i[NÃ—d_k]$, $ğŠ^i[NÃ—d_k]$, and $ğ•^i[NÃ—d_v]$ \n",
    "  - $ğ^i = ğ—ğ–^{ğ^i}, \\quad ğŠ^i = ğ—ğ–^{ğŠ_i}, \\quad ğ•^i = ğ—ğ–^{ğ•_i}$\n",
    "    - Matrix shape: $ğ–^{ğ_i}âˆˆ â„^{dÃ—d_k}$, $ğ–^{ğŠ_i}âˆˆ â„^{dÃ—d_k}$, and $ğ–^{ğ•_i}âˆˆ â„^{dÃ—d_v}$\n",
    "  - $\\textbf{head}_i = \\text{SelfAttention}(ğ^i, ğŠ^i, ğ•^i) = \\text{softmax}\\left( \\dfrac{ğ^i ğŠ^{iğ“}}{\\sqrt{d_k}} \\right) ğ•^i$\n",
    "    - $\\textbf{head}_i âˆˆ NÃ—d_v$, so the concatenation of $h$ attentions as a single output has shape $NÃ—hd_v$\n",
    "\n",
    "- **Final Multi-Head Attention Output:**\n",
    "  - The output of the multi-head attention is concatenated and projected back to dimensionality $[N \\times d]$.\n",
    "  - $\\text{MultiHeadAttention}(ğ—) = (\\textbf{head}_1 \\oplus \\textbf{head}_2 \\oplus \\cdots \\oplus \\textbf{head}_h) ğ–^ğ$\n",
    "    - $ğ–^ğ \\in \\mathbb{R}^{hd_v \\times d}$ is the linear projection matrix.\n",
    "  - This ensures the dimensionality is preserved for further transformer layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **Putting It All Together with the Parallelized Input Matrix $ğ—$**\n",
    "- The entire layer of $N$ transformer block over the entire $N$ input tokens is parallelized.\n",
    "- Layer normalization and feedforward layers are applied in parallel to each token.\n",
    "\n",
    "   - $ğ = \\text{LayerNorm}(ğ— + \\text{MultiHeadAttention}(ğ—))$\n",
    "   - $ğ‡ = \\text{LayerNorm}(ğ + \\text{FFN}(ğ))$\n",
    "\n",
    "- Break it down with one equation for each component computation:\n",
    "   - $ğ“^1 = \\text{MultiHeadAttention}(ğ—)$\n",
    "   - $ğ“^2 = ğ— + ğ“^1$\n",
    "   - $ğ“^3 = \\text{LayerNorm}(ğ“^2)$\n",
    "   - $ğ“^4 = \\text{FFN}(ğ“^3)$\n",
    "   - $ğ“^5 = ğ“^4 + ğ“^3$\n",
    "   - $ğ‡ = \\text{LayerNorm}(ğ“^5)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input: Embeddings for Token and Position\n",
    "\n",
    "  - The transformer creates two embeddings: \n",
    "    - `token` embedding and `positional` embedding.\n",
    "  - The combined embeddings form the input matrix $ğ—$ of shape $[N \\times d]$\n",
    "    - where $N$ is the number of tokens, and $d$ is the embedding dimensionality.\n",
    "- Token Embeddings from the Vocabulary\n",
    "  - Token embeddings are vectors of dimension $d$ stored in the embedding matrix $ğ„$.\n",
    "  - Each word from the vocabulary $V$ has a corresponding row in $ğ„$.\n",
    "    - $ğ„ \\in \\mathbb{R}^{|V| \\times d}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Selecting the Embedding for a Sequence of Tokens**\n",
    "  - Convert tokens into `vocabulary indices` and select the corresponding rows from the embedding matrix $ğ„$.\n",
    "    - This is equivalent to multiplying a `one-hot vector` with the embedding matrix to retrieve the token embedding.\n",
    "\n",
    "- ğŸ **One-Hot Vector:**\n",
    "  - $[0\\ 0\\ 0\\ 0\\ 1\\ 0\\ 0 \\dots 0]$\n",
    "  - The embedding for token $i$ is selected as $ğ„[i]$.\n",
    "\n",
    "![Selecting the embedding vector for word Vâ‚…](./images/trans/seli.png)\n",
    "\n",
    "- The entire token sequence can be selected similarly\n",
    "  \n",
    "![Selecting the embedding matrix for the input sequence of token ids W](./images/trans/selseq.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Positional Embeddings**\n",
    "- Token embeddings are position-independent.\n",
    "- Positional embeddings represent the position of each token in the sequence.\n",
    "  - Stored in `Positional Embedding Matrix`: $ğ„_{pos} \\in \\mathbb{R}^{1 \\times N}$\n",
    "- How to get these positional embeddings?\n",
    "  - Absolute positional embeddings are randomly initialized and learned during training.\n",
    "  - Each position in the sequence has a corresponding embedding.\n",
    "\n",
    "- The final input representation $ğ—[N \\times d]$ is obtained by `adding token embeddings and positional embeddings.`\n",
    "  - $ğ—[i] = ğ„[\\text{id}(i)] + ğ[i]$\n",
    "\n",
    "![A simple way to model position](./images/trans/posem.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations of Absolute Position Embeddings**\n",
    "\n",
    "- Early positions are well-trained due to their frequency in training\n",
    "  - while later positions may be undertrained and may not generalize well.\n",
    "\n",
    "- A more robust alternative is using a `static function` like sine and cosine functions to map integer inputs to real-valued vectors in a way that captures the inherent relationships among the positions.\n",
    "- `Relative positional embeddings` capture relationships between positions and are often implemented in the attention mechanism at each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling Head\n",
    "  - A \"head\" refers to additional neural circuitry added to the transformer for specific tasks.\n",
    "  - It enables transformers to predict the next word given a context.\n",
    "- **Word Prediction with Language Models**\n",
    "  - Given a context like â€œThanks for all theâ€, the model computes $P(\\text{fish}|\\text{Thanks for all the})$.\n",
    "  - The language model outputs a distribution over the entire vocabulary for predicting the next word.\n",
    "  - In transformers, the context size is determined by the modelâ€™s window, allowing large contexts (e.g., 2K, 4K tokens).\n",
    "\n",
    "---\n",
    "\n",
    "### **Structure of the Language Modeling Head**\n",
    "- The head takes the output embedding $h^L_N$ from the last transformer layer for the last token $N$.\n",
    "- The goal is to produce a probability distribution over the vocabulary for the next token $N+1$.\n",
    "\n",
    "![Word Prediction with Language Models](./images/trans/lmhead.png)\n",
    "\n",
    "- Linear Layer: From Embedding to Logits\n",
    "  - The first step is a linear layer that projects the output embedding $ğ¡^ğ‹_ğ[1 \\times d]$ into a logit vector $ğ®[1 \\times |V|]$.\n",
    "  - This vector contains a score for each word in the vocabulary $V$.\n",
    "  - $ğ® = ğ¡^ğ‹_ğ ğ„^T$\n",
    "- Weight Tying with the Embedding Matrix\n",
    "  - The same embedding matrix $ğ„[|V| \\times d]$ used to embed tokens is reused as $ğ„^T$ to map embeddings back to the vocabulary space.\n",
    "  - This process is known as **unembedding**.\n",
    "- Softmax: Turning Logits into Probabilities\n",
    "  - The logit vector $ğ®$ is passed through a **softmax** function to produce a probability distribution $ğ²$ over the vocabulary.\n",
    "    - $ğ² = \\text{softmax}(ğ®)$\n",
    "    - The output $ğ²$ represents the probabilities of each word in the vocabulary being the next word.\n",
    "- Generating Text with the Language Model\n",
    "  - The most probable word (highest $y_k$) can be selected (greedy decoding) or sampled probabilistically using other methods.\n",
    "  - The word corresponding to the selected index $k$ is the generated next word.\n",
    "\n",
    "![A transformer language model (decoder-only)](./images/trans/gen.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **The Logit Lens: A Tool for Model Interpretability**\n",
    "- The **logit lens** is a method to interpret the internal states of a transformer model.\n",
    "-  **How it Works:**\n",
    "   1. Take a vector from **any layer** of the transformer.\n",
    "   2. Multiply it by the **unembedding layer** (transpose of the embedding matrix).\n",
    "   3. Apply a **softmax** to compute a probability distribution over words.\n",
    "   4. This gives insight into the words that internal layers might be predicting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformer Decoder for Language Modeling**\n",
    "- Transformers originally used an **encoder-decoder** architecture.\n",
    "- For causal language modeling, we only use the **decoder** part to generate the next word in sequence.\n",
    "  - This architecture is commonly used for text generation and machine translation.\n",
    "\n",
    "### ğŸ Building a basic transformer-based language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set the model in evaluation mode to predict\n",
    "model.eval()\n",
    "\n",
    "def predict_next_word(text, model, tokenizer, max_length=50):\n",
    "    # Tokenize the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\n",
    "    # Generate the next token logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get the predicted next token id\n",
    "    predicted_token_id = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "\n",
    "    # Decode the predicted token id back to a word\n",
    "    predicted_word = tokenizer.decode(predicted_token_id)\n",
    "\n",
    "    return predicted_word\n",
    "\n",
    "# Test the function\n",
    "input_text = \"The quick brown fox jumps over\"\n",
    "predicted_word = predict_next_word(input_text, model, tokenizer)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Predicted next word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Formal Algorithms for Transformers](https://arxiv.org/pdf/2207.09238)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
